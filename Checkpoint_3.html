<h1 id="predicting-malignant-melanoma-by-appearance">Predicting Malignant Melanoma by Appearance</h1>
<p>Tutorial by Sankha Ghosal, Jacob Keller and Madhur Maheshwari</p>
<h2 id="why-melanoma">Why melanoma?</h2>
<p>Malignant melanoma is a serious health risk, a cancer of the skin most commonly caused by chronic overexposure to the sun. Like any cancer, melanoma can metastasize and attack the entire body, potentially fatally, if not identified and addressed early in its cycle. And yet, most people think of sunburns as minor annoyances, like papercuts, not giving this serious risk the attention it deserves. Given that members of our families have suffered from this disease, we decided that designing deep learning model to rapidly identify and diagnose melanoma could greatly improve health outcomes for young Americans and for people worldwide.</p>
<p>In addition to its personal and medical relevance, malignant melanoma also presents an intriguing challenge as we construct our model, because it is often difficult to distinguish visually from freckles, moles, or even benign melanoma. To ensure the validity of our model in this difficult task, we have chosen a dataset that includes a training set of 4605 malignant melanoma images and 5000 benign melanoma images, plus a test set of 500 malignant melanoma images and 500 benign melanoma images.</p>
<p><strong>Source</strong></p>
<p>Muhammad Hasnain Javid. (2022). Melanoma Skin Cancer Dataset of 10000 Images [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DSV/3376422</p>
<h1 id="gathering-and-exploring-our-data">Gathering and Exploring Our Data</h1>
<p>Thanks to the International Skin Imaging Collaboration (ISIC) project, hundreds of thousands of images of melanoma are available on the Internet, all labeled by type and malignance. We chose to analyze <a href="https://https://www.kaggle.com/datasets/hasnainjaved/melanoma-skin-cancer-dataset-of-10000-images/datahttps://www.kaggle.com/datasets/hasnainjaved/melanoma-skin-cancer-dataset-of-10000-images/data">this dataset from Kaggle</a> because it contains a large number of images, binarily classified as benign or malignant, and lacking any other patient data. This will enable us to train our model to rapidly identify melanoma based just on its appearance, without requiring us to gather any additional medical data.</p>
<p>The images in this dataset are of the JPEG file type. In the future, we plan to extend our model to handle other image types as well, particularly DICOM files, a file type commonly used in medical imaging for its ability to store images alongside patient information and imaging metadata. The additional information stored in these DICOM files could make our model even more impactful and useful for medical professionals.</p>
<h2 id="data-import-and-resizing">Data Import and Resizing</h2>
<p>Below, we begin by mounting our Google Drive, so that we can access our dataset (uploaded there as a .zip file):</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="co"># Mount our Google Drive to our Colab workspace. This is the first step of making our data accessible within our notebook.</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a><span class="im">from</span> google.colab <span class="im">import</span> drive</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a>drive.mount(<span class="st">&#39;/content/drive&#39;</span>, force_remount<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<pre><code>Mounted at /content/drive</code></pre>
<p>As a next step, we need to import the libraries we’ll be using for this project, along with our data itself.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="co"># Install and import necessary libraries</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a><span class="op">!</span>pip install torch torchvision torchaudio wandb timm opencv<span class="op">-</span>python</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a><span class="op">!</span>pip install timm</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true"></a><span class="im">import</span> os</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true"></a><span class="im">import</span> zipfile</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true"></a><span class="im">import</span> wandb</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true"></a><span class="im">import</span> torch</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true"></a><span class="im">import</span> torch.nn</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true"></a><span class="im">import</span> timm</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, Subset</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets, transforms</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> StratifiedKFold</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true"></a><span class="im">from</span> zipfile <span class="im">import</span> ZipFile</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true"></a><span class="im">from</span> pathlib <span class="im">import</span> PurePosixPath</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true"></a><span class="im">import</span> io</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true"></a><span class="im">import</span> random</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true"></a><span class="im">import</span> pickle</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true"></a><span class="im">from</span> PIL <span class="im">import</span> Image, ImageStat, ImageEnhance</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true"></a><span class="im">import</span> cv2</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true"></a><span class="im">from</span> scipy.stats <span class="im">import</span> chisquare, ttest_ind, shapiro, chi2_contingency</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true"></a><span class="im">from</span> pandas.plotting <span class="im">import</span> scatter_matrix</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true"></a><span class="im">import</span> os</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true"></a><span class="im">import</span> zipfile</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true"></a><span class="im">import</span> pickle</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true"></a><span class="im">import</span> shutil</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> recall_score, precision_score, f1_score, roc_auc_score</span></code></pre></div>
<pre><code>Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)
Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)
Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)
Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.23.1)
Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.22)
Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)
Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)
Requirement already satisfied: typing-extensions&gt;=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)
Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)
Requirement already satisfied: sympy&gt;=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)
Requirement already satisfied: networkx&gt;=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)
Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)
Requirement already satisfied: fsspec&gt;=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)
Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)
Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)
Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)
Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)
Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)
Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)
Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)
Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)
Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)
Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)
Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)
Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)
Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)
Requirement already satisfied: pillow!=8.3.*,&gt;=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)
Requirement already satisfied: click&gt;=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)
Requirement already satisfied: gitpython!=3.1.29,&gt;=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)
Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)
Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.1)
Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,&lt;7,&gt;=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)
Requirement already satisfied: pydantic&lt;3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.3)
Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)
Requirement already satisfied: requests&lt;3,&gt;=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)
Requirement already satisfied: sentry-sdk&gt;=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.47.0)
Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.36.0)
Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.7.0)
Requirement already satisfied: gitdb&lt;5,&gt;=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,&gt;=1.0.0-&gt;wandb) (4.0.12)
Requirement already satisfied: annotated-types&gt;=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic&lt;3-&gt;wandb) (0.7.0)
Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic&lt;3-&gt;wandb) (2.41.4)
Requirement already satisfied: typing-inspection&gt;=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic&lt;3-&gt;wandb) (0.4.2)
Requirement already satisfied: charset_normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.12/dist-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (3.4.4)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.12/dist-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (3.11)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (2.5.0)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (2025.11.12)
Requirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy&gt;=1.13.3-&gt;torch) (1.3.0)
Requirement already satisfied: tqdm&gt;=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub-&gt;timm) (4.67.1)
Requirement already satisfied: hf-xet&lt;2.0.0,&gt;=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub-&gt;timm) (1.2.0)
Requirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2-&gt;torch) (3.0.3)
Requirement already satisfied: smmap&lt;6,&gt;=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb&lt;5,&gt;=4.0.1-&gt;gitpython!=3.1.29,&gt;=1.0.0-&gt;wandb) (5.0.2)
Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.22)
Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.9.0+cu126)
Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.24.0+cu126)
Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)
Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.36.0)
Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.7.0)
Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub-&gt;timm) (3.20.0)
Requirement already satisfied: fsspec&gt;=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub-&gt;timm) (2025.3.0)
Requirement already satisfied: packaging&gt;=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub-&gt;timm) (25.0)
Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub-&gt;timm) (2.32.4)
Requirement already satisfied: tqdm&gt;=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub-&gt;timm) (4.67.1)
Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub-&gt;timm) (4.15.0)
Requirement already satisfied: hf-xet&lt;2.0.0,&gt;=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub-&gt;timm) (1.2.0)
Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch-&gt;timm) (75.2.0)
Requirement already satisfied: sympy&gt;=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch-&gt;timm) (1.14.0)
Requirement already satisfied: networkx&gt;=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch-&gt;timm) (3.6.1)
Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-&gt;timm) (3.1.6)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch-&gt;timm) (12.6.77)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch-&gt;timm) (12.6.77)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch-&gt;timm) (12.6.80)
Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch-&gt;timm) (9.10.2.21)
Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch-&gt;timm) (12.6.4.1)
Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch-&gt;timm) (11.3.0.4)
Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch-&gt;timm) (10.3.7.77)
Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch-&gt;timm) (11.7.1.2)
Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch-&gt;timm) (12.5.4.2)
Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch-&gt;timm) (0.7.1)
Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch-&gt;timm) (2.27.5)
Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch-&gt;timm) (3.3.20)
Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch-&gt;timm) (12.6.77)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch-&gt;timm) (12.6.85)
Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch-&gt;timm) (1.11.1.6)
Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch-&gt;timm) (3.5.0)
Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision-&gt;timm) (2.0.2)
Requirement already satisfied: pillow!=8.3.*,&gt;=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision-&gt;timm) (11.3.0)
Requirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy&gt;=1.13.3-&gt;torch-&gt;timm) (1.3.0)
Requirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2-&gt;torch-&gt;timm) (3.0.3)
Requirement already satisfied: charset_normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.12/dist-packages (from requests-&gt;huggingface_hub-&gt;timm) (3.4.4)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.12/dist-packages (from requests-&gt;huggingface_hub-&gt;timm) (3.11)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests-&gt;huggingface_hub-&gt;timm) (2.5.0)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests-&gt;huggingface_hub-&gt;timm) (2025.11.12)</code></pre>
<p>Since most machine learning models prefer or require images of an equal, standard size, we will clean our data by resizing all our images to the standard 256x256 during import.</p>
<p>You may also consider enhancing image quality at this stage, as we did at an earlier stage of building our model. However, after several iterations we believe that leaving images as is, given that our data are high-grade medical images, will actually allow our model to learn and generalize to new images better.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a><span class="co"># Define the path to our full dataset</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true"></a>full_dataset_path <span class="op">=</span> <span class="st">&quot;/content/drive/MyDrive/DATA602_Final_Project/full_dataset.zip&quot;</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true"></a><span class="co"># Define path to our future resized training set</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true"></a>resized_train_dir <span class="op">=</span> <span class="st">&quot;/content/drive/MyDrive/DATA602_Final_Project/train_resized.zip&quot;</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true"></a><span class="co"># Define the path to our future resized test set</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true"></a>resized_test_dir <span class="op">=</span> <span class="st">&quot;/content/drive/MyDrive/DATA602_Final_Project/test_resized.zip&quot;</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true"></a><span class="co"># Open full dataset as read-only for resizing</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true"></a><span class="cf">with</span> ZipFile(full_dataset_path, <span class="st">&#39;r&#39;</span>) <span class="im">as</span> dataset:</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true"></a>  <span class="co"># Split dataset into train and test based on file label</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true"></a>  train_files <span class="op">=</span> [f <span class="cf">for</span> f <span class="kw">in</span> dataset.namelist() <span class="cf">if</span> <span class="st">&#39;/train/&#39;</span> <span class="kw">in</span> f <span class="kw">and</span> f.lower().endswith(<span class="st">&#39;.jpg&#39;</span>)]</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true"></a>  test_files  <span class="op">=</span> [f <span class="cf">for</span> f <span class="kw">in</span> dataset.namelist() <span class="cf">if</span> <span class="st">&#39;/test/&#39;</span>  <span class="kw">in</span> f <span class="kw">and</span> f.lower().endswith(<span class="st">&#39;.jpg&#39;</span>)]</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true"></a>  <span class="co"># Open resized train set path as write-only</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true"></a>  <span class="cf">with</span> ZipFile(resized_train_dir, <span class="st">&#39;w&#39;</span>) <span class="im">as</span> resized_train:</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true"></a>    <span class="co"># Iterate over all files in train set</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true"></a>    <span class="cf">for</span> file_name <span class="kw">in</span> train_files:</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true"></a>      <span class="co"># Read image in file location</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true"></a>      <span class="cf">try</span>:</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true"></a>        img_data <span class="op">=</span> dataset.read(file_name)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true"></a>        img <span class="op">=</span> Image.<span class="bu">open</span>(io.BytesIO(img_data))</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true"></a></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true"></a>        <span class="co"># Resize image to (256, 256) for optimal machine learning</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true"></a>        img_resized <span class="op">=</span> img.resize((<span class="dv">256</span>, <span class="dv">256</span>), Image.LANCZOS)</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true"></a>        <span class="co"># Extract relative path</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true"></a>        parts <span class="op">=</span> PurePosixPath(file_name).parts</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true"></a>        train_index <span class="op">=</span> parts.index(<span class="st">&quot;train&quot;</span>)</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true"></a>        relative_path <span class="op">=</span> <span class="st">&quot;/&quot;</span>.join(parts[train_index<span class="op">+</span><span class="dv">1</span>:])</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true"></a></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true"></a>        <span class="co"># Save enhanced and resized image</span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true"></a>        buf <span class="op">=</span> io.BytesIO()</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true"></a>        img_resized.save(buf, <span class="bu">format</span><span class="op">=</span>img.<span class="bu">format</span> <span class="kw">or</span> <span class="st">&quot;JPEG&quot;</span>)</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true"></a>        resized_train.writestr(relative_path, buf.getvalue())</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true"></a></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true"></a>      <span class="co"># Handle any processing errors</span></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true"></a>      <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true"></a></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true"></a>        <span class="co"># Output error message</span></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true"></a>        <span class="bu">print</span>(<span class="ss">f&quot;Error processing </span><span class="sc">{</span>file_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{e}</span><span class="ss">&quot;</span>)</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true"></a>        <span class="cf">continue</span></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true"></a></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true"></a>  <span class="co"># Open resized test set path as write-only</span></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true"></a>  <span class="cf">with</span> ZipFile(resized_test_dir, <span class="st">&#39;w&#39;</span>) <span class="im">as</span> resized_test:</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true"></a></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true"></a>    <span class="co"># Iterate over all files in test set</span></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true"></a>    <span class="cf">for</span> file_name <span class="kw">in</span> test_files:</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true"></a></span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true"></a>      <span class="co"># Read image in file location</span></span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true"></a>      <span class="cf">try</span>:</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true"></a>        img_data <span class="op">=</span> dataset.read(file_name)</span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true"></a>        img <span class="op">=</span> Image.<span class="bu">open</span>(io.BytesIO(img_data))</span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true"></a></span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true"></a>        <span class="co"># Resize image to (256, 256) for optimal machine learning processing</span></span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true"></a>        img_resized <span class="op">=</span> img.resize((<span class="dv">256</span>, <span class="dv">256</span>), Image.LANCZOS)</span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true"></a></span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true"></a>        <span class="co"># Extract parts of path to individual image file</span></span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true"></a>        parts <span class="op">=</span> PurePosixPath(file_name).parts</span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true"></a></span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true"></a>        <span class="co"># Find test folder in individual image path</span></span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true"></a>        test_index <span class="op">=</span> parts.index(<span class="st">&quot;test&quot;</span>)</span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true"></a></span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true"></a>        <span class="co"># Skip every part of individual image path before &quot;test&quot;</span></span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true"></a>        relative_path <span class="op">=</span> <span class="st">&quot;/&quot;</span>.join(parts[test_index<span class="op">+</span><span class="dv">1</span>:])</span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true"></a></span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true"></a>        <span class="co"># Save resized image</span></span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true"></a>        buf <span class="op">=</span> io.BytesIO()</span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true"></a>        img_resized.save(buf, <span class="bu">format</span><span class="op">=</span>img.<span class="bu">format</span> <span class="kw">or</span> <span class="st">&quot;JPEG&quot;</span>)</span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true"></a></span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true"></a>        <span class="co"># Write cleaned image to cleaned train folder</span></span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true"></a>        resized_test.writestr(relative_path, buf.getvalue())</span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true"></a></span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true"></a>      <span class="co"># Handle any processing errors</span></span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true"></a>      <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true"></a></span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true"></a>        <span class="co"># Output error message</span></span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true"></a>        <span class="bu">print</span>(<span class="ss">f&quot;Error processing </span><span class="sc">{</span>file_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{e}</span><span class="ss">&quot;</span>)</span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true"></a>        <span class="cf">continue</span></span></code></pre></div>
<p>To verify our success in the above tasks, we’ll now print the classes present in our “train” and “test” sets.</p>
<p>Based on what we know about our dataset, these should be “benign” and “malignant” for both the train and test sets.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true"></a>  <span class="co"># Define a list of classes, based on the classes in our train set</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true"></a>  classes_train <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">set</span>(</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true"></a>    PurePosixPath(f).parts[PurePosixPath(f).parts.index(<span class="st">&quot;train&quot;</span>)<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true"></a>    <span class="cf">for</span> f <span class="kw">in</span> train_files</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true"></a>    <span class="cf">if</span> <span class="st">&quot;train&quot;</span> <span class="kw">in</span> PurePosixPath(f).parts</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true"></a>  ))</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true"></a>  <span class="co"># Define a list of classes, based on the classes in our test set</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true"></a>  classes_test <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">set</span>(</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true"></a>    PurePosixPath(f).parts[PurePosixPath(f).parts.index(<span class="st">&quot;test&quot;</span>)<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true"></a>    <span class="cf">for</span> f <span class="kw">in</span> test_files</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true"></a>    <span class="cf">if</span> <span class="st">&quot;test&quot;</span> <span class="kw">in</span> PurePosixPath(f).parts</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true"></a>  ))</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true"></a><span class="co"># Notify user of detected classes in train &amp; test sets</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Detected train classes:&quot;</span>, classes_train)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Detected test classes:&quot;</span>, classes_test)</span></code></pre></div>
<pre><code>Detected train classes: [&#39;benign&#39;, &#39;malignant&#39;]

Detected test classes: [&#39;benign&#39;, &#39;malignant&#39;]</code></pre>
<h2 id="dataframe-building-and-exploration">DataFrame Building and Exploration</h2>
<p>The next step is to explore our data in order to fully understand it. We’ll begin by extracting the key features of our image data into a pandas DataFrame for easy analysis.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true"></a><span class="co"># Define a function that will load our images into a DataFrame for easy analysis</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true"></a><span class="kw">def</span> image_df(zip_path, classes, limit<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true"></a>  <span class="co"># Initialize an empty list we will fill with our data</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true"></a>  data<span class="op">=</span>[]</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true"></a>  <span class="co"># Open zipfile of our image data</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true"></a>  <span class="cf">with</span> zipfile.ZipFile(zip_path, <span class="st">&#39;r&#39;</span>) <span class="im">as</span> database:</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true"></a>    <span class="co"># List all filenames in .zip</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true"></a>    file_list <span class="op">=</span> database.namelist()</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true"></a>    <span class="co"># Iterate over all classes in our dataset</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true"></a>    <span class="cf">for</span> cls <span class="kw">in</span> classes:</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true"></a></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true"></a>      <span class="co"># Filter so we only see images in current class</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true"></a>      class_images <span class="op">=</span> [<span class="bu">file</span> <span class="cf">for</span> <span class="bu">file</span> <span class="kw">in</span> file_list <span class="cf">if</span> <span class="bu">file</span>.startswith(cls <span class="op">+</span> <span class="st">&#39;/&#39;</span>)]</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true"></a>      <span class="co"># Remove images beyond max number specified (if passed)</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true"></a>      <span class="cf">if</span> limit:</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true"></a>        class_images <span class="op">=</span> class_images[:limit]</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true"></a></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true"></a>      <span class="co"># Iterate over all images in current class</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true"></a>      <span class="cf">for</span> image_path <span class="kw">in</span> class_images:</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true"></a></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true"></a>        <span class="co"># Read image data directly from file</span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true"></a>        <span class="cf">try</span>:</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true"></a>          <span class="cf">with</span> database.<span class="bu">open</span>(image_path) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true"></a></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true"></a>            <span class="co"># Read image</span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true"></a>            image <span class="op">=</span> Image.<span class="bu">open</span>(io.BytesIO(<span class="bu">file</span>.read()))</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true"></a></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true"></a>            <span class="co"># Ensure image is RGB color format</span></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true"></a>            image <span class="op">=</span> image.convert(<span class="st">&quot;RGB&quot;</span>)</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true"></a></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true"></a>          <span class="co"># Read image size</span></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true"></a>          width, height <span class="op">=</span> image.size</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true"></a></span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true"></a>          <span class="co"># Calculate average intensity of each color channel</span></span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true"></a>          stat <span class="op">=</span> ImageStat.Stat(image)</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true"></a>          red_mean, green_mean, blue_mean <span class="op">=</span> stat.mean</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true"></a></span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true"></a>          <span class="co"># Calculate standard deviation of each color channel</span></span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true"></a>          red_std, green_std, blue_std <span class="op">=</span> stat.stddev</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true"></a></span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true"></a>          <span class="co"># Convert image to grayscale to analyze intensity</span></span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true"></a>          gray <span class="op">=</span> image.convert(<span class="st">&#39;L&#39;</span>)</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true"></a></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true"></a>          <span class="co"># Calculate image intensity</span></span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true"></a>          gray_np <span class="op">=</span> np.array(gray, dtype<span class="op">=</span>np.uint8)</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true"></a>          laplacian <span class="op">=</span> cv2.Laplacian(gray_np, cv2.CV_64F)</span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true"></a>          sharpness <span class="op">=</span> laplacian.var()</span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true"></a></span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true"></a>          <span class="co"># Add data we&#39;ve extracted to our list</span></span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true"></a>          data.append({</span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true"></a>              <span class="st">&quot;Filename&quot;</span>: image_path.split(<span class="st">&#39;/&#39;</span>)[<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true"></a>              <span class="st">&quot;Class&quot;</span>: cls,</span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true"></a>              <span class="st">&quot;Width&quot;</span>: width,</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true"></a>              <span class="st">&quot;Height&quot;</span>: height,</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true"></a>              <span class="st">&quot;Blue_mean&quot;</span>: blue_mean,</span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true"></a>              <span class="st">&quot;Green_mean&quot;</span>: green_mean,</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true"></a>              <span class="st">&quot;Red_mean&quot;</span>: red_mean,</span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true"></a>              <span class="st">&quot;Blue_std&quot;</span>: blue_std,</span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true"></a>              <span class="st">&quot;Green_std&quot;</span>: green_std,</span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true"></a>              <span class="st">&quot;Red_std&quot;</span>: red_std,</span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true"></a>              <span class="st">&quot;Sharpness&quot;</span>: sharpness</span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true"></a>              })</span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true"></a></span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true"></a>          <span class="co"># Handle any unexpected file errors</span></span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true"></a>        <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true"></a>          <span class="bu">print</span>(<span class="ss">f&quot;Unexpected error with </span><span class="sc">{</span>image_path<span class="sc">}</span><span class="ss">: </span><span class="sc">{e}</span><span class="ss">&quot;</span>)</span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true"></a>          <span class="cf">continue</span></span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true"></a></span>
<span id="cb8-74"><a href="#cb8-74" aria-hidden="true"></a>  <span class="co"># Return image DataFrame</span></span>
<span id="cb8-75"><a href="#cb8-75" aria-hidden="true"></a>  <span class="cf">return</span> pd.DataFrame(data)</span>
<span id="cb8-76"><a href="#cb8-76" aria-hidden="true"></a></span>
<span id="cb8-77"><a href="#cb8-77" aria-hidden="true"></a>resized_train_dir <span class="op">=</span> <span class="st">&quot;/content/drive/MyDrive/DATA602_Final_Project/train_resized.zip&quot;</span></span>
<span id="cb8-78"><a href="#cb8-78" aria-hidden="true"></a>resized_test_dir <span class="op">=</span> <span class="st">&quot;/content/drive/MyDrive/DATA602_Final_Project/test_resized.zip&quot;</span></span>
<span id="cb8-79"><a href="#cb8-79" aria-hidden="true"></a></span>
<span id="cb8-80"><a href="#cb8-80" aria-hidden="true"></a><span class="co"># Create DataFrame of training images</span></span>
<span id="cb8-81"><a href="#cb8-81" aria-hidden="true"></a>df_train <span class="op">=</span> image_df(resized_train_dir, classes_train, limit<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb8-82"><a href="#cb8-82" aria-hidden="true"></a></span>
<span id="cb8-83"><a href="#cb8-83" aria-hidden="true"></a><span class="co"># Create DataFrame of test images</span></span>
<span id="cb8-84"><a href="#cb8-84" aria-hidden="true"></a>df_test <span class="op">=</span> image_df(resized_test_dir, classes_test, limit<span class="op">=</span><span class="va">None</span>)</span></code></pre></div>
<p>Now, for a sanity check, we’ll print the first five lines of our dataframes, their summary statistics and other key column information, as well as a random sample of the images in both sets. This will allow us to examine our data, its types, and the missing value counts of each feature, as well as assess the central tendency and spread of our numerical data.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true"></a><span class="co"># Define a function that will display some of the sample images</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true"></a><span class="kw">def</span> display_sample_images(zip_path, df, sample_size<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true"></a>    <span class="co"># Randomly sample a few images</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true"></a>    sample_df <span class="op">=</span> df.sample(sample_size, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true"></a>    <span class="co"># Open the zip file to read the image data</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true"></a>    <span class="cf">with</span> zipfile.ZipFile(zip_path, <span class="st">&#39;r&#39;</span>) <span class="im">as</span> database:</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true"></a>        plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true"></a>        <span class="cf">for</span> i, row <span class="kw">in</span> <span class="bu">enumerate</span>(sample_df.itertuples(), <span class="dv">1</span>):</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true"></a>            image_path <span class="op">=</span> <span class="ss">f&quot;</span><span class="sc">{</span>row<span class="sc">.</span>Class<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>row<span class="sc">.</span>Filename<span class="sc">}</span><span class="ss">&quot;</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true"></a>            <span class="cf">try</span>:</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true"></a>                <span class="co"># Read the image and show it</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true"></a>                <span class="cf">with</span> database.<span class="bu">open</span>(image_path) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true"></a>                    image <span class="op">=</span> Image.<span class="bu">open</span>(io.BytesIO(<span class="bu">file</span>.read()))</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true"></a>                    image <span class="op">=</span> image.convert(<span class="st">&quot;RGB&quot;</span>)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true"></a>                <span class="co"># Plot the image</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true"></a>                plt.subplot(<span class="dv">1</span>, sample_size, i)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true"></a>                plt.imshow(image)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true"></a>                plt.axis(<span class="st">&quot;off&quot;</span>)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true"></a>                plt.title(<span class="ss">f&quot;Class: </span><span class="sc">{</span>row<span class="sc">.</span>Class<span class="sc">}</span><span class="ch">\n</span><span class="ss">(</span><span class="sc">{</span>row<span class="sc">.</span>Filename<span class="sc">}</span><span class="ss">)&quot;</span>, fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true"></a>            <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true"></a>                <span class="bu">print</span>(<span class="ss">f&quot;Could not load </span><span class="sc">{</span>image_path<span class="sc">}</span><span class="ss">: </span><span class="sc">{e}</span><span class="ss">&quot;</span>)</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true"></a>                <span class="cf">continue</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true"></a></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true"></a>        plt.tight_layout()</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true"></a>        plt.show()</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true"></a></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true"></a><span class="co"># Print size of train and test datasets</span></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;Train Dataset Size: </span><span class="sc">{</span><span class="bu">len</span>(df_train)<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Test Dataset Size: </span><span class="sc">{</span><span class="bu">len</span>(df_test)<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true"></a></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true"></a><span class="co"># Print preview of train and test datasets</span></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Train Dataset Preview: </span><span class="ch">\n</span><span class="st">&quot;</span>, df_train.head())</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Test Dataset Preview: </span><span class="ch">\n</span><span class="st">&quot;</span>, df_test.head())</span></code></pre></div>
<pre><code>Train Dataset Size: 9605

Test Dataset Size: 1000

Train Dataset Preview: 
             Filename   Class  Width  Height   Blue_mean  Green_mean  \
0     melanoma_0.jpg  benign    256     256  199.485367  203.990738   
1     melanoma_1.jpg  benign    256     256  185.211121  195.162567   
2    melanoma_10.jpg  benign    256     256  178.852386  176.928574   
3   melanoma_100.jpg  benign    256     256  201.164017  195.680069   
4  melanoma_1000.jpg  benign    256     256  122.928284  127.036591   

     Red_mean   Blue_std  Green_std    Red_std   Sharpness  
0  242.673523  22.331822  15.909801  13.110996   42.487120  
1  239.156235  19.644778  16.621198  15.767713  215.245001  
2  227.864975  31.898888  24.282133  20.175481  371.665336  
3  241.964966  16.805410  15.906786  14.864253   31.183485  
4  165.470535  38.843133  38.359877  38.876573   37.963984  

Test Dataset Preview: 
              Filename   Class  Width  Height   Blue_mean  Green_mean  \
0  melanoma_10000.jpg  benign    256     256  183.734039  162.186279   
1  melanoma_10001.jpg  benign    256     256  161.992584  177.983871   
2  melanoma_10002.jpg  benign    256     256   96.235748   95.010880   
3  melanoma_10003.jpg  benign    256     256  127.001389  124.535141   
4  melanoma_10004.jpg  benign    256     256  188.948776  194.068909   

     Red_mean   Blue_std  Green_std    Red_std    Sharpness  
0  193.797333  29.808587  26.747274  26.203962    24.110775  
1  223.335709  26.715424  18.281802  15.683679    66.509490  
2  180.593674  31.630017  33.208187  33.651232    59.376454  
3  169.232147  36.801103  34.572358  36.275918  2492.112579  
4  240.482544  23.413261  16.607956  14.274689    40.108118  </code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true"></a><span class="co"># Print train data info</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Train DataFrame Info: </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true"></a>df_train.info()</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true"></a><span class="co"># Print test data info</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Test DataFrame Info: </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true"></a>df_test.info()</span></code></pre></div>
<pre><code>Train DataFrame Info: 

&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 9605 entries, 0 to 9604
Data columns (total 11 columns):
 #   Column      Non-Null Count  Dtype  
---  ------      --------------  -----  
 0   Filename    9605 non-null   object 
 1   Class       9605 non-null   object 
 2   Width       9605 non-null   int64  
 3   Height      9605 non-null   int64  
 4   Blue_mean   9605 non-null   float64
 5   Green_mean  9605 non-null   float64
 6   Red_mean    9605 non-null   float64
 7   Blue_std    9605 non-null   float64
 8   Green_std   9605 non-null   float64
 9   Red_std     9605 non-null   float64
 10  Sharpness   9605 non-null   float64
dtypes: float64(7), int64(2), object(2)
memory usage: 825.6+ KB

Test DataFrame Info: 

&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 1000 entries, 0 to 999
Data columns (total 11 columns):
 #   Column      Non-Null Count  Dtype  
---  ------      --------------  -----  
 0   Filename    1000 non-null   object 
 1   Class       1000 non-null   object 
 2   Width       1000 non-null   int64  
 3   Height      1000 non-null   int64  
 4   Blue_mean   1000 non-null   float64
 5   Green_mean  1000 non-null   float64
 6   Red_mean    1000 non-null   float64
 7   Blue_std    1000 non-null   float64
 8   Green_std   1000 non-null   float64
 9   Red_std     1000 non-null   float64
 10  Sharpness   1000 non-null   float64
dtypes: float64(7), int64(2), object(2)
memory usage: 86.1+ KB</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true"></a><span class="co"># Print train data summary statistics</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Train Data Summary Stats: </span><span class="ch">\n</span><span class="sc">{</span>df_train<span class="sc">.</span>describe()<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true"></a><span class="co"># Print test data summary statistics</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Test Data Summary Stats: </span><span class="ch">\n</span><span class="sc">{</span>df_test<span class="sc">.</span>describe()<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>Train Data Summary Stats: 
        Width  Height    Blue_mean   Green_mean     Red_mean     Blue_std  \
count  9605.0  9605.0  9605.000000  9605.000000  9605.000000  9605.000000   
mean    256.0   256.0   139.163977   144.752983   183.043869    36.920379   
std       0.0     0.0    40.011337    37.080680    41.598023    18.569259   
min     256.0   256.0     4.643234    18.050079    32.185028     6.553150   
25%     256.0   256.0   112.306396   121.357758   160.332260    22.892501   
50%     256.0   256.0   138.573486   144.278076   185.482925    31.320780   
75%     256.0   256.0   167.877945   173.222168   216.419540    46.829141   
max     256.0   256.0   243.250122   241.259064   251.234024   101.917013   

         Green_std      Red_std    Sharpness  
count  9605.000000  9605.000000  9605.000000  
mean     34.195921    31.451243   175.610364  
std      18.999948    21.977869   388.619777  
min       6.829028     4.863052     3.854461  
25%      19.638917    16.205155    29.948115  
50%      28.180845    22.686283    60.146049  
75%      43.157802    38.644482   132.143875  
max      96.336945   109.437984  6877.040557   


Test Data Summary Stats: 
        Width  Height    Blue_mean   Green_mean     Red_mean     Blue_std  \
count  1000.0  1000.0  1000.000000  1000.000000  1000.000000  1000.000000   
mean    256.0   256.0   134.534389   141.796100   177.572035    39.260971   
std       0.0     0.0    41.102135    38.760274    45.509354    19.643315   
min     256.0   256.0     7.658539    26.726837    39.186676     8.809927   
25%     256.0   256.0   105.352509   117.058727   153.191200    23.840996   
50%     256.0   256.0   132.978951   141.262413   182.286758    33.682233   
75%     256.0   256.0   163.519405   171.677174   214.012314    51.685908   
max     256.0   256.0   236.518555   243.066422   249.866882    98.825392   

         Green_std      Red_std    Sharpness  
count  1000.000000  1000.000000  1000.000000  
mean     37.297253    35.677070   183.369235  
std      20.532496    24.405054   428.297493  
min       9.346626     8.215627     3.518580  
25%      20.681527    17.322672    26.666662  
50%      31.118099    25.490407    57.402896  
75%      49.298442    47.562574   137.989022  
max      93.221186   106.192828  4933.917330   </code></pre>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true"></a><span class="co"># Print count of classes in train set</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Train Data Class Counts: </span><span class="ch">\n</span><span class="sc">{</span>df_train[<span class="st">&#39;Class&#39;</span>]<span class="sc">.</span>value_counts()<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true"></a><span class="co"># Print count of classes in test set</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Test Data Class Counts: </span><span class="ch">\n</span><span class="sc">{</span>df_test[<span class="st">&#39;Class&#39;</span>]<span class="sc">.</span>value_counts()<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true"></a><span class="co"># Print sample images of classes in train set</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;Sample Training Images:&quot;</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true"></a>display_sample_images(resized_train_dir, df_train, sample_size<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true"></a><span class="co"># Print sample images of classes in test set</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;Sample Test Images:&quot;</span>)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true"></a>display_sample_images(resized_test_dir, df_test, sample_size<span class="op">=</span><span class="dv">5</span>)</span></code></pre></div>
<pre><code>Train Data Class Counts: 
Class
benign       5000
malignant    4605
Name: count, dtype: int64 


Test Data Class Counts: 
Class
benign       500
malignant    500
Name: count, dtype: int64 

Sample Training Images:</code></pre>
<figure>
<img src="https://theonlineconverter.com/uploads/Checkpoint_3_1765927044_files/Checkpoint_3_1765927044_19_1.png" alt="" /><figcaption>png</figcaption>
</figure>
<pre><code>Sample Test Images:</code></pre>
<figure>
<img src="https://theonlineconverter.com/uploads/Checkpoint_3_1765927044_files/Checkpoint_3_1765927044_19_3.png" alt="" /><figcaption>png</figcaption>
</figure>
<h2 id="handling-duplicates-and-missing-values">Handling Duplicates and Missing Values</h2>
<p>We can see from the sanity check above that our data does not contain any missing values. However, it’s important to include steps to deal with these missing values in our workflow, to ensure our methods are generalizable to any dataset future users may pass to it.</p>
<p>Below we’ll assess the duplicates and missing values in our dataset.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true"></a><span class="co"># Print duplicate rows from training set</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;Duplicates in training set:</span><span class="ch">\n</span><span class="st">&quot;</span>, df_train[df_train.duplicated()])</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true"></a><span class="co"># Print duplicate rows from test set</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Duplicates in test set:</span><span class="ch">\n</span><span class="st">&quot;</span>, df_test[df_test.duplicated()])</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true"></a><span class="co"># Print count of missing values in train set</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true"></a>train_df_missing <span class="op">=</span> df_train.isnull().<span class="bu">sum</span>()</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Count of missing values in training set:</span><span class="ch">\n</span><span class="st">&quot;</span>, train_df_missing)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true"></a><span class="co"># Print count of missing values in test set</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true"></a>test_df_missing <span class="op">=</span> df_test.isnull().<span class="bu">sum</span>()</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Count of missing values in test set:</span><span class="ch">\n</span><span class="st">&quot;</span>, test_df_missing)</span></code></pre></div>
<pre><code>Duplicates in training set:
 Empty DataFrame
Columns: [Filename, Class, Width, Height, Blue_mean, Green_mean, Red_mean, Blue_std, Green_std, Red_std, Sharpness]
Index: []

Duplicates in test set:
 Empty DataFrame
Columns: [Filename, Class, Width, Height, Blue_mean, Green_mean, Red_mean, Blue_std, Green_std, Red_std, Sharpness]
Index: []

Count of missing values in training set:
 Filename      0
Class         0
Width         0
Height        0
Blue_mean     0
Green_mean    0
Red_mean      0
Blue_std      0
Green_std     0
Red_std       0
Sharpness     0
dtype: int64

Count of missing values in test set:
 Filename      0
Class         0
Width         0
Height        0
Blue_mean     0
Green_mean    0
Red_mean      0
Blue_std      0
Green_std     0
Red_std       0
Sharpness     0
dtype: int64</code></pre>
<p>This confirms that our data has no duplicates or missing values. Phew! Although we avoided it this time, it’s theoretically possible for our data to include duplicates or even missing values. Therefore, for generalizability’s sake, we’ll include some steps below to remove duplicates and/or missing values, since they could bias our model’s predictions.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true"></a><span class="co"># Drop rows with missing values from both our train and test set</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true"></a>df_train <span class="op">=</span> df_train.dropna()</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true"></a>df_test <span class="op">=</span> df_test.dropna()</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true"></a><span class="co"># Drop duplicate rows from both our train and test set</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true"></a>df_train <span class="op">=</span> df_train.drop_duplicates()</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true"></a>df_test <span class="op">=</span> df_test.drop_duplicates()</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true"></a><span class="co"># Print duplicate rows from training set</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;Duplicates in training set (after cleaning):</span><span class="ch">\n</span><span class="st">&quot;</span>, df_train[df_train.duplicated()])</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true"></a></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true"></a><span class="co"># Print duplicate rows from test set</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Duplicates in test set (after cleaning):</span><span class="ch">\n</span><span class="st">&quot;</span>, df_test[df_test.duplicated()])</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true"></a></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true"></a><span class="co"># Print count of missing values in train set</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true"></a>train_df_missing <span class="op">=</span> df_train.isnull().<span class="bu">sum</span>()</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Count of missing values in training set (after cleaning):</span><span class="ch">\n</span><span class="st">&quot;</span>, train_df_missing)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true"></a></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true"></a><span class="co"># Print count of missing values in test set</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true"></a>test_df_missing <span class="op">=</span> df_test.isnull().<span class="bu">sum</span>()</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Count of missing values in test set (after cleaning):</span><span class="ch">\n</span><span class="st">&quot;</span>, test_df_missing)</span></code></pre></div>
<pre><code>Duplicates in training set (after cleaning):
 Empty DataFrame
Columns: [Filename, Class, Width, Height, Blue_mean, Green_mean, Red_mean, Blue_std, Green_std, Red_std, Sharpness]
Index: []

Duplicates in test set (after cleaning):
 Empty DataFrame
Columns: [Filename, Class, Width, Height, Blue_mean, Green_mean, Red_mean, Blue_std, Green_std, Red_std, Sharpness]
Index: []

Count of missing values in training set (after cleaning):
 Filename      0
Class         0
Width         0
Height        0
Blue_mean     0
Green_mean    0
Red_mean      0
Blue_std      0
Green_std     0
Red_std       0
Sharpness     0
dtype: int64

Count of missing values in test set (after cleaning):
 Filename      0
Class         0
Width         0
Height        0
Blue_mean     0
Green_mean    0
Red_mean      0
Blue_std      0
Green_std     0
Red_std       0
Sharpness     0
dtype: int64</code></pre>
<h2 id="visualization-exploratory-analysis">Visualization &amp; Exploratory Analysis</h2>
<p>Visualizing our data in various ways can often help us understand the unique characteristics of our data, most crucially how it is distributed and interrelated. Performing any statistically valid analysis requires that we understand these characteristics first, to avoid statistical errors or invalid conclusions.</p>
<h3 id="hypothesis-1-color-class-relationship">Hypothesis 1: Color-Class Relationship</h3>
<p>Below, we construct a boxplot and a histogram to visualize how colors vary between our benign and malignant images. We also perform some quick statistical tests to check whether the differences in color values between benign and malignant images are statistically significant (i.e., if the difference(s) are unlikely to be due to random variation).</p>
<p>In each of these tests, our <strong>Null Hypothesis</strong> is that there is no significant difference in average color value between the benign and malignant groups. Conversely, our <strong>Alternative Hypothesis</strong> is that there <em>is</em> a significant difference in average color value between the benign and malignant groups.</p>
<p>Finally, we create a scatter plot comparing color values to image sharpness, to evaluate whether this may be a confounding variable in the relationship between color value and benign/malignant nature of melanoma.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true"></a><span class="co"># Combine both datasets (train and test) for visualization &amp; exploratory analysis</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true"></a>full_df <span class="op">=</span> pd.concat([df_train, df_test], axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true"></a><span class="co"># Create figure on which to plot &amp; compare mean color data</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">18</span>,<span class="dv">10</span>))</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true"></a><span class="co"># Build boxplot to compare color means by class</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true"></a>sns.boxplot(data <span class="op">=</span> full_df, x <span class="op">=</span> <span class="st">&#39;Class&#39;</span>, y <span class="op">=</span> <span class="st">&#39;Red_mean&#39;</span>, hue <span class="op">=</span> <span class="st">&#39;Class&#39;</span>, palette<span class="op">=</span><span class="st">&#39;Reds_r&#39;</span>)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true"></a></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true"></a><span class="co"># Set plot title and labels</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true"></a>plt.title(<span class="st">&#39;Distribution of Mean Red Intensity&#39;</span>)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true"></a>plt.ylabel(<span class="st">&#39;Average Red Intensity of Pixels&#39;</span>)</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true"></a>plt.xlabel(<span class="st">&#39;Class&#39;</span>)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true"></a></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true"></a><span class="co"># Reveal plot</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true"></a>plt.show()</span></code></pre></div>
<figure>
<img src="https://theonlineconverter.com/uploads/Checkpoint_3_1765927044_files/Checkpoint_3_1765927044_25_0.png" alt="" /><figcaption>png</figcaption>
</figure>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true"></a><span class="co"># Create figure on which to plot color mean against sharpness</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">18</span>,<span class="dv">10</span>))</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true"></a><span class="co"># Build scatterplot of color mean vs sharpness</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true"></a>sns.scatterplot(data<span class="op">=</span>full_df, x<span class="op">=</span><span class="st">&#39;Red_mean&#39;</span>, y<span class="op">=</span><span class="st">&#39;Sharpness&#39;</span>, hue <span class="op">=</span> <span class="st">&#39;Class&#39;</span>, palette <span class="op">=</span> <span class="st">&#39;Reds&#39;</span>)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true"></a></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true"></a><span class="co"># Set plot title and labels</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true"></a>plt.title(<span class="st">&#39;Image Red Mean vs Sharpness&#39;</span>)</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true"></a>plt.xlabel(<span class="st">&#39;Red Mean&#39;</span>)</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true"></a>plt.ylabel(<span class="st">&#39;Sharpness&#39;</span>)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true"></a></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true"></a><span class="co"># Reveal plot</span></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true"></a>plt.show()</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true"></a></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true"></a><span class="co"># Perform t-test of color means between train benign and train malignant</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true"></a>t_stat, p_val <span class="op">=</span> ttest_ind(full_df.loc[df_train[<span class="st">&#39;Class&#39;</span>] <span class="op">==</span> <span class="st">&#39;benign&#39;</span>, <span class="ss">f&#39;Red_mean&#39;</span>], <span class="op">\</span></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true"></a>                                                     full_df.loc[df_train[<span class="st">&#39;Class&#39;</span>] <span class="op">==</span> <span class="st">&#39;malignant&#39;</span>, <span class="ss">f&#39;Red_mean&#39;</span>])</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true"></a></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true"></a><span class="co"># Print results of t-test</span></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Test statistic: </span><span class="sc">{</span>t_stat<span class="sc">}</span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;P-value: </span><span class="sc">{</span>p_val<span class="sc">}</span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true"></a></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true"></a><span class="co"># Print &quot;difference statistically significant!&quot; if p-value less than alpha (0.05)</span></span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true"></a><span class="cf">if</span> p_val <span class="op">&lt;</span> <span class="fl">0.05</span>:</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true"></a>  <span class="bu">print</span>(<span class="st">&quot;Red mean difference statistically significant!</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true"></a></span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true"></a><span class="co"># Print &quot;difference not statistically significant&quot; if p-value greater than alpha (0.05)</span></span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true"></a><span class="cf">else</span>:</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true"></a>  <span class="bu">print</span>(<span class="st">&quot;Red mean difference not statistically significant.</span><span class="ch">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<figure>
<img src="https://theonlineconverter.com/uploads/Checkpoint_3_1765927044_files/Checkpoint_3_1765927044_26_0.png" alt="" /><figcaption>png</figcaption>
</figure>
<pre><code>Test statistic: 61.118310327229736

P-value: 0.0

Red mean difference statistically significant!</code></pre>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true"></a><span class="co"># Create figure on which to plot &amp; compare mean color data</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">18</span>,<span class="dv">10</span>))</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true"></a><span class="co"># Build boxplot to compare color means by class</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true"></a>sns.boxplot(data <span class="op">=</span> full_df, x <span class="op">=</span> <span class="st">&#39;Class&#39;</span>, y <span class="op">=</span> <span class="st">&#39;Green_mean&#39;</span>, hue <span class="op">=</span> <span class="st">&#39;Class&#39;</span>, palette<span class="op">=</span><span class="st">&#39;Greens_r&#39;</span>)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true"></a><span class="co"># Set plot title and labels</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true"></a>plt.title(<span class="st">&#39;Distribution of Mean Green Intensity&#39;</span>)</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true"></a>plt.ylabel(<span class="st">&#39;Average Green Intensity of Pixels&#39;</span>)</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true"></a>plt.xlabel(<span class="st">&#39;Class&#39;</span>)</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true"></a></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true"></a><span class="co"># Reveal plot</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true"></a>plt.show()</span></code></pre></div>
<figure>
<img src="https://theonlineconverter.com/uploads/Checkpoint_3_1765927044_files/Checkpoint_3_1765927044_27_0.png" alt="" /><figcaption>png</figcaption>
</figure>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true"></a><span class="co"># Create figure on which to plot color mean against sharpness</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">18</span>,<span class="dv">10</span>))</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true"></a><span class="co"># Build scatterplot of color mean vs sharpness</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true"></a>sns.scatterplot(data<span class="op">=</span>full_df, x<span class="op">=</span><span class="st">&#39;Green_mean&#39;</span>, y<span class="op">=</span><span class="st">&#39;Sharpness&#39;</span>, hue <span class="op">=</span> <span class="st">&#39;Class&#39;</span>, palette <span class="op">=</span> <span class="st">&#39;Greens&#39;</span>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true"></a><span class="co"># Set plot title and labels</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true"></a>plt.title(<span class="st">&#39;Image Green Mean vs Sharpness&#39;</span>)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true"></a>plt.xlabel(<span class="st">&#39;Green Mean&#39;</span>)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true"></a>plt.ylabel(<span class="st">&#39;Sharpness&#39;</span>)</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true"></a></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true"></a><span class="co"># Reveal plot</span></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true"></a>plt.show()</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true"></a></span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true"></a><span class="co"># Perform t-test of color means between train benign and train malignant</span></span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true"></a>t_stat, p_val <span class="op">=</span> ttest_ind(full_df.loc[df_train[<span class="st">&#39;Class&#39;</span>] <span class="op">==</span> <span class="st">&#39;benign&#39;</span>, <span class="st">&#39;Green_mean&#39;</span>], <span class="op">\</span></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true"></a>                                                     full_df.loc[df_train[<span class="st">&#39;Class&#39;</span>] <span class="op">==</span> <span class="st">&#39;malignant&#39;</span>, <span class="st">&#39;Green_mean&#39;</span>])</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true"></a></span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true"></a><span class="co"># Print results of t-test</span></span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Test statistic: </span><span class="sc">{</span>t_stat<span class="sc">}</span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;P-value: </span><span class="sc">{</span>p_val<span class="sc">}</span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true"></a></span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true"></a><span class="co"># Print &quot;difference statistically significant!&quot; if p-value less than alpha (0.05)</span></span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true"></a><span class="cf">if</span> p_val <span class="op">&lt;</span> <span class="fl">0.05</span>:</span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true"></a>  <span class="bu">print</span>(<span class="st">&quot;Green mean difference statistically significant!</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true"></a></span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true"></a><span class="co"># Print &quot;difference not statistically significant&quot; if p-value greater than alpha (0.05)</span></span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true"></a><span class="cf">else</span>:</span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true"></a>  <span class="bu">print</span>(<span class="st">&quot;Green mean difference not statistically significant.</span><span class="ch">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<figure>
<img src="https://theonlineconverter.com/uploads/Checkpoint_3_1765927044_files/Checkpoint_3_1765927044_28_0.png" alt="" /><figcaption>png</figcaption>
</figure>
<pre><code>Test statistic: 39.10226305965332

P-value: 0.0

Green mean difference statistically significant!</code></pre>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true"></a><span class="co"># Create figure on which to plot &amp; compare mean color data</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">18</span>,<span class="dv">10</span>))</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true"></a><span class="co"># Build boxplot to compare color means by class</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true"></a>sns.boxplot(data <span class="op">=</span> full_df, x <span class="op">=</span> <span class="st">&#39;Class&#39;</span>, y <span class="op">=</span> <span class="st">&#39;Blue_mean&#39;</span>, hue <span class="op">=</span> <span class="st">&#39;Class&#39;</span>, palette<span class="op">=</span><span class="st">&#39;Blues_r&#39;</span>)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true"></a></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true"></a><span class="co"># Set plot title and labels</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true"></a>plt.title(<span class="st">&#39;Distribution of Mean Blue Intensity&#39;</span>)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true"></a>plt.ylabel(<span class="st">&#39;Average Blue Intensity of Pixels&#39;</span>)</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true"></a>plt.xlabel(<span class="st">&#39;Class&#39;</span>)</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true"></a></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true"></a><span class="co"># Reveal plot</span></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true"></a>plt.show()</span></code></pre></div>
<figure>
<img src="https://theonlineconverter.com/uploads/Checkpoint_3_1765927044_files/Checkpoint_3_1765927044_29_0.png" alt="" /><figcaption>png</figcaption>
</figure>
<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true"></a><span class="co"># Create figure on which to plot color mean against sharpness</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">18</span>,<span class="dv">10</span>))</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true"></a><span class="co"># Build scatterplot of color mean vs sharpness</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true"></a>sns.scatterplot(data<span class="op">=</span>full_df, x<span class="op">=</span><span class="st">&#39;Blue_mean&#39;</span>, y<span class="op">=</span><span class="st">&#39;Sharpness&#39;</span>, hue <span class="op">=</span> <span class="st">&#39;Class&#39;</span>, palette <span class="op">=</span> <span class="st">&#39;Blues&#39;</span>)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true"></a><span class="co"># Set plot title and labels</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true"></a>plt.title(<span class="st">&#39;Image Blue Mean vs Sharpness&#39;</span>)</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true"></a>plt.xlabel(<span class="st">&#39;Blue Mean&#39;</span>)</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true"></a>plt.ylabel(<span class="st">&#39;Sharpness&#39;</span>)</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true"></a></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true"></a><span class="co"># Reveal plot</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true"></a>plt.show()</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true"></a></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true"></a><span class="co"># Perform t-test of color means between train benign and train malignant</span></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true"></a>t_stat, p_val <span class="op">=</span> ttest_ind(full_df.loc[df_train[<span class="st">&#39;Class&#39;</span>] <span class="op">==</span> <span class="st">&#39;benign&#39;</span>, <span class="st">&#39;Blue_mean&#39;</span>], <span class="op">\</span></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true"></a>                                                     full_df.loc[df_train[<span class="st">&#39;Class&#39;</span>] <span class="op">==</span> <span class="st">&#39;malignant&#39;</span>, <span class="st">&#39;Blue_mean&#39;</span>])</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true"></a></span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true"></a><span class="co"># Print results of t-test</span></span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Test statistic: </span><span class="sc">{</span>t_stat<span class="sc">}</span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;P-value: </span><span class="sc">{</span>p_val<span class="sc">}</span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true"></a></span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true"></a><span class="co"># Print &quot;difference statistically significant!&quot; if p-value less than alpha (0.05)</span></span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true"></a><span class="cf">if</span> p_val <span class="op">&lt;</span> <span class="fl">0.05</span>:</span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true"></a>  <span class="bu">print</span>(<span class="st">&quot;Blue mean difference statistically significant!</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true"></a></span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true"></a><span class="co"># Print &quot;difference not statistically significant&quot; if p-value greater than alpha (0.05)</span></span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true"></a><span class="cf">else</span>:</span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true"></a>  <span class="bu">print</span>(<span class="st">&quot;Blue mean difference not statistically significant.</span><span class="ch">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<figure>
<img src="https://theonlineconverter.com/uploads/Checkpoint_3_1765927044_files/Checkpoint_3_1765927044_30_0.png" alt="" /><figcaption>png</figcaption>
</figure>
<pre><code>Test statistic: 31.19322451584223

P-value: 1.8973716174060774e-204

Blue mean difference statistically significant!</code></pre>
<p>Fascinating! According to our hypothesis tests, there is a significant difference in average color value for <em>each color</em> between benign and malignant melanoma. Therefore, we can <strong>reject our Null Hypothesis!</strong></p>
<p>Since our visualizations clearly show a higher mean value for each color in the benign group, it appears that each color may be negatively correlated with melanoma’s malignancy. It also appears from our scatterplots that image sharpness does not correlate with any of the colors, meaning that quality of color capture likely does not affect the relationship between average color values and benign/malignant status. In other words, it would seem that the difference in color values between benign and malignant melanoma cannot be blamed on image quality.</p>
<p>We’ve now learned that color could be a powerful prediction factor for our machine learning model!</p>
<h3 id="hypothesis-2-intra-color-correlation">Hypothesis 2: Intra-Color Correlation</h3>
<p>Before we get too excited, it’s also important to check the correlation between the three colors. Since we only performed pairwise comparisons between the benign and malignant groups for each color individually, it’s possible that the color values are <em>correlated with each other</em>. If true, this would mean the two other colors are confounding the relationship we see between benignness and a given color.</p>
<p>Since we have already used a t-test in this checkpoint, below we’ll create granular bins for each color value and perform a chi-square test instead.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true"></a><span class="co"># Create dataframe to hold granular color bins</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true"></a>bin_df <span class="op">=</span> pd.DataFrame()</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true"></a><span class="co"># Iterate over all three color channels</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true"></a><span class="cf">for</span> color <span class="kw">in</span> [<span class="st">&#39;Red&#39;</span>, <span class="st">&#39;Green&#39;</span>, <span class="st">&#39;Blue&#39;</span>]:</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true"></a></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true"></a>  <span class="co"># Create bin series</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true"></a>  bin_df[<span class="ss">f&#39;</span><span class="sc">{</span>color<span class="sc">}</span><span class="ss">_bin&#39;</span>] <span class="op">=</span> pd.cut(full_df[<span class="ss">f&#39;</span><span class="sc">{</span>color<span class="sc">}</span><span class="ss">_mean&#39;</span>], bins <span class="op">=</span> <span class="dv">100</span>)</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true"></a></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true"></a><span class="co"># Create contingency table</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true"></a>contingency_table <span class="op">=</span> pd.crosstab([bin_df[<span class="st">&#39;Red_bin&#39;</span>], bin_df[<span class="st">&#39;Green_bin&#39;</span>], bin_df[<span class="st">&#39;Blue_bin&#39;</span>]], columns <span class="op">=</span> <span class="st">&#39;count&#39;</span>)</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true"></a></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true"></a><span class="co"># Perform chi-square test and extract test statistics</span></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true"></a>chi2, p_val, dof, expected <span class="op">=</span> chi2_contingency(contingency_table)</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true"></a></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true"></a><span class="co"># Print test statistics</span></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;Chi-square statistic: </span><span class="sc">{</span>chi2<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">P-value: </span><span class="sc">{</span>p_val<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Degrees of Freedom: </span><span class="sc">{</span>dof<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Expected Frequencies: </span><span class="sc">{</span>expected<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true"></a></span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true"></a><span class="co"># If p-value below our chosen alpha (0.05), notify user of significance</span></span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true"></a><span class="cf">if</span> p_val <span class="op">&lt;</span> <span class="fl">0.05</span>:</span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true"></a>  <span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Color bins statistically correlated!&quot;</span>)</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true"></a></span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true"></a><span class="co"># If p-value above our chosen alpha (0.05), notify user of insignificance</span></span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true"></a><span class="cf">else</span>:</span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true"></a>  <span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Color bins not statistically correlated.&quot;</span>)</span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true"></a></span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true"></a><span class="co"># Create figure on which to plot correlation matrix</span></span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">18</span>,<span class="dv">5</span>))</span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true"></a></span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true"></a><span class="co"># Plot correlation matrix of numeric features of full dataset as heatmap</span></span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true"></a>sns.heatmap(contingency_table, annot <span class="op">=</span> <span class="va">True</span>, cmap <span class="op">=</span> <span class="st">&#39;inferno&#39;</span>)</span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true"></a></span>
<span id="cb31-36"><a href="#cb31-36" aria-hidden="true"></a><span class="co"># Set plot title and labels</span></span>
<span id="cb31-37"><a href="#cb31-37" aria-hidden="true"></a>plt.title(<span class="st">&#39;Heatmap of Color Bin Contingency Table&#39;</span>)</span>
<span id="cb31-38"><a href="#cb31-38" aria-hidden="true"></a>plt.xlabel(<span class="st">&#39;Color Bin&#39;</span>)</span>
<span id="cb31-39"><a href="#cb31-39" aria-hidden="true"></a>plt.ylabel(<span class="st">&#39;Color Bin&#39;</span>)</span>
<span id="cb31-40"><a href="#cb31-40" aria-hidden="true"></a></span>
<span id="cb31-41"><a href="#cb31-41" aria-hidden="true"></a><span class="co"># Reveal plot</span></span>
<span id="cb31-42"><a href="#cb31-42" aria-hidden="true"></a>plt.show()</span>
<span id="cb31-43"><a href="#cb31-43" aria-hidden="true"></a></span></code></pre></div>
<pre><code>Chi-square statistic: 0.0

P-value: 1.0

Degrees of Freedom: 0

Expected Frequencies: [[1.]
 [1.]
 [1.]
 ...
 [1.]
 [1.]
 [1.]]

Color bins not statistically correlated.</code></pre>
<figure>
<img src="https://theonlineconverter.com/uploads/Checkpoint_3_1765927044_files/Checkpoint_3_1765927044_33_1.png" alt="" /><figcaption>png</figcaption>
</figure>
<p>Since the color values do not appear to be correlated, this indicates they are not <em>multicollinear</em>, validating our conclusion in test 1.</p>
<h2 id="hypothesis-3-color-equity-between-train-and-test">Hypothesis 3: Color Equity between Train and Test</h2>
<p>Based on our previous two tests, it appears that each color channel may be a powerful predictive feature for our machine learning model. However, a crucial assumption we must validate is that that this difference is not just a unique quirk of our train dataset. For this apparent correlation to be useful information, it must hold for the general population as well, which in this case is represented by our test set.</p>
<p>Below, we perform a One-Way Analysis of Variance (ANOVA) test to detect whether there is a significant difference in each of the color means between these four groups. If this is the case, the chances of this difference proving useful to our convolutional neural network grow even more likely.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true"></a><span class="co"># Separate data by set and class</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true"></a>df_train_benign <span class="op">=</span> df_train[df_train[<span class="st">&#39;Class&#39;</span>] <span class="op">==</span> <span class="st">&#39;benign&#39;</span>]</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true"></a>df_train_malignant <span class="op">=</span> df_train[df_train[<span class="st">&#39;Class&#39;</span>] <span class="op">==</span> <span class="st">&#39;malignant&#39;</span>]</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true"></a>df_test_benign <span class="op">=</span> df_test[df_test[<span class="st">&#39;Class&#39;</span>] <span class="op">==</span> <span class="st">&#39;benign&#39;</span>]</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true"></a>df_test_malignant <span class="op">=</span> df_test[df_test[<span class="st">&#39;Class&#39;</span>] <span class="op">==</span> <span class="st">&#39;malignant&#39;</span>]</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true"></a></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true"></a><span class="co"># Iterate over all three RGB channels</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true"></a><span class="cf">for</span> color <span class="kw">in</span> [<span class="st">&#39;Red&#39;</span>, <span class="st">&#39;Green&#39;</span>, <span class="st">&#39;Blue&#39;</span>]:</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true"></a></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true"></a>  <span class="co"># Perform one-way ANOVA test</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true"></a>  t_stat, p_val <span class="op">=</span> stats.f_oneway(df_train_benign[<span class="ss">f&#39;</span><span class="sc">{</span>color<span class="sc">}</span><span class="ss">_mean&#39;</span>], df_train_malignant[<span class="ss">f&#39;</span><span class="sc">{</span>color<span class="sc">}</span><span class="ss">_mean&#39;</span>], <span class="op">\</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true"></a>                                 df_test_benign[<span class="ss">f&#39;</span><span class="sc">{</span>color<span class="sc">}</span><span class="ss">_mean&#39;</span>], df_test_malignant[<span class="ss">f&#39;</span><span class="sc">{</span>color<span class="sc">}</span><span class="ss">_mean&#39;</span>])</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true"></a>  <span class="co"># Print test statistics</span></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true"></a>  <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="sc">{</span>color<span class="sc">}</span><span class="ss"> test statistic: </span><span class="sc">{</span>t_stat<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true"></a>  <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="sc">{</span>color<span class="sc">}</span><span class="ss"> p-value: </span><span class="sc">{</span>p_val<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true"></a></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true"></a>  <span class="co"># If p-value below our chosen alpha (0.05), notify user of statistical significance</span></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true"></a>  <span class="cf">if</span> p_val <span class="op">&lt;</span> <span class="fl">0.05</span>:</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="sc">{</span>color<span class="sc">}</span><span class="ss"> difference statistically significant!&quot;</span>)</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true"></a></span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true"></a>  <span class="co"># If p-value above our chosen alpha (0.05), notify user of statistical insignificance</span></span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true"></a>  <span class="cf">else</span>:</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="sc">{</span>color<span class="sc">}</span><span class="ss"> difference not statistically significant.&quot;</span>)</span></code></pre></div>
<pre><code>Red test statistic: 1830.0061692283537

Red p-value: 0.0

Red difference statistically significant!

Green test statistic: 689.3463615360778

Green p-value: 0.0

Green difference statistically significant!

Blue test statistic: 465.4326322362371

Blue p-value: 4.140098571599196e-284

Blue difference statistically significant!</code></pre>
<p>Fascinating again! We found a statistically significant difference in colors between the benign test, benign train, malignant test, and malignant train groups. This means we can <strong>reject our null hypothesis</strong>, and conclude that it is very unlikely that the difference in colorfulness in our groups is attributable to random variation.</p>
<p>For easier comparison, see the histograms of colorfulness below:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true"></a><span class="co"># Create figure on which to plot &amp; compare mean color data</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">18</span>,<span class="dv">10</span>))</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true"></a><span class="co"># Build histogram to compare color means by training class</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true"></a>sns.histplot(data<span class="op">=</span>full_df, x <span class="op">=</span> <span class="st">&#39;Class&#39;</span>, y <span class="op">=</span> <span class="st">&#39;Red_mean&#39;</span>, hue <span class="op">=</span> <span class="st">&#39;Class&#39;</span>, palette <span class="op">=</span> <span class="st">&#39;Reds_r&#39;</span>)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true"></a></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true"></a><span class="co"># Set plot title and labels</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true"></a>plt.title(<span class="st">&#39;Intensity of Red by Class&#39;</span>)</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true"></a>plt.xlabel(<span class="st">&#39;Class&#39;</span>)</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true"></a></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true"></a><span class="co"># Reveal plot</span></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true"></a>plt.show()</span></code></pre></div>
<figure>
<img src="https://theonlineconverter.com/uploads/Checkpoint_3_1765927044_files/Checkpoint_3_1765927044_38_0.png" alt="" /><figcaption>png</figcaption>
</figure>
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true"></a><span class="co"># Create figure on which to plot &amp; compare mean color data</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">18</span>,<span class="dv">10</span>))</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true"></a><span class="co"># Build histogram to compare color means by training class</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true"></a>sns.histplot(data<span class="op">=</span>full_df, x <span class="op">=</span> <span class="st">&#39;Class&#39;</span>, y <span class="op">=</span> <span class="st">&#39;Green_mean&#39;</span>, hue <span class="op">=</span> <span class="st">&#39;Class&#39;</span>, palette <span class="op">=</span> <span class="st">&#39;Greens_r&#39;</span>)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true"></a></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true"></a><span class="co"># Set plot title and labels</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true"></a>plt.title(<span class="st">&#39;Intensity of Green by Class&#39;</span>)</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true"></a>plt.xlabel(<span class="st">&#39;Class&#39;</span>)</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true"></a></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true"></a><span class="co"># Reveal plot</span></span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true"></a>plt.show()</span></code></pre></div>
<figure>
<img src="https://theonlineconverter.com/uploads/Checkpoint_3_1765927044_files/Checkpoint_3_1765927044_39_0.png" alt="" /><figcaption>png</figcaption>
</figure>
<div class="sourceCode" id="cb37"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true"></a><span class="co"># Create figure on which to plot &amp; compare mean color data</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">18</span>,<span class="dv">10</span>))</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true"></a></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true"></a><span class="co"># Build histogram to compare color means by training class</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true"></a>sns.histplot(data<span class="op">=</span>full_df, x <span class="op">=</span> <span class="st">&#39;Class&#39;</span>, y <span class="op">=</span> <span class="st">&#39;Blue_mean&#39;</span>, hue <span class="op">=</span> <span class="st">&#39;Class&#39;</span>, palette <span class="op">=</span> <span class="st">&#39;Blues_r&#39;</span>)</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true"></a></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true"></a><span class="co"># Set plot title and labels</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true"></a>plt.title(<span class="st">&#39;Intensity of Blue by Class&#39;</span>)</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true"></a>plt.xlabel(<span class="st">&#39;Class&#39;</span>)</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true"></a></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true"></a><span class="co"># Reveal plot</span></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true"></a>plt.show()</span></code></pre></div>
<figure>
<img src="https://theonlineconverter.com/uploads/Checkpoint_3_1765927044_files/Checkpoint_3_1765927044_40_0.png" alt="" /><figcaption>png</figcaption>
</figure>
<h2 id="dataset-augmentation">Dataset Augmentation</h2>
<p>Although our training dataset of 10,000 images may appear quite large, similar projects have required hundreds of thousands or even millions of images to attain a level of precision and accuracy appropriate for deployment in the medical field.</p>
<p>In order to approach this industry-class level of readiness, we need to artificially increase the size of our dataset. We will achieve this by “jittering”, or randomly modifying, the visual qualities of copies of images in our dataset, in order to simulate the diverse medical imaging conditions our model may encounter in the real world. We will also rotate our images to ensure that our model does not overfit to a particular orientation of lesion, increasing our model’s accuracy in real-world scenarios.</p>
<p>Below, we’ve written some functions that we’ll use later while training our model to augment our training data.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true"></a><span class="co"># Define function to jitter our images</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true"></a><span class="kw">def</span> color_jitter(image):</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true"></a></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true"></a>  <span class="co"># Create a copy of the image passed to the function</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true"></a>  img <span class="op">=</span> image.copy()</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true"></a></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true"></a>  <span class="co"># Randomly enhance image brightness</span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true"></a>  img <span class="op">=</span> ImageEnhance.Brightness(img).enhance(np.random.uniform(<span class="fl">0.8</span>, <span class="fl">1.2</span>))</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true"></a></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true"></a>  <span class="co"># Randomly enhance image contrast</span></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true"></a>  img <span class="op">=</span> ImageEnhance.Contrast(img).enhance(np.random.uniform(<span class="fl">0.8</span>, <span class="fl">1.2</span>))</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true"></a></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true"></a>  <span class="co"># Randomly enhance image color</span></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true"></a>  img <span class="op">=</span> ImageEnhance.Color(img).enhance(np.random.uniform(<span class="fl">0.8</span>, <span class="fl">1.2</span>))</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true"></a></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true"></a>  <span class="co"># Randomly enhance image sharpness</span></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true"></a>  img <span class="op">=</span> ImageEnhance.Sharpness(img).enhance(np.random.uniform(<span class="fl">0.8</span>, <span class="fl">1.2</span>))</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true"></a></span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true"></a>  <span class="co"># Return enhanced image</span></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true"></a>  <span class="cf">return</span> img</span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true"></a></span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true"></a><span class="co"># Define a function to rotate image</span></span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true"></a><span class="kw">def</span> rotate_image(image, angle):</span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true"></a></span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true"></a>  <span class="co"># Return rotated image</span></span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true"></a>  <span class="cf">return</span> image.rotate(angle)</span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true"></a></span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true"></a><span class="co"># Define a function to augment our train set</span></span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true"></a><span class="kw">def</span> augment_fold_train_set(dataset, indices, output_dir, jitters <span class="op">=</span> <span class="dv">2</span>):</span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true"></a></span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true"></a>  <span class="co"># Create a directory to hold our augmented train images</span></span>
<span id="cb38-32"><a href="#cb38-32" aria-hidden="true"></a>  os.makedirs(output_dir, exist_ok <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb38-33"><a href="#cb38-33" aria-hidden="true"></a></span>
<span id="cb38-34"><a href="#cb38-34" aria-hidden="true"></a>  <span class="co"># Iterate over all indices passed</span></span>
<span id="cb38-35"><a href="#cb38-35" aria-hidden="true"></a>  <span class="cf">for</span> index <span class="kw">in</span> tqdm(indices, desc <span class="op">=</span> <span class="st">&quot;Augmenting train set...&quot;</span>):</span>
<span id="cb38-36"><a href="#cb38-36" aria-hidden="true"></a></span>
<span id="cb38-37"><a href="#cb38-37" aria-hidden="true"></a>    <span class="co"># Extract filepath and class index for index of dataset</span></span>
<span id="cb38-38"><a href="#cb38-38" aria-hidden="true"></a>    path, class_index <span class="op">=</span> dataset.samples[index]</span>
<span id="cb38-39"><a href="#cb38-39" aria-hidden="true"></a></span>
<span id="cb38-40"><a href="#cb38-40" aria-hidden="true"></a>    <span class="co"># Extract class name based on class index</span></span>
<span id="cb38-41"><a href="#cb38-41" aria-hidden="true"></a>    cls <span class="op">=</span> dataset.classes[class_index]</span>
<span id="cb38-42"><a href="#cb38-42" aria-hidden="true"></a></span>
<span id="cb38-43"><a href="#cb38-43" aria-hidden="true"></a>    <span class="co"># Extract image at path, ensuring RGB colorscale for safety</span></span>
<span id="cb38-44"><a href="#cb38-44" aria-hidden="true"></a>    img <span class="op">=</span> Image.<span class="bu">open</span>(path).convert(<span class="st">&#39;RGB&#39;</span>)</span>
<span id="cb38-45"><a href="#cb38-45" aria-hidden="true"></a></span>
<span id="cb38-46"><a href="#cb38-46" aria-hidden="true"></a>    <span class="co"># Extract base of image path</span></span>
<span id="cb38-47"><a href="#cb38-47" aria-hidden="true"></a>    base <span class="op">=</span> os.path.splitext(os.path.basename(path))[<span class="dv">0</span>]</span>
<span id="cb38-48"><a href="#cb38-48" aria-hidden="true"></a></span>
<span id="cb38-49"><a href="#cb38-49" aria-hidden="true"></a>    <span class="co"># Define file extension expected</span></span>
<span id="cb38-50"><a href="#cb38-50" aria-hidden="true"></a>    ext <span class="op">=</span> <span class="st">&#39;.jpg&#39;</span></span>
<span id="cb38-51"><a href="#cb38-51" aria-hidden="true"></a></span>
<span id="cb38-52"><a href="#cb38-52" aria-hidden="true"></a>    <span class="co"># Define path to class folder in output directory</span></span>
<span id="cb38-53"><a href="#cb38-53" aria-hidden="true"></a>    class_folder <span class="op">=</span> os.path.join(output_dir, cls)</span>
<span id="cb38-54"><a href="#cb38-54" aria-hidden="true"></a></span>
<span id="cb38-55"><a href="#cb38-55" aria-hidden="true"></a>    <span class="co"># Create class folder</span></span>
<span id="cb38-56"><a href="#cb38-56" aria-hidden="true"></a>    os.makedirs(class_folder, exist_ok <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb38-57"><a href="#cb38-57" aria-hidden="true"></a></span>
<span id="cb38-58"><a href="#cb38-58" aria-hidden="true"></a>    <span class="co"># Save original image in class folder</span></span>
<span id="cb38-59"><a href="#cb38-59" aria-hidden="true"></a>    img.save(os.path.join(class_folder, base <span class="op">+</span> <span class="st">&#39;OG&#39;</span> <span class="op">+</span> ext))</span>
<span id="cb38-60"><a href="#cb38-60" aria-hidden="true"></a></span>
<span id="cb38-61"><a href="#cb38-61" aria-hidden="true"></a>    <span class="co"># Iterate over rotation angles</span></span>
<span id="cb38-62"><a href="#cb38-62" aria-hidden="true"></a>    <span class="cf">for</span> angle <span class="kw">in</span> [<span class="dv">90</span>, <span class="dv">180</span>, <span class="dv">270</span>]:</span>
<span id="cb38-63"><a href="#cb38-63" aria-hidden="true"></a></span>
<span id="cb38-64"><a href="#cb38-64" aria-hidden="true"></a>      <span class="co"># Rotate original image</span></span>
<span id="cb38-65"><a href="#cb38-65" aria-hidden="true"></a>      rotated <span class="op">=</span> rotate_image(img, angle)</span>
<span id="cb38-66"><a href="#cb38-66" aria-hidden="true"></a></span>
<span id="cb38-67"><a href="#cb38-67" aria-hidden="true"></a>      <span class="co"># Save rotated original image</span></span>
<span id="cb38-68"><a href="#cb38-68" aria-hidden="true"></a>      rotated.save(os.path.join(class_folder, base <span class="op">+</span> <span class="ss">f&#39;rot</span><span class="sc">{</span>angle<span class="sc">}</span><span class="ss">&#39;</span> <span class="op">+</span> ext))</span>
<span id="cb38-69"><a href="#cb38-69" aria-hidden="true"></a></span>
<span id="cb38-70"><a href="#cb38-70" aria-hidden="true"></a>    <span class="co"># Iterate over specified number of jitters</span></span>
<span id="cb38-71"><a href="#cb38-71" aria-hidden="true"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(jitters):</span>
<span id="cb38-72"><a href="#cb38-72" aria-hidden="true"></a></span>
<span id="cb38-73"><a href="#cb38-73" aria-hidden="true"></a>      <span class="co"># Jitter image color, contrast, brightness and sharpness</span></span>
<span id="cb38-74"><a href="#cb38-74" aria-hidden="true"></a>      jittered <span class="op">=</span> color_jitter(img)</span>
<span id="cb38-75"><a href="#cb38-75" aria-hidden="true"></a></span>
<span id="cb38-76"><a href="#cb38-76" aria-hidden="true"></a>      <span class="co"># Save jittered image in class folder</span></span>
<span id="cb38-77"><a href="#cb38-77" aria-hidden="true"></a>      jittered.save(os.path.join(class_folder, base <span class="op">+</span> <span class="ss">f&#39;jitter</span><span class="sc">{j}</span><span class="ss">&#39;</span> <span class="op">+</span> ext))</span>
<span id="cb38-78"><a href="#cb38-78" aria-hidden="true"></a></span>
<span id="cb38-79"><a href="#cb38-79" aria-hidden="true"></a>      <span class="co"># Iterate over rotation angles</span></span>
<span id="cb38-80"><a href="#cb38-80" aria-hidden="true"></a>      <span class="cf">for</span> angle <span class="kw">in</span> [<span class="dv">90</span>, <span class="dv">180</span>, <span class="dv">270</span>]:</span>
<span id="cb38-81"><a href="#cb38-81" aria-hidden="true"></a></span>
<span id="cb38-82"><a href="#cb38-82" aria-hidden="true"></a>        <span class="co"># Rotate jittered image</span></span>
<span id="cb38-83"><a href="#cb38-83" aria-hidden="true"></a>        rotated <span class="op">=</span> rotate_image(jittered, angle)</span>
<span id="cb38-84"><a href="#cb38-84" aria-hidden="true"></a></span>
<span id="cb38-85"><a href="#cb38-85" aria-hidden="true"></a>        <span class="co"># Save rotated jittered image</span></span>
<span id="cb38-86"><a href="#cb38-86" aria-hidden="true"></a>        rotated.save(os.path.join(class_folder, base <span class="op">+</span> <span class="ss">f&#39;jitrot</span><span class="sc">{</span>angle<span class="sc">}</span><span class="ss">&#39;</span> <span class="op">+</span> ext))</span></code></pre></div>
<h2 id="convolutional-neural-network">Convolutional Neural Network</h2>
<p>Here is where the real fun starts!</p>
<p>Convolutional neural networks, or CNNs, are incredible machine learning architectures that leverage complex linear algebra and calculus to actually <em>learn</em> information from images. How does it do this? Essentially, it examines groups of pixels from right to left, top to bottom, to decide how much that group of pixels looks like what it’s trained to look for. The network has multiple convolutions (essentially layers) that build up from recognizing simple lines to shapes to groups of shapes to objects and finally to entire image contents.</p>
<p>We’ll see a bit more of how this works in the coming code. The main action takes place during training and testing, when our model will teach itself the characteristics of benign and malignant images using our train set, and then try to predict the class of the test set based on what it’s learned.</p>
<h3 id="experiment-tracking">Experiment Tracking</h3>
<p>Sounds simple, right? Well, like any software development process, success requires plenty of iterations and tweaks. One tool that will help us track our iterations (often called experiments in machine learning) is called <a href="https://wandb.ai/site/">Weights &amp; Biases</a>.</p>
<p>This tool will allow us to easily track and visualize a bunch of success metrics for our CNN as we tweak its <em>hyperparameters</em>. “Hyperparameters” is a big word that essentially means “the ways we configure how our CNN will learn.” Below, we’ll log into Weights and Biases and define a configuration that contains all the hyperparameters we’ll use for the current run.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true"></a><span class="co"># Log into Weights &amp; Biases for experiment tracking</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true"></a><span class="op">!</span>wandb login <span class="op">--</span>relogin</span></code></pre></div>
<pre><code>[34m[1mwandb[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&amp;B server locally: https://wandb.me/wandb-server)
[34m[1mwandb[0m: Find your API key here: https://wandb.ai/authorize?ref=models
[34m[1mwandb[0m: Paste an API key from your profile and hit enter: 
[34m[1mwandb[0m: No netrc file found, creating one.
[34m[1mwandb[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc
[34m[1mwandb[0m: W&amp;B API key is configured. Use [1m`wandb login --relogin`[0m to force relogin</code></pre>
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true"></a><span class="co"># Initialize Weights &amp; Biases tracking of our project</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true"></a><span class="co"># and define hyperparameters of current run</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true"></a>wandb.init(</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true"></a>    project<span class="op">=</span><span class="st">&#39;melanoma-classifier&#39;</span>,</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true"></a>    config<span class="op">=</span>{</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true"></a>        <span class="st">&#39;base&#39;</span>: <span class="st">&#39;EfficientNetV2B0&#39;</span>,</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true"></a>        <span class="st">&#39;img_size&#39;</span>: <span class="dv">256</span>,</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true"></a>        <span class="st">&#39;batch_size&#39;</span>: <span class="dv">16</span>,</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true"></a>        <span class="st">&#39;learn_rate&#39;</span>: <span class="fl">0.0001</span>,</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true"></a>        <span class="st">&#39;dropout_ratio&#39;</span>: <span class="fl">0.5</span>,</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true"></a>        <span class="st">&#39;dense_units&#39;</span>: <span class="dv">512</span>,</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true"></a>        <span class="st">&#39;epochs&#39;</span>: <span class="dv">30</span>,</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true"></a>        <span class="st">&#39;finetune_after&#39;</span>: <span class="dv">8</span>,</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true"></a>        <span class="st">&#39;unfreeze_pct&#39;</span>: <span class="fl">0.25</span>,</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true"></a>        <span class="st">&#39;k&#39;</span>: <span class="dv">5</span>,</span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true"></a>        <span class="st">&#39;patience_steps&#39;</span>: <span class="dv">6</span>,</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true"></a>        <span class="st">&#39;weight_decay&#39;</span>: <span class="fl">1e-4</span>,</span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true"></a>        <span class="st">&#39;grad_clip&#39;</span>: <span class="fl">1.0</span>,</span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true"></a>        <span class="st">&#39;warmup_epochs&#39;</span>: <span class="dv">3</span>,</span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true"></a>        <span class="st">&#39;optimizer&#39;</span>: <span class="st">&#39;AdamW&#39;</span></span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true"></a>    }</span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true"></a>)</span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true"></a></span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true"></a><span class="co"># Define a variable to hold our config for easy referencing</span></span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true"></a>config <span class="op">=</span> wandb.config</span></code></pre></div>
<pre><code>/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence &#39;\/&#39;
  | |_| | &#39;_ \/ _` / _` |  _/ -_)
[34m[1mwandb[0m: Currently logged in as: [33mjkelle11[0m ([33mjkelle11-university-of-maryland[0m) to [32mhttps://api.wandb.ai[0m. Use [1m`wandb login --relogin`[0m to force relogin</code></pre>
<p>Tracking run with wandb version 0.23.1</p>
<p>Run data is saved locally in <code>/content/wandb/run-20251213_161147-r4i8gsph</code></p>
<p>Syncing run <strong><a href='https://wandb.ai/jkelle11-university-of-maryland/melanoma-classifier/runs/r4i8gsph' target="_blank">rich-field-21</a></strong> to <a href='https://wandb.ai/jkelle11-university-of-maryland/melanoma-classifier' target="_blank">Weights &amp; Biases</a> (<a href='https://wandb.me/developer-guide' target="_blank">docs</a>)<br></p>
<p>View project at <a href='https://wandb.ai/jkelle11-university-of-maryland/melanoma-classifier' target="_blank">https://wandb.ai/jkelle11-university-of-maryland/melanoma-classifier</a></p>
<p>View run at <a href='https://wandb.ai/jkelle11-university-of-maryland/melanoma-classifier/runs/r4i8gsph' target="_blank">https://wandb.ai/jkelle11-university-of-maryland/melanoma-classifier/runs/r4i8gsph</a></p>
<h3 id="hardware-check">Hardware Check</h3>
<p>Now before we begin this run of our CNN, it’s important to check that a GPU is available. A GPU (Graphics Processing Unit) is a special kind of computer chip that can run millions of small calculations in parallel at once, which is exactly what powers the magic of neural networks.</p>
<p>If GPU is not available in our Colab environment, we’ll have to rely on CPU, which is <em>much</em> slower.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true"></a><span class="co"># If a GPU is available</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true"></a><span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true"></a></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true"></a>    <span class="co"># Define a variable to refer to GPU</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true"></a>    chip <span class="op">=</span> torch.device(<span class="st">&#39;cuda&#39;</span>)</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true"></a></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true"></a>    <span class="co"># Notify user of type of GPU</span></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&#39;</span><span class="ch">\n</span><span class="ss">GPU in Use: </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>get_device_name(<span class="dv">0</span>)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true"></a></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true"></a>    <span class="co"># Notify user of GPU memory specs</span></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&#39;</span><span class="ch">\n</span><span class="ss">GPU Memory: </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>get_device_properties(<span class="dv">0</span>)<span class="sc">.</span>total_memory <span class="op">/</span> <span class="fl">1e9</span><span class="sc">:.2f}</span><span class="ss"> GB&#39;</span>)</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true"></a></span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true"></a>    <span class="co"># Notify user of CUDA version (language for communicating w/ GPU)</span></span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&#39;</span><span class="ch">\n</span><span class="ss">CUDA Version: </span><span class="sc">{</span>torch<span class="sc">.</span>version<span class="sc">.</span>cuda<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true"></a></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true"></a>    <span class="co"># Turn on benchmarking to speed up CNN training</span></span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true"></a>    torch.backends.cudnn.benchmark <span class="op">=</span> <span class="va">True</span></span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true"></a></span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true"></a><span class="co"># If GPU is not available</span></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true"></a><span class="cf">else</span>:</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true"></a></span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true"></a>    <span class="co"># Define a variable to refer to CPU</span></span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true"></a>    chip <span class="op">=</span> torch.device(<span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true"></a></span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true"></a>    <span class="co"># Notify user that GPU is not available</span></span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="st">&#39;</span><span class="ch">\n</span><span class="st">Caution: Only CPU currently available&#39;</span>)</span></code></pre></div>
<pre><code>GPU in Use: NVIDIA A100-SXM4-80GB

GPU Memory: 85.17 GB

CUDA Version: 12.6</code></pre>
<h3 id="data-preparation">Data Preparation</h3>
<p>Now that we’ve configured our chip, we’ll unzip our training set images so our CNN can ingest them.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true"></a><span class="co"># Define path to zipped train set</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true"></a>train_zip <span class="op">=</span> <span class="st">&quot;/content/drive/MyDrive/DATA602_Final_Project/train_resized.zip&quot;</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true"></a></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true"></a><span class="co"># Define path to output unzipped train set</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true"></a>unzipped_train <span class="op">=</span> <span class="st">&quot;/content/Unzipped_Train&quot;</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true"></a></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true"></a><span class="co"># If path to unzipped train set exists from previous run</span></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true"></a><span class="cf">if</span> os.path.exists(unzipped_train):</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true"></a></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true"></a>  <span class="co"># Remove leftover folder</span></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true"></a>  shutil.rmtree(unzipped_train)</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true"></a></span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true"></a><span class="co"># Open zipped train set as read-only</span></span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true"></a><span class="cf">with</span> zipfile.ZipFile(train_zip, <span class="st">&#39;r&#39;</span>) <span class="im">as</span> zipped_train:</span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true"></a></span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true"></a>  <span class="co"># Extract all images into unzipped train folder,</span></span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true"></a>  <span class="co"># preserving class subfolders</span></span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true"></a>  zipped_train.extractall(unzipped_train)</span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true"></a></span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true"></a><span class="co"># Notify user that extraction is complete</span></span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;Extraction complete.&quot;</span>)</span></code></pre></div>
<pre><code>Extraction complete.</code></pre>
<p>Next, we’ll create a folder to hold checkpoints from our model training, so that if training is interrupted for some reason, we won’t lose all our progress!</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true"></a><span class="co"># Define path to checkpoint folder</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true"></a>checkpoints_folder <span class="op">=</span> <span class="st">&quot;/content/drive/MyDrive/DATA602_Final_Project/model_checkpoints3&quot;</span></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true"></a></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true"></a><span class="co"># Create checkpoint folder at path</span></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true"></a>os.makedirs(checkpoints_folder, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true"></a></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true"></a><span class="co"># Define name of checkpoint file</span></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true"></a>checkpoint_f <span class="op">=</span> <span class="ss">f&quot;</span><span class="sc">{</span>checkpoints_folder<span class="sc">}</span><span class="ss">/training_checkpoint.pkl&quot;</span></span></code></pre></div>
<p>For our final data preparation step, we’ll normalize the color channel values of our images using the ImageNet means and standard deviations. To explain what these are, we’ll have to back up and discuss transfer learning.</p>
<p>Training CNNs from scratch is very, very computationally expensive. The models need to learn how to recognize lines, then shapes, then combinations of shapes, then objects, and finally entire image contents. As a couple of college students, we simply don’t have the compute resources or money needed for this process. Luckily, instead of running this expensive and extremely long training from scratch, we can take a faster, simpler approach: <em>transfer learning</em>.</p>
<p>Transfer learning involves importing an existing CNN that has already learned how to recognize lines, shapes, combinations of shapes, objects, and entire image contents. We then add a few custom neural layers on top of that CNN that will learn how to classify the image it’s recognized as either benign or malignant melanoma. This approach saves us a ton of time and computational cost, not just on model training, but also on experimentation and tweaking hyperparameters.</p>
<p>Our model will leverage transfer learning from the EfficientNetV2-B0 CNN, a model trained on the ImageNet dataset. In order to match our data to the ENV2-B0 model’s assumptions and thus speed up its learning, we’ll normalize our images’ color channels to the mean and standard deviation that this model was trained on. Check out these next steps below!</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true"></a><span class="co"># Define lists of ImageNet color channel means and standard deviations</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true"></a>imagenet_means <span class="op">=</span> [<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>]</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true"></a>imagenet_stds <span class="op">=</span> [<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>]</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true"></a></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true"></a><span class="co"># Create data preparation pipeline</span></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true"></a>data_prep <span class="op">=</span> transforms.Compose([</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true"></a></span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true"></a>    <span class="co"># Convert images to tensor format</span></span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true"></a>    transforms.ToTensor(),</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true"></a></span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true"></a>    <span class="co"># Normalize images&#39; color channels by ImageNet summary stats</span></span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true"></a>    transforms.Normalize(imagenet_means, imagenet_stds)</span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true"></a>])</span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true"></a></span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true"></a><span class="co"># Create ImageFolder object of our train set,</span></span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true"></a><span class="co"># applying data preparation pipeline</span></span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true"></a>train_full_set <span class="op">=</span> datasets.ImageFolder(root<span class="op">=</span>unzipped_train, transform <span class="op">=</span> data_prep)</span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true"></a></span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true"></a><span class="co"># Extract indexes of train set</span></span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true"></a>indexes <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(train_full_set)))</span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true"></a></span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true"></a><span class="co"># Extract class labels of train set images</span></span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true"></a>labels <span class="op">=</span> [train_full_set.imgs[i][<span class="dv">1</span>] <span class="cf">for</span> i <span class="kw">in</span> indexes]</span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true"></a></span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true"></a><span class="co"># Notify user of train set size</span></span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Loaded </span><span class="sc">{</span><span class="bu">len</span>(train_full_set)<span class="sc">}</span><span class="ss"> images&quot;</span>)</span>
<span id="cb48-27"><a href="#cb48-27" aria-hidden="true"></a></span>
<span id="cb48-28"><a href="#cb48-28" aria-hidden="true"></a><span class="co"># Notify user of train set classes</span></span>
<span id="cb48-29"><a href="#cb48-29" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Classes: </span><span class="sc">{</span>train_full_set<span class="sc">.</span>classes<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb48-30"><a href="#cb48-30" aria-hidden="true"></a></span>
<span id="cb48-31"><a href="#cb48-31" aria-hidden="true"></a><span class="co"># Notify user of class distribution for imbalance checking</span></span>
<span id="cb48-32"><a href="#cb48-32" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Class distribution: </span><span class="sc">{np.</span>bincount(labels)<span class="sc">}</span><span class="ch">\n</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>Loaded 9605 images

Classes: [&#39;benign&#39;, &#39;malignant&#39;]

Class distribution: [5000 4605]</code></pre>
<h3 id="k-folds-cross-validation">K-Folds Cross-Validation</h3>
<p>One last thing before we begin our main training loop: we need to set up <em>K-folds Cross-Validation</em>. This sounds complex, but it’s actually pretty simple:</p>
<ol type="1">
<li>We choose an integer (K).</li>
<li>We split our train set into K equally-sized subsets.</li>
<li>We train our CNN on K - 1 of these subsets.</li>
<li>We evaluate our CNN’s performance on the Kth subset.</li>
<li>We repeat this process until each subset has served as the evaluation (validation) set exactly once.</li>
</ol>
<p>This technique allows us to avoid overly trusting a single training process to follow the absolute best path. This is important because the complexity of neural networks means that a lot can go wrong and prevent our CNN from learning the best ways to distinguish benign and malignant melanoma.</p>
<p>With K-Folds Cross-Validation, we can create K models and compare their performance. This allows us to better estimate the generalization performance of our architecture, again by not simply relying on one training run (which could result in unrealistically high performance), but by averaging the performances of the same architecture trained on different subsets of data.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true"></a><span class="co"># Define Stratified K Fold object</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true"></a>stratkf <span class="op">=</span> StratifiedKFold(n_splits<span class="op">=</span>config[<span class="st">&#39;k&#39;</span>], shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">23</span>)</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true"></a></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true"></a><span class="co"># Create variable to track current fold</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true"></a>curr_fold <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true"></a></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true"></a><span class="co"># Create list to hold recall scores of each fold&#39;s model</span></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true"></a>fold_val_recalls <span class="op">=</span> []</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true"></a></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true"></a><span class="co"># Create list to hold recall score curves of each fold&#39;s model</span></span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true"></a>fold_recall_curves <span class="op">=</span> []</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true"></a></span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true"></a><span class="co"># Check if previous model checkpoints exist</span></span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true"></a><span class="co"># (i.e., training was interrupted)</span></span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true"></a><span class="cf">if</span> os.path.exists(checkpoint_f):</span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true"></a></span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true"></a>    <span class="co"># Open checkpoint file in read-binary mode</span></span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true"></a>    <span class="cf">with</span> <span class="bu">open</span>(checkpoint_f, <span class="st">&#39;rb&#39;</span>) <span class="im">as</span> cp:</span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true"></a></span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true"></a>        <span class="co"># Extract checkpoint data</span></span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true"></a>        checkp <span class="op">=</span> pickle.load(cp)</span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true"></a></span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true"></a>        <span class="co"># Define fold to resume from</span></span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true"></a>        curr_fold <span class="op">=</span> checkp[<span class="st">&#39;last_completed_fold&#39;</span>] <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true"></a></span>
<span id="cb50-26"><a href="#cb50-26" aria-hidden="true"></a>        <span class="co"># Extract recall score list to resume building</span></span>
<span id="cb50-27"><a href="#cb50-27" aria-hidden="true"></a>        fold_val_recalls <span class="op">=</span> checkp[<span class="st">&#39;fold_val_recalls&#39;</span>]</span>
<span id="cb50-28"><a href="#cb50-28" aria-hidden="true"></a></span>
<span id="cb50-29"><a href="#cb50-29" aria-hidden="true"></a>        <span class="co"># Notify user that checkpoint was found</span></span>
<span id="cb50-30"><a href="#cb50-30" aria-hidden="true"></a>        <span class="bu">print</span>(<span class="ss">f&quot;Resuming from fold </span><span class="sc">{</span>curr_fold <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ch">\n</span><span class="ss">&quot;</span>)</span></code></pre></div>
<h3 id="model-training">Model Training</h3>
<p>This is where our model learns! As mentioned before, there is a bit of complex linear algebra and calculus involved in this process. Follow along with the code comments to learn more!</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true"></a><span class="co"># Iterate over all K-Folds</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true"></a><span class="cf">for</span> fold, (train_index, val_index) <span class="kw">in</span> <span class="bu">enumerate</span>(stratkf.split(indexes, labels)):</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true"></a></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true"></a>    <span class="co"># Notify user of current fold</span></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="sc">{</span><span class="st">&#39;=&#39;</span><span class="op">*</span><span class="dv">50</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&quot;FOLD </span><span class="sc">{</span>fold <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss"> / </span><span class="sc">{</span>config[<span class="st">&#39;k&#39;</span>]<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span><span class="st">&#39;=&#39;</span><span class="op">*</span><span class="dv">50</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true"></a></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true"></a>    <span class="co"># Skip fold if completed in previous checkpoint</span></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true"></a>    <span class="cf">if</span> fold <span class="op">&lt;</span> curr_fold:</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true"></a>        <span class="bu">print</span>(<span class="ss">f&quot;Fold </span><span class="sc">{</span>fold <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss"> already completed.</span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true"></a>        <span class="cf">continue</span></span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true"></a></span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true"></a>    <span class="co"># Define path to augmented train directory</span></span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true"></a>    augmented_train_dir <span class="op">=</span> <span class="ss">f&#39;/content/augmented_fold</span><span class="sc">{</span>fold<span class="sc">}</span><span class="ss">_train&#39;</span></span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true"></a></span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true"></a>    <span class="co"># Remove augmented train directory if already exists</span></span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true"></a>    <span class="cf">if</span> os.path.exists(augmented_train_dir):</span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true"></a>      shutil.rmtree(augmented_train_dir)</span>
<span id="cb51-20"><a href="#cb51-20" aria-hidden="true"></a></span>
<span id="cb51-21"><a href="#cb51-21" aria-hidden="true"></a>    <span class="co"># Augment train set for current fold and</span></span>
<span id="cb51-22"><a href="#cb51-22" aria-hidden="true"></a>    <span class="co"># output in augmented train directory</span></span>
<span id="cb51-23"><a href="#cb51-23" aria-hidden="true"></a>    augment_fold_train_set(train_full_set, train_index, augmented_train_dir)</span>
<span id="cb51-24"><a href="#cb51-24" aria-hidden="true"></a></span>
<span id="cb51-25"><a href="#cb51-25" aria-hidden="true"></a>    <span class="co"># Create ImageFolder object from augmented train directory,</span></span>
<span id="cb51-26"><a href="#cb51-26" aria-hidden="true"></a>    <span class="co"># applying data preparation pipeline</span></span>
<span id="cb51-27"><a href="#cb51-27" aria-hidden="true"></a>    train_set <span class="op">=</span> datasets.ImageFolder(augmented_train_dir, transform <span class="op">=</span> data_prep)</span>
<span id="cb51-28"><a href="#cb51-28" aria-hidden="true"></a></span>
<span id="cb51-29"><a href="#cb51-29" aria-hidden="true"></a>    <span class="co"># Create validation set as subset of full train set</span></span>
<span id="cb51-30"><a href="#cb51-30" aria-hidden="true"></a>    val_set <span class="op">=</span> Subset(train_full_set, val_index)</span>
<span id="cb51-31"><a href="#cb51-31" aria-hidden="true"></a></span>
<span id="cb51-32"><a href="#cb51-32" aria-hidden="true"></a>    <span class="co"># Create DataLoader to serve batches of training data</span></span>
<span id="cb51-33"><a href="#cb51-33" aria-hidden="true"></a>    <span class="co"># to model</span></span>
<span id="cb51-34"><a href="#cb51-34" aria-hidden="true"></a>    train_loader <span class="op">=</span> DataLoader(train_set, batch_size<span class="op">=</span>config[<span class="st">&#39;batch_size&#39;</span>],</span>
<span id="cb51-35"><a href="#cb51-35" aria-hidden="true"></a>                              shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">2</span>, pin_memory<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb51-36"><a href="#cb51-36" aria-hidden="true"></a></span>
<span id="cb51-37"><a href="#cb51-37" aria-hidden="true"></a>    <span class="co"># Create DataLoader to serve batches of validation data</span></span>
<span id="cb51-38"><a href="#cb51-38" aria-hidden="true"></a>    <span class="co"># to model</span></span>
<span id="cb51-39"><a href="#cb51-39" aria-hidden="true"></a>    val_loader <span class="op">=</span> DataLoader(val_set, batch_size<span class="op">=</span>config[<span class="st">&#39;batch_size&#39;</span>],</span>
<span id="cb51-40"><a href="#cb51-40" aria-hidden="true"></a>                            shuffle<span class="op">=</span><span class="va">False</span>, num_workers<span class="op">=</span><span class="dv">2</span>, pin_memory<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb51-41"><a href="#cb51-41" aria-hidden="true"></a></span>
<span id="cb51-42"><a href="#cb51-42" aria-hidden="true"></a>    <span class="co"># Notify user of train set and validation set size</span></span>
<span id="cb51-43"><a href="#cb51-43" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Train samples: </span><span class="sc">{</span><span class="bu">len</span>(train_set)<span class="sc">}</span><span class="ss"> | Val samples: </span><span class="sc">{</span><span class="bu">len</span>(val_set)<span class="sc">}</span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb51-44"><a href="#cb51-44" aria-hidden="true"></a></span>
<span id="cb51-45"><a href="#cb51-45" aria-hidden="true"></a>    <span class="co"># Create EfficientNetV2-B0 instance to use as base model</span></span>
<span id="cb51-46"><a href="#cb51-46" aria-hidden="true"></a>    ENV2B0 <span class="op">=</span> timm.create_model(<span class="st">&#39;tf_efficientnetv2_b0&#39;</span>, pretrained<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb51-47"><a href="#cb51-47" aria-hidden="true"></a>                                num_classes<span class="op">=</span><span class="dv">0</span>, global_pool<span class="op">=</span><span class="st">&#39;avg&#39;</span>)</span>
<span id="cb51-48"><a href="#cb51-48" aria-hidden="true"></a></span>
<span id="cb51-49"><a href="#cb51-49" aria-hidden="true"></a>    <span class="co"># Ensure base model does not change for now</span></span>
<span id="cb51-50"><a href="#cb51-50" aria-hidden="true"></a>    <span class="cf">for</span> param <span class="kw">in</span> ENV2B0.parameters():</span>
<span id="cb51-51"><a href="#cb51-51" aria-hidden="true"></a>        param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb51-52"><a href="#cb51-52" aria-hidden="true"></a></span>
<span id="cb51-53"><a href="#cb51-53" aria-hidden="true"></a>    <span class="co"># Define architecture of custom melanoma classifier</span></span>
<span id="cb51-54"><a href="#cb51-54" aria-hidden="true"></a>    classifier_head <span class="op">=</span> torch.nn.Sequential(</span>
<span id="cb51-55"><a href="#cb51-55" aria-hidden="true"></a></span>
<span id="cb51-56"><a href="#cb51-56" aria-hidden="true"></a>        <span class="co"># Linear layer that learns and applies parameters to</span></span>
<span id="cb51-57"><a href="#cb51-57" aria-hidden="true"></a>        <span class="co"># inputs, just like regression computes parameters</span></span>
<span id="cb51-58"><a href="#cb51-58" aria-hidden="true"></a>        <span class="co"># for variables</span></span>
<span id="cb51-59"><a href="#cb51-59" aria-hidden="true"></a>        torch.nn.Linear(ENV2B0.num_features, config[<span class="st">&#39;dense_units&#39;</span>]),</span>
<span id="cb51-60"><a href="#cb51-60" aria-hidden="true"></a></span>
<span id="cb51-61"><a href="#cb51-61" aria-hidden="true"></a>        <span class="co"># Normalization layer that normalizes batch</span></span>
<span id="cb51-62"><a href="#cb51-62" aria-hidden="true"></a>        <span class="co"># output of previous layer</span></span>
<span id="cb51-63"><a href="#cb51-63" aria-hidden="true"></a>        torch.nn.BatchNorm1d(config[<span class="st">&#39;dense_units&#39;</span>]),</span>
<span id="cb51-64"><a href="#cb51-64" aria-hidden="true"></a></span>
<span id="cb51-65"><a href="#cb51-65" aria-hidden="true"></a>        <span class="co"># Activation function that introduces nonlinearity,</span></span>
<span id="cb51-66"><a href="#cb51-66" aria-hidden="true"></a>        <span class="co"># allowing our model to learn complex patterns</span></span>
<span id="cb51-67"><a href="#cb51-67" aria-hidden="true"></a>        torch.nn.ReLU(),</span>
<span id="cb51-68"><a href="#cb51-68" aria-hidden="true"></a></span>
<span id="cb51-69"><a href="#cb51-69" aria-hidden="true"></a>        <span class="co"># Randomly turn off a certain proportion of</span></span>
<span id="cb51-70"><a href="#cb51-70" aria-hidden="true"></a>        <span class="co"># our model&#39;s neurons, preventing our model</span></span>
<span id="cb51-71"><a href="#cb51-71" aria-hidden="true"></a>        <span class="co"># from simply memorizing training data</span></span>
<span id="cb51-72"><a href="#cb51-72" aria-hidden="true"></a>        <span class="co"># instead of learning its important patterns</span></span>
<span id="cb51-73"><a href="#cb51-73" aria-hidden="true"></a>        torch.nn.Dropout(config[<span class="st">&#39;dropout_ratio&#39;</span>]),</span>
<span id="cb51-74"><a href="#cb51-74" aria-hidden="true"></a></span>
<span id="cb51-75"><a href="#cb51-75" aria-hidden="true"></a>        <span class="co"># Another linear layer</span></span>
<span id="cb51-76"><a href="#cb51-76" aria-hidden="true"></a>        torch.nn.Linear(config[<span class="st">&#39;dense_units&#39;</span>], <span class="dv">1</span>),</span>
<span id="cb51-77"><a href="#cb51-77" aria-hidden="true"></a></span>
<span id="cb51-78"><a href="#cb51-78" aria-hidden="true"></a>        <span class="co"># Final layer that outputs a computed probability</span></span>
<span id="cb51-79"><a href="#cb51-79" aria-hidden="true"></a>        <span class="co"># of malignancy for each image</span></span>
<span id="cb51-80"><a href="#cb51-80" aria-hidden="true"></a>        torch.nn.Sigmoid()</span>
<span id="cb51-81"><a href="#cb51-81" aria-hidden="true"></a>    )</span>
<span id="cb51-82"><a href="#cb51-82" aria-hidden="true"></a></span>
<span id="cb51-83"><a href="#cb51-83" aria-hidden="true"></a>    <span class="co"># Combine our pretrained model with our custom classifier</span></span>
<span id="cb51-84"><a href="#cb51-84" aria-hidden="true"></a>    model <span class="op">=</span> torch.nn.Sequential(ENV2B0, classifier_head).to(chip)</span>
<span id="cb51-85"><a href="#cb51-85" aria-hidden="true"></a></span>
<span id="cb51-86"><a href="#cb51-86" aria-hidden="true"></a>    <span class="co"># Create loss function to calculate model error</span></span>
<span id="cb51-87"><a href="#cb51-87" aria-hidden="true"></a>    loss_bce <span class="op">=</span> torch.nn.BCELoss()</span>
<span id="cb51-88"><a href="#cb51-88" aria-hidden="true"></a></span>
<span id="cb51-89"><a href="#cb51-89" aria-hidden="true"></a>    <span class="co"># Create initial learning rate, which decides how fast our model will learn</span></span>
<span id="cb51-90"><a href="#cb51-90" aria-hidden="true"></a>    initial_lr <span class="op">=</span> config[<span class="st">&#39;learn_rate&#39;</span>] <span class="op">*</span> <span class="fl">0.1</span> <span class="cf">if</span> config[<span class="st">&#39;warmup_epochs&#39;</span>] <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> config[<span class="st">&#39;learn_rate&#39;</span>]</span>
<span id="cb51-91"><a href="#cb51-91" aria-hidden="true"></a></span>
<span id="cb51-92"><a href="#cb51-92" aria-hidden="true"></a>    <span class="co"># Create optimizer that will guide our model in learning</span></span>
<span id="cb51-93"><a href="#cb51-93" aria-hidden="true"></a>    optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span>initial_lr,</span>
<span id="cb51-94"><a href="#cb51-94" aria-hidden="true"></a>                                  weight_decay<span class="op">=</span>config[<span class="st">&#39;weight_decay&#39;</span>])</span>
<span id="cb51-95"><a href="#cb51-95" aria-hidden="true"></a></span>
<span id="cb51-96"><a href="#cb51-96" aria-hidden="true"></a>    <span class="co"># Create scheduler to adapt our learning rate if our model stops improving</span></span>
<span id="cb51-97"><a href="#cb51-97" aria-hidden="true"></a>    scheduler <span class="op">=</span> torch.optim.lr_scheduler.ReduceLROnPlateau(</span>
<span id="cb51-98"><a href="#cb51-98" aria-hidden="true"></a>        optimizer, mode<span class="op">=</span><span class="st">&#39;min&#39;</span>, factor<span class="op">=</span><span class="fl">0.5</span>, patience<span class="op">=</span><span class="dv">3</span>, min_lr<span class="op">=</span><span class="fl">1e-7</span></span>
<span id="cb51-99"><a href="#cb51-99" aria-hidden="true"></a>    )</span>
<span id="cb51-100"><a href="#cb51-100" aria-hidden="true"></a></span>
<span id="cb51-101"><a href="#cb51-101" aria-hidden="true"></a>    <span class="co"># Create variable to hold best recall</span></span>
<span id="cb51-102"><a href="#cb51-102" aria-hidden="true"></a>    best_recall <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb51-103"><a href="#cb51-103" aria-hidden="true"></a></span>
<span id="cb51-104"><a href="#cb51-104" aria-hidden="true"></a>    <span class="co"># Create variable to hold epoch patience. i.e., the number</span></span>
<span id="cb51-105"><a href="#cb51-105" aria-hidden="true"></a>    <span class="co"># of epochs we&#39;ve run without our model improving</span></span>
<span id="cb51-106"><a href="#cb51-106" aria-hidden="true"></a>    patience <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb51-107"><a href="#cb51-107" aria-hidden="true"></a></span>
<span id="cb51-108"><a href="#cb51-108" aria-hidden="true"></a>    <span class="co"># Create variable to signify whether fine-tuning is complete</span></span>
<span id="cb51-109"><a href="#cb51-109" aria-hidden="true"></a>    ft_done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb51-110"><a href="#cb51-110" aria-hidden="true"></a></span>
<span id="cb51-111"><a href="#cb51-111" aria-hidden="true"></a>    <span class="co"># Create list to store epoch recall scores</span></span>
<span id="cb51-112"><a href="#cb51-112" aria-hidden="true"></a>    epoch_recalls <span class="op">=</span> []</span>
<span id="cb51-113"><a href="#cb51-113" aria-hidden="true"></a></span>
<span id="cb51-114"><a href="#cb51-114" aria-hidden="true"></a>    <span class="co"># Create curve to hold best fold recall score for plotting later</span></span>
<span id="cb51-115"><a href="#cb51-115" aria-hidden="true"></a>    best_recall_curve <span class="op">=</span> <span class="va">None</span></span>
<span id="cb51-116"><a href="#cb51-116" aria-hidden="true"></a></span>
<span id="cb51-117"><a href="#cb51-117" aria-hidden="true"></a>    <span class="co"># Iterate over all training epochs</span></span>
<span id="cb51-118"><a href="#cb51-118" aria-hidden="true"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(config[<span class="st">&#39;epochs&#39;</span>]):</span>
<span id="cb51-119"><a href="#cb51-119" aria-hidden="true"></a></span>
<span id="cb51-120"><a href="#cb51-120" aria-hidden="true"></a>        <span class="co"># Check if current training epoch is a warmup epoch</span></span>
<span id="cb51-121"><a href="#cb51-121" aria-hidden="true"></a>        <span class="cf">if</span> epoch <span class="op">&lt;</span> config[<span class="st">&#39;warmup_epochs&#39;</span>]:</span>
<span id="cb51-122"><a href="#cb51-122" aria-hidden="true"></a></span>
<span id="cb51-123"><a href="#cb51-123" aria-hidden="true"></a>            <span class="co"># Compute warmup factor and warmup epoch learning rate</span></span>
<span id="cb51-124"><a href="#cb51-124" aria-hidden="true"></a>            warmup_factor <span class="op">=</span> (epoch <span class="op">+</span> <span class="dv">1</span>) <span class="op">/</span> config[<span class="st">&#39;warmup_epochs&#39;</span>]</span>
<span id="cb51-125"><a href="#cb51-125" aria-hidden="true"></a>            warmup_lr <span class="op">=</span> config[<span class="st">&#39;learn_rate&#39;</span>] <span class="op">*</span> warmup_factor</span>
<span id="cb51-126"><a href="#cb51-126" aria-hidden="true"></a></span>
<span id="cb51-127"><a href="#cb51-127" aria-hidden="true"></a>            <span class="co"># Propagate warmup learning rate to all parameters</span></span>
<span id="cb51-128"><a href="#cb51-128" aria-hidden="true"></a>            <span class="cf">for</span> param_group <span class="kw">in</span> optimizer.param_groups:</span>
<span id="cb51-129"><a href="#cb51-129" aria-hidden="true"></a>                param_group[<span class="st">&#39;lr&#39;</span>] <span class="op">=</span> warmup_lr</span>
<span id="cb51-130"><a href="#cb51-130" aria-hidden="true"></a></span>
<span id="cb51-131"><a href="#cb51-131" aria-hidden="true"></a>        <span class="co"># If current epoch is not a warmup epoch,</span></span>
<span id="cb51-132"><a href="#cb51-132" aria-hidden="true"></a>        <span class="co"># keep main learning rate</span></span>
<span id="cb51-133"><a href="#cb51-133" aria-hidden="true"></a>        <span class="cf">elif</span> epoch <span class="op">==</span> config[<span class="st">&#39;warmup_epochs&#39;</span>]:</span>
<span id="cb51-134"><a href="#cb51-134" aria-hidden="true"></a>            <span class="cf">for</span> param_group <span class="kw">in</span> optimizer.param_groups:</span>
<span id="cb51-135"><a href="#cb51-135" aria-hidden="true"></a>                param_group[<span class="st">&#39;lr&#39;</span>] <span class="op">=</span> config[<span class="st">&#39;learn_rate&#39;</span>]</span>
<span id="cb51-136"><a href="#cb51-136" aria-hidden="true"></a></span>
<span id="cb51-137"><a href="#cb51-137" aria-hidden="true"></a>        <span class="co"># If fine-tuning is not complete and it&#39;s time to fine-tune</span></span>
<span id="cb51-138"><a href="#cb51-138" aria-hidden="true"></a>        <span class="cf">if</span> <span class="kw">not</span> ft_done <span class="kw">and</span> epoch <span class="op">==</span> config[<span class="st">&#39;finetune_after&#39;</span>]:</span>
<span id="cb51-139"><a href="#cb51-139" aria-hidden="true"></a></span>
<span id="cb51-140"><a href="#cb51-140" aria-hidden="true"></a>            <span class="co"># Extract all parameters from pretrained base model</span></span>
<span id="cb51-141"><a href="#cb51-141" aria-hidden="true"></a>            total_params <span class="op">=</span> <span class="bu">list</span>(ENV2B0.parameters())</span>
<span id="cb51-142"><a href="#cb51-142" aria-hidden="true"></a></span>
<span id="cb51-143"><a href="#cb51-143" aria-hidden="true"></a>            <span class="co"># Calculate unfreezing threshold, or number of parameters</span></span>
<span id="cb51-144"><a href="#cb51-144" aria-hidden="true"></a>            <span class="co"># in pretrained model we will retrain to capture additional</span></span>
<span id="cb51-145"><a href="#cb51-145" aria-hidden="true"></a>            <span class="co"># accuracy and recall score</span></span>
<span id="cb51-146"><a href="#cb51-146" aria-hidden="true"></a>            threshold <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(total_params) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> config[<span class="st">&#39;unfreeze_pct&#39;</span>]))</span>
<span id="cb51-147"><a href="#cb51-147" aria-hidden="true"></a></span>
<span id="cb51-148"><a href="#cb51-148" aria-hidden="true"></a>            <span class="co"># Iterate over all parameters</span></span>
<span id="cb51-149"><a href="#cb51-149" aria-hidden="true"></a>            <span class="cf">for</span> i, param <span class="kw">in</span> <span class="bu">enumerate</span>(total_params):</span>
<span id="cb51-150"><a href="#cb51-150" aria-hidden="true"></a></span>
<span id="cb51-151"><a href="#cb51-151" aria-hidden="true"></a>                <span class="co"># If parameter is above threshold</span></span>
<span id="cb51-152"><a href="#cb51-152" aria-hidden="true"></a>                <span class="cf">if</span> i <span class="op">&gt;=</span> threshold:</span>
<span id="cb51-153"><a href="#cb51-153" aria-hidden="true"></a></span>
<span id="cb51-154"><a href="#cb51-154" aria-hidden="true"></a>                    <span class="co"># Unfreeze parameter (set to learning mode)</span></span>
<span id="cb51-155"><a href="#cb51-155" aria-hidden="true"></a>                    param.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb51-156"><a href="#cb51-156" aria-hidden="true"></a></span>
<span id="cb51-157"><a href="#cb51-157" aria-hidden="true"></a>            <span class="co"># Create optimizer that will guide our pretrained model&#39;s</span></span>
<span id="cb51-158"><a href="#cb51-158" aria-hidden="true"></a>            <span class="co"># parameters in re-learning</span></span>
<span id="cb51-159"><a href="#cb51-159" aria-hidden="true"></a>            optimizer <span class="op">=</span> torch.optim.AdamW(</span>
<span id="cb51-160"><a href="#cb51-160" aria-hidden="true"></a>                <span class="bu">filter</span>(<span class="kw">lambda</span> x: x.requires_grad, model.parameters()),</span>
<span id="cb51-161"><a href="#cb51-161" aria-hidden="true"></a>                lr<span class="op">=</span>config[<span class="st">&#39;learn_rate&#39;</span>] <span class="op">/</span> <span class="dv">10</span>,</span>
<span id="cb51-162"><a href="#cb51-162" aria-hidden="true"></a>                weight_decay<span class="op">=</span>config[<span class="st">&#39;weight_decay&#39;</span>] <span class="op">/</span> <span class="dv">10</span></span>
<span id="cb51-163"><a href="#cb51-163" aria-hidden="true"></a>            )</span>
<span id="cb51-164"><a href="#cb51-164" aria-hidden="true"></a></span>
<span id="cb51-165"><a href="#cb51-165" aria-hidden="true"></a>            <span class="co"># Create scheduler to adapt our learning rate if our model stops improving</span></span>
<span id="cb51-166"><a href="#cb51-166" aria-hidden="true"></a>            scheduler <span class="op">=</span> torch.optim.lr_scheduler.ReduceLROnPlateau(</span>
<span id="cb51-167"><a href="#cb51-167" aria-hidden="true"></a>                optimizer, mode<span class="op">=</span><span class="st">&#39;min&#39;</span>, factor<span class="op">=</span><span class="fl">0.5</span>, patience<span class="op">=</span><span class="dv">3</span>, min_lr<span class="op">=</span><span class="fl">1e-8</span></span>
<span id="cb51-168"><a href="#cb51-168" aria-hidden="true"></a>            )</span>
<span id="cb51-169"><a href="#cb51-169" aria-hidden="true"></a></span>
<span id="cb51-170"><a href="#cb51-170" aria-hidden="true"></a>            <span class="co"># Mark fine-tuning as complete</span></span>
<span id="cb51-171"><a href="#cb51-171" aria-hidden="true"></a>            ft_done <span class="op">=</span> <span class="va">True</span></span>
<span id="cb51-172"><a href="#cb51-172" aria-hidden="true"></a>            <span class="bu">print</span>(<span class="ss">f&quot;Fine-tuning activated at epoch </span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb51-173"><a href="#cb51-173" aria-hidden="true"></a></span>
<span id="cb51-174"><a href="#cb51-174" aria-hidden="true"></a>        <span class="co"># Set model to training mode</span></span>
<span id="cb51-175"><a href="#cb51-175" aria-hidden="true"></a>        model.train()</span>
<span id="cb51-176"><a href="#cb51-176" aria-hidden="true"></a></span>
<span id="cb51-177"><a href="#cb51-177" aria-hidden="true"></a>        <span class="co"># Create variables to hold training set loss and total data trained on</span></span>
<span id="cb51-178"><a href="#cb51-178" aria-hidden="true"></a>        train_loss, total <span class="op">=</span> <span class="fl">0.0</span>, <span class="dv">0</span></span>
<span id="cb51-179"><a href="#cb51-179" aria-hidden="true"></a></span>
<span id="cb51-180"><a href="#cb51-180" aria-hidden="true"></a>        <span class="co"># Create list to hold predictions, true class labels,</span></span>
<span id="cb51-181"><a href="#cb51-181" aria-hidden="true"></a>        <span class="co"># and predicted probabilities</span></span>
<span id="cb51-182"><a href="#cb51-182" aria-hidden="true"></a>        train_all_preds, train_all_labels, train_all_probs <span class="op">=</span> [], [], []</span>
<span id="cb51-183"><a href="#cb51-183" aria-hidden="true"></a></span>
<span id="cb51-184"><a href="#cb51-184" aria-hidden="true"></a>        <span class="co"># Iterate over all images in each batch</span></span>
<span id="cb51-185"><a href="#cb51-185" aria-hidden="true"></a>        <span class="cf">for</span> images, labels_batch <span class="kw">in</span> tqdm(train_loader, desc<span class="op">=</span><span class="ss">f&quot;Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> [Train]&quot;</span>):</span>
<span id="cb51-186"><a href="#cb51-186" aria-hidden="true"></a></span>
<span id="cb51-187"><a href="#cb51-187" aria-hidden="true"></a>            <span class="co"># Load images and class labels to device</span></span>
<span id="cb51-188"><a href="#cb51-188" aria-hidden="true"></a>            images <span class="op">=</span> images.to(chip)</span>
<span id="cb51-189"><a href="#cb51-189" aria-hidden="true"></a>            labels_batch <span class="op">=</span> labels_batch.<span class="bu">float</span>().unsqueeze(<span class="dv">1</span>).to(chip)</span>
<span id="cb51-190"><a href="#cb51-190" aria-hidden="true"></a></span>
<span id="cb51-191"><a href="#cb51-191" aria-hidden="true"></a>            <span class="co"># Reset optimizer gradient</span></span>
<span id="cb51-192"><a href="#cb51-192" aria-hidden="true"></a>            optimizer.zero_grad()</span>
<span id="cb51-193"><a href="#cb51-193" aria-hidden="true"></a></span>
<span id="cb51-194"><a href="#cb51-194" aria-hidden="true"></a>            <span class="co"># Predict class of images using model</span></span>
<span id="cb51-195"><a href="#cb51-195" aria-hidden="true"></a>            outputs <span class="op">=</span> model(images)</span>
<span id="cb51-196"><a href="#cb51-196" aria-hidden="true"></a></span>
<span id="cb51-197"><a href="#cb51-197" aria-hidden="true"></a>            <span class="co"># Calculate loss</span></span>
<span id="cb51-198"><a href="#cb51-198" aria-hidden="true"></a>            loss <span class="op">=</span> loss_bce(outputs, labels_batch)</span>
<span id="cb51-199"><a href="#cb51-199" aria-hidden="true"></a></span>
<span id="cb51-200"><a href="#cb51-200" aria-hidden="true"></a>            <span class="co"># Backpropagate partial derivatives of loss so that</span></span>
<span id="cb51-201"><a href="#cb51-201" aria-hidden="true"></a>            <span class="co"># optimizer knows how to adjust weights</span></span>
<span id="cb51-202"><a href="#cb51-202" aria-hidden="true"></a>            loss.backward()</span>
<span id="cb51-203"><a href="#cb51-203" aria-hidden="true"></a></span>
<span id="cb51-204"><a href="#cb51-204" aria-hidden="true"></a>            <span class="co"># Prevent exploding gradients for training stability</span></span>
<span id="cb51-205"><a href="#cb51-205" aria-hidden="true"></a>            torch.nn.utils.clip_grad_norm_(model.parameters(), config[<span class="st">&#39;grad_clip&#39;</span>])</span>
<span id="cb51-206"><a href="#cb51-206" aria-hidden="true"></a></span>
<span id="cb51-207"><a href="#cb51-207" aria-hidden="true"></a>            <span class="co"># Adjust weights according to backpropagated loss</span></span>
<span id="cb51-208"><a href="#cb51-208" aria-hidden="true"></a>            optimizer.step()</span>
<span id="cb51-209"><a href="#cb51-209" aria-hidden="true"></a></span>
<span id="cb51-210"><a href="#cb51-210" aria-hidden="true"></a>            <span class="co"># Increment total train loss</span></span>
<span id="cb51-211"><a href="#cb51-211" aria-hidden="true"></a>            train_loss <span class="op">+=</span> loss.item() <span class="op">*</span> images.size(<span class="dv">0</span>)</span>
<span id="cb51-212"><a href="#cb51-212" aria-hidden="true"></a></span>
<span id="cb51-213"><a href="#cb51-213" aria-hidden="true"></a>            <span class="co"># Extract predicted class labels</span></span>
<span id="cb51-214"><a href="#cb51-214" aria-hidden="true"></a>            predicted_labels <span class="op">=</span> (outputs <span class="op">&gt;</span> <span class="fl">0.5</span>)</span>
<span id="cb51-215"><a href="#cb51-215" aria-hidden="true"></a></span>
<span id="cb51-216"><a href="#cb51-216" aria-hidden="true"></a>            <span class="co"># Add batch predictions to list</span></span>
<span id="cb51-217"><a href="#cb51-217" aria-hidden="true"></a>            train_all_preds.extend(predicted_labels.cpu().numpy().flatten())</span>
<span id="cb51-218"><a href="#cb51-218" aria-hidden="true"></a></span>
<span id="cb51-219"><a href="#cb51-219" aria-hidden="true"></a>            <span class="co"># Add batch true labels to list</span></span>
<span id="cb51-220"><a href="#cb51-220" aria-hidden="true"></a>            train_all_labels.extend(labels_batch.cpu().numpy().flatten())</span>
<span id="cb51-221"><a href="#cb51-221" aria-hidden="true"></a></span>
<span id="cb51-222"><a href="#cb51-222" aria-hidden="true"></a>            <span class="co"># Add batch predicted probabilities to list</span></span>
<span id="cb51-223"><a href="#cb51-223" aria-hidden="true"></a>            train_all_probs.extend(outputs.detach().cpu().numpy().flatten())</span>
<span id="cb51-224"><a href="#cb51-224" aria-hidden="true"></a></span>
<span id="cb51-225"><a href="#cb51-225" aria-hidden="true"></a>            <span class="co"># Increment total data trained on by batch size</span></span>
<span id="cb51-226"><a href="#cb51-226" aria-hidden="true"></a>            total <span class="op">+=</span> labels_batch.size(<span class="dv">0</span>)</span>
<span id="cb51-227"><a href="#cb51-227" aria-hidden="true"></a></span>
<span id="cb51-228"><a href="#cb51-228" aria-hidden="true"></a>        <span class="co"># Calculate average train loss</span></span>
<span id="cb51-229"><a href="#cb51-229" aria-hidden="true"></a>        train_loss <span class="op">/=</span> total</span>
<span id="cb51-230"><a href="#cb51-230" aria-hidden="true"></a></span>
<span id="cb51-231"><a href="#cb51-231" aria-hidden="true"></a>        <span class="co"># Compute training set recall score</span></span>
<span id="cb51-232"><a href="#cb51-232" aria-hidden="true"></a>        train_recall <span class="op">=</span> recall_score(train_all_labels, train_all_preds, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb51-233"><a href="#cb51-233" aria-hidden="true"></a></span>
<span id="cb51-234"><a href="#cb51-234" aria-hidden="true"></a>        <span class="co"># Compute training set precision score</span></span>
<span id="cb51-235"><a href="#cb51-235" aria-hidden="true"></a>        train_precision <span class="op">=</span> precision_score(train_all_labels, train_all_preds, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb51-236"><a href="#cb51-236" aria-hidden="true"></a></span>
<span id="cb51-237"><a href="#cb51-237" aria-hidden="true"></a>        <span class="co"># Compute training set F1 score</span></span>
<span id="cb51-238"><a href="#cb51-238" aria-hidden="true"></a>        train_f1 <span class="op">=</span> f1_score(train_all_labels, train_all_preds, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb51-239"><a href="#cb51-239" aria-hidden="true"></a></span>
<span id="cb51-240"><a href="#cb51-240" aria-hidden="true"></a>        <span class="co"># Compute training set ROC-AUC score</span></span>
<span id="cb51-241"><a href="#cb51-241" aria-hidden="true"></a>        train_auc <span class="op">=</span> roc_auc_score(train_all_labels, train_all_probs) <span class="cf">if</span> <span class="bu">len</span>(<span class="bu">set</span>(train_all_labels)) <span class="op">&gt;</span> <span class="dv">1</span> <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb51-242"><a href="#cb51-242" aria-hidden="true"></a></span>
<span id="cb51-243"><a href="#cb51-243" aria-hidden="true"></a>        <span class="co"># Set model to evaluation mode</span></span>
<span id="cb51-244"><a href="#cb51-244" aria-hidden="true"></a>        model.<span class="bu">eval</span>()</span>
<span id="cb51-245"><a href="#cb51-245" aria-hidden="true"></a></span>
<span id="cb51-246"><a href="#cb51-246" aria-hidden="true"></a>        <span class="co"># Create variables to hold validation set loss and total data validated on</span></span>
<span id="cb51-247"><a href="#cb51-247" aria-hidden="true"></a>        val_loss, total <span class="op">=</span> <span class="fl">0.0</span>, <span class="dv">0</span></span>
<span id="cb51-248"><a href="#cb51-248" aria-hidden="true"></a></span>
<span id="cb51-249"><a href="#cb51-249" aria-hidden="true"></a>        <span class="co"># Create list to hold predictions, true class labels,</span></span>
<span id="cb51-250"><a href="#cb51-250" aria-hidden="true"></a>        <span class="co"># and predicted probabilities</span></span>
<span id="cb51-251"><a href="#cb51-251" aria-hidden="true"></a>        val_all_preds, val_all_labels, val_all_probs <span class="op">=</span> [], [], []</span>
<span id="cb51-252"><a href="#cb51-252" aria-hidden="true"></a></span>
<span id="cb51-253"><a href="#cb51-253" aria-hidden="true"></a>        <span class="co"># Turn off gradient tracking (loss computation)</span></span>
<span id="cb51-254"><a href="#cb51-254" aria-hidden="true"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb51-255"><a href="#cb51-255" aria-hidden="true"></a></span>
<span id="cb51-256"><a href="#cb51-256" aria-hidden="true"></a>            <span class="co"># Iterate over all images in each batch</span></span>
<span id="cb51-257"><a href="#cb51-257" aria-hidden="true"></a>            <span class="cf">for</span> images, labels_batch <span class="kw">in</span> tqdm(val_loader, desc<span class="op">=</span><span class="ss">f&quot;Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> [Val]&quot;</span>):</span>
<span id="cb51-258"><a href="#cb51-258" aria-hidden="true"></a></span>
<span id="cb51-259"><a href="#cb51-259" aria-hidden="true"></a>                <span class="co"># Load images and class labels to device</span></span>
<span id="cb51-260"><a href="#cb51-260" aria-hidden="true"></a>                images <span class="op">=</span> images.to(chip)</span>
<span id="cb51-261"><a href="#cb51-261" aria-hidden="true"></a>                labels_batch <span class="op">=</span> labels_batch.<span class="bu">float</span>().unsqueeze(<span class="dv">1</span>).to(chip)</span>
<span id="cb51-262"><a href="#cb51-262" aria-hidden="true"></a></span>
<span id="cb51-263"><a href="#cb51-263" aria-hidden="true"></a>                <span class="co"># Predict class of images using model</span></span>
<span id="cb51-264"><a href="#cb51-264" aria-hidden="true"></a>                outputs <span class="op">=</span> model(images)</span>
<span id="cb51-265"><a href="#cb51-265" aria-hidden="true"></a></span>
<span id="cb51-266"><a href="#cb51-266" aria-hidden="true"></a>                <span class="co"># Calculate loss</span></span>
<span id="cb51-267"><a href="#cb51-267" aria-hidden="true"></a>                loss <span class="op">=</span> loss_bce(outputs, labels_batch)</span>
<span id="cb51-268"><a href="#cb51-268" aria-hidden="true"></a></span>
<span id="cb51-269"><a href="#cb51-269" aria-hidden="true"></a>                <span class="co"># Increment total validation loss</span></span>
<span id="cb51-270"><a href="#cb51-270" aria-hidden="true"></a>                val_loss <span class="op">+=</span> loss.item() <span class="op">*</span> images.size(<span class="dv">0</span>)</span>
<span id="cb51-271"><a href="#cb51-271" aria-hidden="true"></a></span>
<span id="cb51-272"><a href="#cb51-272" aria-hidden="true"></a>                <span class="co"># Extract predicted class labels</span></span>
<span id="cb51-273"><a href="#cb51-273" aria-hidden="true"></a>                predicted_labels <span class="op">=</span> (outputs <span class="op">&gt;</span> <span class="fl">0.5</span>)</span>
<span id="cb51-274"><a href="#cb51-274" aria-hidden="true"></a></span>
<span id="cb51-275"><a href="#cb51-275" aria-hidden="true"></a>                <span class="co"># Add batch predictions to list</span></span>
<span id="cb51-276"><a href="#cb51-276" aria-hidden="true"></a>                val_all_preds.extend(predicted_labels.cpu().numpy().flatten())</span>
<span id="cb51-277"><a href="#cb51-277" aria-hidden="true"></a></span>
<span id="cb51-278"><a href="#cb51-278" aria-hidden="true"></a>                <span class="co"># Add batch true labels to list</span></span>
<span id="cb51-279"><a href="#cb51-279" aria-hidden="true"></a>                val_all_labels.extend(labels_batch.cpu().numpy().flatten())</span>
<span id="cb51-280"><a href="#cb51-280" aria-hidden="true"></a></span>
<span id="cb51-281"><a href="#cb51-281" aria-hidden="true"></a>                <span class="co"># Add batch predicted probabilities to list</span></span>
<span id="cb51-282"><a href="#cb51-282" aria-hidden="true"></a>                val_all_probs.extend(outputs.cpu().numpy().flatten())</span>
<span id="cb51-283"><a href="#cb51-283" aria-hidden="true"></a></span>
<span id="cb51-284"><a href="#cb51-284" aria-hidden="true"></a>                <span class="co"># Increment total data validated on by batch size</span></span>
<span id="cb51-285"><a href="#cb51-285" aria-hidden="true"></a>                total <span class="op">+=</span> labels_batch.size(<span class="dv">0</span>)</span>
<span id="cb51-286"><a href="#cb51-286" aria-hidden="true"></a></span>
<span id="cb51-287"><a href="#cb51-287" aria-hidden="true"></a>        <span class="co"># Calculate average validation loss</span></span>
<span id="cb51-288"><a href="#cb51-288" aria-hidden="true"></a>        val_loss <span class="op">/=</span> total</span>
<span id="cb51-289"><a href="#cb51-289" aria-hidden="true"></a></span>
<span id="cb51-290"><a href="#cb51-290" aria-hidden="true"></a>        <span class="co"># Compute validation set recall score</span></span>
<span id="cb51-291"><a href="#cb51-291" aria-hidden="true"></a>        val_recall <span class="op">=</span> recall_score(val_all_labels, val_all_preds, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb51-292"><a href="#cb51-292" aria-hidden="true"></a></span>
<span id="cb51-293"><a href="#cb51-293" aria-hidden="true"></a>        <span class="co"># Append recall score to epoch list</span></span>
<span id="cb51-294"><a href="#cb51-294" aria-hidden="true"></a>        epoch_recalls.append(val_recall)</span>
<span id="cb51-295"><a href="#cb51-295" aria-hidden="true"></a></span>
<span id="cb51-296"><a href="#cb51-296" aria-hidden="true"></a>        <span class="co"># Compute validation set precision score</span></span>
<span id="cb51-297"><a href="#cb51-297" aria-hidden="true"></a>        val_precision <span class="op">=</span> precision_score(val_all_labels, val_all_preds, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb51-298"><a href="#cb51-298" aria-hidden="true"></a></span>
<span id="cb51-299"><a href="#cb51-299" aria-hidden="true"></a>        <span class="co"># Compute validation set F1 score</span></span>
<span id="cb51-300"><a href="#cb51-300" aria-hidden="true"></a>        val_f1 <span class="op">=</span> f1_score(val_all_labels, val_all_preds, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb51-301"><a href="#cb51-301" aria-hidden="true"></a></span>
<span id="cb51-302"><a href="#cb51-302" aria-hidden="true"></a>        <span class="co"># Compute validation set ROC-AUC score</span></span>
<span id="cb51-303"><a href="#cb51-303" aria-hidden="true"></a>        val_auc <span class="op">=</span> roc_auc_score(val_all_labels, val_all_probs) <span class="cf">if</span> <span class="bu">len</span>(<span class="bu">set</span>(val_all_labels)) <span class="op">&gt;</span> <span class="dv">1</span> <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb51-304"><a href="#cb51-304" aria-hidden="true"></a></span>
<span id="cb51-305"><a href="#cb51-305" aria-hidden="true"></a>        <span class="co"># Update scheduler</span></span>
<span id="cb51-306"><a href="#cb51-306" aria-hidden="true"></a>        scheduler.step(val_loss)</span>
<span id="cb51-307"><a href="#cb51-307" aria-hidden="true"></a></span>
<span id="cb51-308"><a href="#cb51-308" aria-hidden="true"></a>        <span class="co"># Log epoch stats to W&amp;B</span></span>
<span id="cb51-309"><a href="#cb51-309" aria-hidden="true"></a>        wandb.log(</span>
<span id="cb51-310"><a href="#cb51-310" aria-hidden="true"></a>            {</span>
<span id="cb51-311"><a href="#cb51-311" aria-hidden="true"></a>                <span class="st">&#39;fold&#39;</span>: fold <span class="op">+</span> <span class="dv">1</span>,</span>
<span id="cb51-312"><a href="#cb51-312" aria-hidden="true"></a>                <span class="st">&#39;epoch&#39;</span>: epoch <span class="op">+</span> <span class="dv">1</span>,</span>
<span id="cb51-313"><a href="#cb51-313" aria-hidden="true"></a>                <span class="st">&#39;train_loss&#39;</span>: train_loss,</span>
<span id="cb51-314"><a href="#cb51-314" aria-hidden="true"></a>                <span class="st">&#39;train_recall&#39;</span>: train_recall,</span>
<span id="cb51-315"><a href="#cb51-315" aria-hidden="true"></a>                <span class="st">&#39;train_precision&#39;</span>: train_precision,</span>
<span id="cb51-316"><a href="#cb51-316" aria-hidden="true"></a>                <span class="st">&#39;train_f1&#39;</span>: train_f1,</span>
<span id="cb51-317"><a href="#cb51-317" aria-hidden="true"></a>                <span class="st">&#39;train_auc&#39;</span>: train_auc,</span>
<span id="cb51-318"><a href="#cb51-318" aria-hidden="true"></a>                <span class="st">&#39;val_loss&#39;</span>: val_loss,</span>
<span id="cb51-319"><a href="#cb51-319" aria-hidden="true"></a>                <span class="st">&#39;val_recall&#39;</span>: val_recall,</span>
<span id="cb51-320"><a href="#cb51-320" aria-hidden="true"></a>                <span class="st">&#39;val_precision&#39;</span>: val_precision,</span>
<span id="cb51-321"><a href="#cb51-321" aria-hidden="true"></a>                <span class="st">&#39;val_f1&#39;</span>: val_f1,</span>
<span id="cb51-322"><a href="#cb51-322" aria-hidden="true"></a>                <span class="st">&#39;val_auc&#39;</span>: val_auc,</span>
<span id="cb51-323"><a href="#cb51-323" aria-hidden="true"></a>                <span class="st">&#39;learning_rate&#39;</span>: optimizer.param_groups[<span class="dv">0</span>][<span class="st">&#39;lr&#39;</span>]</span>
<span id="cb51-324"><a href="#cb51-324" aria-hidden="true"></a>            }</span>
<span id="cb51-325"><a href="#cb51-325" aria-hidden="true"></a>        )</span>
<span id="cb51-326"><a href="#cb51-326" aria-hidden="true"></a></span>
<span id="cb51-327"><a href="#cb51-327" aria-hidden="true"></a>        <span class="co"># Print epoch metrics</span></span>
<span id="cb51-328"><a href="#cb51-328" aria-hidden="true"></a>        <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Epoch </span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">:&quot;</span>)</span>
<span id="cb51-329"><a href="#cb51-329" aria-hidden="true"></a>        <span class="bu">print</span>(<span class="ss">f&quot;  Train → Loss: </span><span class="sc">{</span>train_loss<span class="sc">:.4f}</span><span class="ss"> | Recall: </span><span class="sc">{</span>train_recall<span class="sc">:.4f}</span><span class="ss"> | Precision: </span><span class="sc">{</span>train_precision<span class="sc">:.4f}</span><span class="ss"> | F1: </span><span class="sc">{</span>train_f1<span class="sc">:.4f}</span><span class="ss"> | AUC: </span><span class="sc">{</span>train_auc<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb51-330"><a href="#cb51-330" aria-hidden="true"></a>        <span class="bu">print</span>(<span class="ss">f&quot;  Val   → Loss: </span><span class="sc">{</span>val_loss<span class="sc">:.4f}</span><span class="ss"> | Recall: </span><span class="sc">{</span>val_recall<span class="sc">:.4f}</span><span class="ss"> | Precision: </span><span class="sc">{</span>val_precision<span class="sc">:.4f}</span><span class="ss"> | F1: </span><span class="sc">{</span>val_f1<span class="sc">:.4f}</span><span class="ss"> | AUC: </span><span class="sc">{</span>val_auc<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb51-331"><a href="#cb51-331" aria-hidden="true"></a>        <span class="bu">print</span>(<span class="ss">f&quot;  LR: </span><span class="sc">{</span>optimizer<span class="sc">.</span>param_groups[<span class="dv">0</span>][<span class="st">&#39;lr&#39;</span>]<span class="sc">:.2e}</span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb51-332"><a href="#cb51-332" aria-hidden="true"></a></span>
<span id="cb51-333"><a href="#cb51-333" aria-hidden="true"></a>        <span class="co"># Check if validation recall score achieved</span></span>
<span id="cb51-334"><a href="#cb51-334" aria-hidden="true"></a>        <span class="co"># exceeds previous best</span></span>
<span id="cb51-335"><a href="#cb51-335" aria-hidden="true"></a>        <span class="cf">if</span> val_recall <span class="op">&gt;</span> best_recall:</span>
<span id="cb51-336"><a href="#cb51-336" aria-hidden="true"></a></span>
<span id="cb51-337"><a href="#cb51-337" aria-hidden="true"></a>            <span class="co"># Store best validation recall score achieved</span></span>
<span id="cb51-338"><a href="#cb51-338" aria-hidden="true"></a>            best_recall <span class="op">=</span> val_recall</span>
<span id="cb51-339"><a href="#cb51-339" aria-hidden="true"></a></span>
<span id="cb51-340"><a href="#cb51-340" aria-hidden="true"></a>            <span class="co"># Define path to checkpoint save file</span></span>
<span id="cb51-341"><a href="#cb51-341" aria-hidden="true"></a>            savepath <span class="op">=</span> <span class="ss">f&quot;</span><span class="sc">{</span>checkpoints_folder<span class="sc">}</span><span class="ss">/best_model_fold</span><span class="sc">{</span>fold<span class="sc">}</span><span class="ss">.pt&quot;</span></span>
<span id="cb51-342"><a href="#cb51-342" aria-hidden="true"></a></span>
<span id="cb51-343"><a href="#cb51-343" aria-hidden="true"></a>            <span class="co"># Save checkpoint</span></span>
<span id="cb51-344"><a href="#cb51-344" aria-hidden="true"></a>            torch.save(model.state_dict(), savepath)</span>
<span id="cb51-345"><a href="#cb51-345" aria-hidden="true"></a></span>
<span id="cb51-346"><a href="#cb51-346" aria-hidden="true"></a>            <span class="co"># Reset patience</span></span>
<span id="cb51-347"><a href="#cb51-347" aria-hidden="true"></a>            patience <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb51-348"><a href="#cb51-348" aria-hidden="true"></a></span>
<span id="cb51-349"><a href="#cb51-349" aria-hidden="true"></a>            <span class="co"># Notify user of save and best recall score yet achieved</span></span>
<span id="cb51-350"><a href="#cb51-350" aria-hidden="true"></a>            <span class="bu">print</span>(<span class="ss">f&quot;Model saved! (Best recall: </span><span class="sc">{</span>best_recall<span class="sc">:.4f}</span><span class="ss">)&quot;</span>)</span>
<span id="cb51-351"><a href="#cb51-351" aria-hidden="true"></a></span>
<span id="cb51-352"><a href="#cb51-352" aria-hidden="true"></a>        <span class="co"># If validation recall score not improved</span></span>
<span id="cb51-353"><a href="#cb51-353" aria-hidden="true"></a>        <span class="cf">else</span>:</span>
<span id="cb51-354"><a href="#cb51-354" aria-hidden="true"></a></span>
<span id="cb51-355"><a href="#cb51-355" aria-hidden="true"></a>            <span class="co"># Increment patience</span></span>
<span id="cb51-356"><a href="#cb51-356" aria-hidden="true"></a>            patience <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb51-357"><a href="#cb51-357" aria-hidden="true"></a></span>
<span id="cb51-358"><a href="#cb51-358" aria-hidden="true"></a>            <span class="co"># If patience exceeds pre-set limit</span></span>
<span id="cb51-359"><a href="#cb51-359" aria-hidden="true"></a>            <span class="co"># (i.e., model has stopped improving)</span></span>
<span id="cb51-360"><a href="#cb51-360" aria-hidden="true"></a>            <span class="cf">if</span> patience <span class="op">&gt;=</span> config[<span class="st">&#39;patience_steps&#39;</span>]:</span>
<span id="cb51-361"><a href="#cb51-361" aria-hidden="true"></a></span>
<span id="cb51-362"><a href="#cb51-362" aria-hidden="true"></a>                <span class="co"># Stop early to avoid wasting resources</span></span>
<span id="cb51-363"><a href="#cb51-363" aria-hidden="true"></a>                <span class="co"># and notify user</span></span>
<span id="cb51-364"><a href="#cb51-364" aria-hidden="true"></a>                <span class="bu">print</span>(<span class="ss">f&quot;Early stopping at epoch </span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb51-365"><a href="#cb51-365" aria-hidden="true"></a>                <span class="cf">break</span></span>
<span id="cb51-366"><a href="#cb51-366" aria-hidden="true"></a></span>
<span id="cb51-367"><a href="#cb51-367" aria-hidden="true"></a>    <span class="co"># Load best model of current fold</span></span>
<span id="cb51-368"><a href="#cb51-368" aria-hidden="true"></a>    model.load_state_dict(torch.load(<span class="ss">f&quot;</span><span class="sc">{</span>checkpoints_folder<span class="sc">}</span><span class="ss">/best_model_fold</span><span class="sc">{</span>fold<span class="sc">}</span><span class="ss">.pt&quot;</span>))</span>
<span id="cb51-369"><a href="#cb51-369" aria-hidden="true"></a></span>
<span id="cb51-370"><a href="#cb51-370" aria-hidden="true"></a>    <span class="co"># Set model to evaluation mode</span></span>
<span id="cb51-371"><a href="#cb51-371" aria-hidden="true"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb51-372"><a href="#cb51-372" aria-hidden="true"></a></span>
<span id="cb51-373"><a href="#cb51-373" aria-hidden="true"></a>    <span class="co"># Create variables to hold validation set loss and total data validated on</span></span>
<span id="cb51-374"><a href="#cb51-374" aria-hidden="true"></a>    val_loss, total <span class="op">=</span> <span class="fl">0.0</span>, <span class="dv">0</span></span>
<span id="cb51-375"><a href="#cb51-375" aria-hidden="true"></a></span>
<span id="cb51-376"><a href="#cb51-376" aria-hidden="true"></a>    <span class="co"># Create list to hold predictions, true class labels,</span></span>
<span id="cb51-377"><a href="#cb51-377" aria-hidden="true"></a>    <span class="co"># and predicted probabilities</span></span>
<span id="cb51-378"><a href="#cb51-378" aria-hidden="true"></a>    final_preds, final_labels, final_probs <span class="op">=</span> [], [], []</span>
<span id="cb51-379"><a href="#cb51-379" aria-hidden="true"></a></span>
<span id="cb51-380"><a href="#cb51-380" aria-hidden="true"></a>    <span class="co"># Turn off gradient tracking (loss computation)</span></span>
<span id="cb51-381"><a href="#cb51-381" aria-hidden="true"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb51-382"><a href="#cb51-382" aria-hidden="true"></a></span>
<span id="cb51-383"><a href="#cb51-383" aria-hidden="true"></a>        <span class="co"># Iterate over all images in each batch</span></span>
<span id="cb51-384"><a href="#cb51-384" aria-hidden="true"></a>        <span class="cf">for</span> images, labels_batch <span class="kw">in</span> val_loader:</span>
<span id="cb51-385"><a href="#cb51-385" aria-hidden="true"></a></span>
<span id="cb51-386"><a href="#cb51-386" aria-hidden="true"></a>            <span class="co"># Load images and class labels to device</span></span>
<span id="cb51-387"><a href="#cb51-387" aria-hidden="true"></a>            images <span class="op">=</span> images.to(chip)</span>
<span id="cb51-388"><a href="#cb51-388" aria-hidden="true"></a>            labels_batch <span class="op">=</span> labels_batch.<span class="bu">float</span>().unsqueeze(<span class="dv">1</span>).to(chip)</span>
<span id="cb51-389"><a href="#cb51-389" aria-hidden="true"></a></span>
<span id="cb51-390"><a href="#cb51-390" aria-hidden="true"></a>            <span class="co"># Predict class of images using model</span></span>
<span id="cb51-391"><a href="#cb51-391" aria-hidden="true"></a>            outputs <span class="op">=</span> model(images)</span>
<span id="cb51-392"><a href="#cb51-392" aria-hidden="true"></a></span>
<span id="cb51-393"><a href="#cb51-393" aria-hidden="true"></a>            <span class="co"># Calculate loss</span></span>
<span id="cb51-394"><a href="#cb51-394" aria-hidden="true"></a>            loss <span class="op">=</span> loss_bce(outputs, labels_batch)</span>
<span id="cb51-395"><a href="#cb51-395" aria-hidden="true"></a></span>
<span id="cb51-396"><a href="#cb51-396" aria-hidden="true"></a>            <span class="co"># Increment total validation loss</span></span>
<span id="cb51-397"><a href="#cb51-397" aria-hidden="true"></a>            val_loss <span class="op">+=</span> loss.item() <span class="op">*</span> images.size(<span class="dv">0</span>)</span>
<span id="cb51-398"><a href="#cb51-398" aria-hidden="true"></a></span>
<span id="cb51-399"><a href="#cb51-399" aria-hidden="true"></a>            <span class="co"># Extract predicted class labels</span></span>
<span id="cb51-400"><a href="#cb51-400" aria-hidden="true"></a>            predicted_labels <span class="op">=</span> (outputs <span class="op">&gt;</span> <span class="fl">0.5</span>)</span>
<span id="cb51-401"><a href="#cb51-401" aria-hidden="true"></a></span>
<span id="cb51-402"><a href="#cb51-402" aria-hidden="true"></a>            <span class="co"># Add batch predictions to list</span></span>
<span id="cb51-403"><a href="#cb51-403" aria-hidden="true"></a>            final_preds.extend(predicted_labels.cpu().numpy().flatten())</span>
<span id="cb51-404"><a href="#cb51-404" aria-hidden="true"></a></span>
<span id="cb51-405"><a href="#cb51-405" aria-hidden="true"></a>            <span class="co"># Add batch true labels to list</span></span>
<span id="cb51-406"><a href="#cb51-406" aria-hidden="true"></a>            final_labels.extend(labels_batch.cpu().numpy().flatten())</span>
<span id="cb51-407"><a href="#cb51-407" aria-hidden="true"></a></span>
<span id="cb51-408"><a href="#cb51-408" aria-hidden="true"></a>            <span class="co"># Add batch predicted probabilities to list</span></span>
<span id="cb51-409"><a href="#cb51-409" aria-hidden="true"></a>            final_probs.extend(outputs.cpu().numpy().flatten())</span>
<span id="cb51-410"><a href="#cb51-410" aria-hidden="true"></a></span>
<span id="cb51-411"><a href="#cb51-411" aria-hidden="true"></a>            <span class="co"># Increment total data validated on by batch size</span></span>
<span id="cb51-412"><a href="#cb51-412" aria-hidden="true"></a>            total <span class="op">+=</span> labels_batch.size(<span class="dv">0</span>)</span>
<span id="cb51-413"><a href="#cb51-413" aria-hidden="true"></a></span>
<span id="cb51-414"><a href="#cb51-414" aria-hidden="true"></a>    <span class="co"># Calculate average validation loss</span></span>
<span id="cb51-415"><a href="#cb51-415" aria-hidden="true"></a>    val_loss <span class="op">/=</span> total</span>
<span id="cb51-416"><a href="#cb51-416" aria-hidden="true"></a></span>
<span id="cb51-417"><a href="#cb51-417" aria-hidden="true"></a>    <span class="co"># Compute validation set recall score</span></span>
<span id="cb51-418"><a href="#cb51-418" aria-hidden="true"></a>    val_recall <span class="op">=</span> recall_score(final_labels, final_preds, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb51-419"><a href="#cb51-419" aria-hidden="true"></a></span>
<span id="cb51-420"><a href="#cb51-420" aria-hidden="true"></a>    <span class="co"># Compute validation set precision score</span></span>
<span id="cb51-421"><a href="#cb51-421" aria-hidden="true"></a>    val_precision <span class="op">=</span> precision_score(final_labels, final_preds, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb51-422"><a href="#cb51-422" aria-hidden="true"></a></span>
<span id="cb51-423"><a href="#cb51-423" aria-hidden="true"></a>    <span class="co"># Compute validation set F1 score</span></span>
<span id="cb51-424"><a href="#cb51-424" aria-hidden="true"></a>    val_f1 <span class="op">=</span> f1_score(final_labels, final_preds, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb51-425"><a href="#cb51-425" aria-hidden="true"></a></span>
<span id="cb51-426"><a href="#cb51-426" aria-hidden="true"></a>    <span class="co"># Compute validation set ROC-AUC score</span></span>
<span id="cb51-427"><a href="#cb51-427" aria-hidden="true"></a>    val_auc <span class="op">=</span> roc_auc_score(final_labels, final_probs) <span class="cf">if</span> <span class="bu">len</span>(<span class="bu">set</span>(final_labels)) <span class="op">&gt;</span> <span class="dv">1</span> <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb51-428"><a href="#cb51-428" aria-hidden="true"></a></span>
<span id="cb51-429"><a href="#cb51-429" aria-hidden="true"></a>    <span class="co"># Append validation recall score to fold list</span></span>
<span id="cb51-430"><a href="#cb51-430" aria-hidden="true"></a>    fold_val_recalls.append(val_recall)</span>
<span id="cb51-431"><a href="#cb51-431" aria-hidden="true"></a></span>
<span id="cb51-432"><a href="#cb51-432" aria-hidden="true"></a>    <span class="co"># Append fold validation recall score curve to list</span></span>
<span id="cb51-433"><a href="#cb51-433" aria-hidden="true"></a>    fold_recall_curves.append(<span class="bu">list</span>(epoch_recalls))</span>
<span id="cb51-434"><a href="#cb51-434" aria-hidden="true"></a></span>
<span id="cb51-435"><a href="#cb51-435" aria-hidden="true"></a>    <span class="co"># Notify user of final fold results</span></span>
<span id="cb51-436"><a href="#cb51-436" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">FOLD </span><span class="sc">{</span>fold <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss"> FINAL RESULTS:</span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb51-437"><a href="#cb51-437" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Recall: </span><span class="sc">{</span>val_recall<span class="sc">:.4f}</span><span class="ss"> | Precision: </span><span class="sc">{</span>val_precision<span class="sc">:.4f}</span><span class="ss"> | F1: </span><span class="sc">{</span>val_f1<span class="sc">:.4f}</span><span class="ss"> | AUC: </span><span class="sc">{</span>val_auc<span class="sc">:.4f}</span><span class="ss"> | Loss: </span><span class="sc">{</span>val_loss<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb51-438"><a href="#cb51-438" aria-hidden="true"></a></span>
<span id="cb51-439"><a href="#cb51-439" aria-hidden="true"></a>    <span class="co"># Define fold checkpoint</span></span>
<span id="cb51-440"><a href="#cb51-440" aria-hidden="true"></a>    checkpoint <span class="op">=</span> {</span>
<span id="cb51-441"><a href="#cb51-441" aria-hidden="true"></a>        <span class="st">&#39;last_completed_fold&#39;</span>: fold,</span>
<span id="cb51-442"><a href="#cb51-442" aria-hidden="true"></a>        <span class="st">&#39;fold_val_recalls&#39;</span>: fold_val_recalls,</span>
<span id="cb51-443"><a href="#cb51-443" aria-hidden="true"></a>        <span class="st">&#39;config&#39;</span>: <span class="bu">dict</span>(config)</span>
<span id="cb51-444"><a href="#cb51-444" aria-hidden="true"></a>    }</span>
<span id="cb51-445"><a href="#cb51-445" aria-hidden="true"></a></span>
<span id="cb51-446"><a href="#cb51-446" aria-hidden="true"></a>    <span class="co"># Save fold checkpoint</span></span>
<span id="cb51-447"><a href="#cb51-447" aria-hidden="true"></a>    <span class="cf">with</span> <span class="bu">open</span>(checkpoint_f, <span class="st">&#39;wb&#39;</span>) <span class="im">as</span> cp:</span>
<span id="cb51-448"><a href="#cb51-448" aria-hidden="true"></a>        pickle.dump(checkpoint, cp)</span>
<span id="cb51-449"><a href="#cb51-449" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Fold </span><span class="sc">{</span>fold <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">: Checkpoint saved.</span><span class="ch">\n</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>==================================================
FOLD 1 / 5
==================================================


Augmenting train set...: 100%|██████████| 7684/7684 [01:54&lt;00:00, 67.23it/s]



Train samples: 69156 | Val samples: 1921



/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: 
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(



model.safetensors:   0%|          | 0.00/28.8M [00:00&lt;?, ?B/s]


Epoch 1 [Train]: 100%|██████████| 4323/4323 [01:48&lt;00:00, 39.73it/s]
Epoch 1 [Val]: 100%|██████████| 121/121 [00:11&lt;00:00, 10.92it/s]



Epoch 1:
  Train → Loss: 0.2959 | Recall: 0.8537 | Precision: 0.8830 | F1: 0.8681 | AUC: 0.9454
  Val   → Loss: 0.2211 | Recall: 0.8610 | Precision: 0.9543 | F1: 0.9053 | AUC: 0.9721
  LR: 3.33e-05

Model saved! (Best recall: 0.8610)


Epoch 2 [Train]: 100%|██████████| 4323/4323 [01:29&lt;00:00, 48.15it/s]
Epoch 2 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 52.27it/s]



Epoch 2:
  Train → Loss: 0.2476 | Recall: 0.8843 | Precision: 0.9037 | F1: 0.8939 | AUC: 0.9620
  Val   → Loss: 0.2008 | Recall: 0.9034 | Precision: 0.9286 | F1: 0.9158 | AUC: 0.9741
  LR: 6.67e-05

Model saved! (Best recall: 0.9034)


Epoch 3 [Train]: 100%|██████████| 4323/4323 [01:29&lt;00:00, 48.48it/s]
Epoch 3 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 52.76it/s]



Epoch 3:
  Train → Loss: 0.2295 | Recall: 0.8947 | Precision: 0.9142 | F1: 0.9043 | AUC: 0.9677
  Val   → Loss: 0.1927 | Recall: 0.9012 | Precision: 0.9432 | F1: 0.9217 | AUC: 0.9756
  LR: 1.00e-04



Epoch 4 [Train]: 100%|██████████| 4323/4323 [01:29&lt;00:00, 48.38it/s]
Epoch 4 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 50.16it/s]



Epoch 4:
  Train → Loss: 0.2136 | Recall: 0.9027 | Precision: 0.9204 | F1: 0.9114 | AUC: 0.9720
  Val   → Loss: 0.2028 | Recall: 0.8817 | Precision: 0.9632 | F1: 0.9206 | AUC: 0.9773
  LR: 1.00e-04



Epoch 5 [Train]: 100%|██████████| 4323/4323 [01:28&lt;00:00, 48.68it/s]
Epoch 5 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 53.52it/s]



Epoch 5:
  Train → Loss: 0.2051 | Recall: 0.9050 | Precision: 0.9270 | F1: 0.9159 | AUC: 0.9742
  Val   → Loss: 0.2046 | Recall: 0.8708 | Precision: 0.9593 | F1: 0.9129 | AUC: 0.9774
  LR: 1.00e-04



Epoch 6 [Train]: 100%|██████████| 4323/4323 [01:28&lt;00:00, 48.64it/s]
Epoch 6 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 52.84it/s]



Epoch 6:
  Train → Loss: 0.1944 | Recall: 0.9129 | Precision: 0.9290 | F1: 0.9209 | AUC: 0.9769
  Val   → Loss: 0.1971 | Recall: 0.9099 | Precision: 0.9448 | F1: 0.9270 | AUC: 0.9774
  LR: 1.00e-04

Model saved! (Best recall: 0.9099)


Epoch 7 [Train]: 100%|██████████| 4323/4323 [01:29&lt;00:00, 48.50it/s]
Epoch 7 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 53.92it/s]



Epoch 7:
  Train → Loss: 0.1895 | Recall: 0.9156 | Precision: 0.9321 | F1: 0.9238 | AUC: 0.9781
  Val   → Loss: 0.1956 | Recall: 0.9055 | Precision: 0.9499 | F1: 0.9272 | AUC: 0.9777
  LR: 5.00e-05



Epoch 8 [Train]: 100%|██████████| 4323/4323 [01:28&lt;00:00, 48.74it/s]
Epoch 8 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 54.42it/s]



Epoch 8:
  Train → Loss: 0.1767 | Recall: 0.9207 | Precision: 0.9365 | F1: 0.9285 | AUC: 0.9808
  Val   → Loss: 0.1877 | Recall: 0.9153 | Precision: 0.9388 | F1: 0.9269 | AUC: 0.9782
  LR: 5.00e-05

Model saved! (Best recall: 0.9153)
Fine-tuning activated at epoch 9


Epoch 9 [Train]: 100%|██████████| 4323/4323 [01:59&lt;00:00, 36.17it/s]
Epoch 9 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 53.74it/s]



Epoch 9:
  Train → Loss: 0.1592 | Recall: 0.9285 | Precision: 0.9451 | F1: 0.9367 | AUC: 0.9844
  Val   → Loss: 0.1855 | Recall: 0.9131 | Precision: 0.9535 | F1: 0.9329 | AUC: 0.9813
  LR: 1.00e-05



Epoch 10 [Train]: 100%|██████████| 4323/4323 [02:00&lt;00:00, 35.98it/s]
Epoch 10 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 53.56it/s]



Epoch 10:
  Train → Loss: 0.1346 | Recall: 0.9375 | Precision: 0.9556 | F1: 0.9465 | AUC: 0.9886
  Val   → Loss: 0.1881 | Recall: 0.9218 | Precision: 0.9518 | F1: 0.9366 | AUC: 0.9814
  LR: 1.00e-05

Model saved! (Best recall: 0.9218)


Epoch 11 [Train]: 100%|██████████| 4323/4323 [01:59&lt;00:00, 36.08it/s]
Epoch 11 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 53.29it/s]



Epoch 11:
  Train → Loss: 0.1203 | Recall: 0.9452 | Precision: 0.9626 | F1: 0.9538 | AUC: 0.9908
  Val   → Loss: 0.2041 | Recall: 0.9121 | Precision: 0.9677 | F1: 0.9391 | AUC: 0.9831
  LR: 1.00e-05



Epoch 12 [Train]: 100%|██████████| 4323/4323 [02:00&lt;00:00, 35.92it/s]
Epoch 12 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 53.62it/s]



Epoch 12:
  Train → Loss: 0.0998 | Recall: 0.9533 | Precision: 0.9691 | F1: 0.9611 | AUC: 0.9934
  Val   → Loss: 0.1901 | Recall: 0.9186 | Precision: 0.9570 | F1: 0.9374 | AUC: 0.9835
  LR: 1.00e-05



Epoch 13 [Train]: 100%|██████████| 4323/4323 [02:00&lt;00:00, 35.75it/s]
Epoch 13 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 56.14it/s]



Epoch 13:
  Train → Loss: 0.0924 | Recall: 0.9586 | Precision: 0.9712 | F1: 0.9649 | AUC: 0.9946
  Val   → Loss: 0.2061 | Recall: 0.9131 | Precision: 0.9656 | F1: 0.9386 | AUC: 0.9840
  LR: 5.00e-06



Epoch 14 [Train]: 100%|██████████| 4323/4323 [01:59&lt;00:00, 36.14it/s]
Epoch 14 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 52.92it/s]



Epoch 14:
  Train → Loss: 0.0782 | Recall: 0.9651 | Precision: 0.9764 | F1: 0.9707 | AUC: 0.9958
  Val   → Loss: 0.1969 | Recall: 0.9175 | Precision: 0.9580 | F1: 0.9373 | AUC: 0.9840
  LR: 5.00e-06



Epoch 15 [Train]: 100%|██████████| 4323/4323 [01:58&lt;00:00, 36.34it/s]
Epoch 15 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 55.05it/s]



Epoch 15:
  Train → Loss: 0.0742 | Recall: 0.9673 | Precision: 0.9770 | F1: 0.9721 | AUC: 0.9963
  Val   → Loss: 0.2208 | Recall: 0.9077 | Precision: 0.9687 | F1: 0.9372 | AUC: 0.9836
  LR: 5.00e-06



Epoch 16 [Train]: 100%|██████████| 4323/4323 [01:59&lt;00:00, 36.29it/s]
Epoch 16 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 53.12it/s]



Epoch 16:
  Train → Loss: 0.0704 | Recall: 0.9685 | Precision: 0.9793 | F1: 0.9739 | AUC: 0.9966
  Val   → Loss: 0.1943 | Recall: 0.9305 | Precision: 0.9554 | F1: 0.9428 | AUC: 0.9843
  LR: 5.00e-06

Model saved! (Best recall: 0.9305)


Epoch 17 [Train]: 100%|██████████| 4323/4323 [01:59&lt;00:00, 36.05it/s]
Epoch 17 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 52.10it/s]



Epoch 17:
  Train → Loss: 0.0666 | Recall: 0.9732 | Precision: 0.9803 | F1: 0.9767 | AUC: 0.9970
  Val   → Loss: 0.2081 | Recall: 0.9283 | Precision: 0.9607 | F1: 0.9442 | AUC: 0.9842
  LR: 2.50e-06



Epoch 18 [Train]: 100%|██████████| 4323/4323 [01:58&lt;00:00, 36.40it/s]
Epoch 18 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 52.88it/s]



Epoch 18:
  Train → Loss: 0.0626 | Recall: 0.9726 | Precision: 0.9819 | F1: 0.9772 | AUC: 0.9973
  Val   → Loss: 0.2323 | Recall: 0.9045 | Precision: 0.9675 | F1: 0.9349 | AUC: 0.9846
  LR: 2.50e-06



Epoch 19 [Train]: 100%|██████████| 4323/4323 [01:59&lt;00:00, 36.14it/s]
Epoch 19 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 54.51it/s]



Epoch 19:
  Train → Loss: 0.0597 | Recall: 0.9751 | Precision: 0.9830 | F1: 0.9790 | AUC: 0.9975
  Val   → Loss: 0.2092 | Recall: 0.9251 | Precision: 0.9595 | F1: 0.9420 | AUC: 0.9839
  LR: 2.50e-06



Epoch 20 [Train]: 100%|██████████| 4323/4323 [01:58&lt;00:00, 36.53it/s]
Epoch 20 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 55.65it/s]



Epoch 20:
  Train → Loss: 0.0580 | Recall: 0.9756 | Precision: 0.9841 | F1: 0.9798 | AUC: 0.9977
  Val   → Loss: 0.2216 | Recall: 0.9207 | Precision: 0.9669 | F1: 0.9433 | AUC: 0.9844
  LR: 2.50e-06



Epoch 21 [Train]: 100%|██████████| 4323/4323 [01:59&lt;00:00, 36.20it/s]
Epoch 21 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 52.72it/s]



Epoch 21:
  Train → Loss: 0.0530 | Recall: 0.9769 | Precision: 0.9846 | F1: 0.9807 | AUC: 0.9980
  Val   → Loss: 0.2090 | Recall: 0.9327 | Precision: 0.9513 | F1: 0.9419 | AUC: 0.9846
  LR: 1.25e-06

Model saved! (Best recall: 0.9327)


Epoch 22 [Train]: 100%|██████████| 4323/4323 [01:59&lt;00:00, 36.27it/s]
Epoch 22 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 56.01it/s]



Epoch 22:
  Train → Loss: 0.0527 | Recall: 0.9776 | Precision: 0.9844 | F1: 0.9810 | AUC: 0.9980
  Val   → Loss: 0.2139 | Recall: 0.9392 | Precision: 0.9433 | F1: 0.9412 | AUC: 0.9842
  LR: 1.25e-06

Model saved! (Best recall: 0.9392)


Epoch 23 [Train]: 100%|██████████| 4323/4323 [02:00&lt;00:00, 36.01it/s]
Epoch 23 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 53.61it/s]



Epoch 23:
  Train → Loss: 0.0520 | Recall: 0.9777 | Precision: 0.9865 | F1: 0.9821 | AUC: 0.9982
  Val   → Loss: 0.2285 | Recall: 0.9142 | Precision: 0.9656 | F1: 0.9392 | AUC: 0.9838
  LR: 1.25e-06



Epoch 24 [Train]: 100%|██████████| 4323/4323 [01:59&lt;00:00, 36.33it/s]
Epoch 24 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 55.76it/s]



Epoch 24:
  Train → Loss: 0.0504 | Recall: 0.9792 | Precision: 0.9853 | F1: 0.9822 | AUC: 0.9981
  Val   → Loss: 0.2019 | Recall: 0.9262 | Precision: 0.9638 | F1: 0.9446 | AUC: 0.9861
  LR: 1.25e-06



Epoch 25 [Train]: 100%|██████████| 4323/4323 [02:00&lt;00:00, 35.87it/s]
Epoch 25 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 53.93it/s]



Epoch 25:
  Train → Loss: 0.0506 | Recall: 0.9786 | Precision: 0.9854 | F1: 0.9820 | AUC: 0.9982
  Val   → Loss: 0.2048 | Recall: 0.9316 | Precision: 0.9565 | F1: 0.9439 | AUC: 0.9862
  LR: 6.25e-07



Epoch 26 [Train]: 100%|██████████| 4323/4323 [01:59&lt;00:00, 36.21it/s]
Epoch 26 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 54.09it/s]



Epoch 26:
  Train → Loss: 0.0499 | Recall: 0.9805 | Precision: 0.9855 | F1: 0.9830 | AUC: 0.9983
  Val   → Loss: 0.2296 | Recall: 0.9262 | Precision: 0.9628 | F1: 0.9441 | AUC: 0.9845
  LR: 6.25e-07



Epoch 27 [Train]: 100%|██████████| 4323/4323 [01:59&lt;00:00, 36.14it/s]
Epoch 27 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 55.49it/s]



Epoch 27:
  Train → Loss: 0.0506 | Recall: 0.9792 | Precision: 0.9859 | F1: 0.9826 | AUC: 0.9982
  Val   → Loss: 0.2165 | Recall: 0.9229 | Precision: 0.9615 | F1: 0.9418 | AUC: 0.9853
  LR: 6.25e-07



Epoch 28 [Train]: 100%|██████████| 4323/4323 [01:59&lt;00:00, 36.26it/s]
Epoch 28 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 53.28it/s]



Epoch 28:
  Train → Loss: 0.0484 | Recall: 0.9793 | Precision: 0.9851 | F1: 0.9822 | AUC: 0.9983
  Val   → Loss: 0.2324 | Recall: 0.9142 | Precision: 0.9623 | F1: 0.9376 | AUC: 0.9838
  LR: 6.25e-07

Early stopping at epoch 28

FOLD 1 FINAL RESULTS:


Recall: 0.9392 | Precision: 0.9433 | F1: 0.9412 | AUC: 0.9842 | Loss: 0.2139
Fold 1: Checkpoint saved.


==================================================
FOLD 2 / 5
==================================================


Augmenting train set...: 100%|██████████| 7684/7684 [01:50&lt;00:00, 69.36it/s]



Train samples: 69156 | Val samples: 1921



Epoch 1 [Train]: 100%|██████████| 4323/4323 [01:30&lt;00:00, 47.93it/s]
Epoch 1 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 54.69it/s]



Epoch 1:
  Train → Loss: 0.2943 | Recall: 0.8528 | Precision: 0.8852 | F1: 0.8687 | AUC: 0.9462
  Val   → Loss: 0.2154 | Recall: 0.8817 | Precision: 0.9453 | F1: 0.9124 | AUC: 0.9712
  LR: 3.33e-05

Model saved! (Best recall: 0.8817)


Epoch 2 [Train]: 100%|██████████| 4323/4323 [01:30&lt;00:00, 47.93it/s]
Epoch 2 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 55.22it/s]



Epoch 2:
  Train → Loss: 0.2448 | Recall: 0.8868 | Precision: 0.9052 | F1: 0.8959 | AUC: 0.9629
  Val   → Loss: 0.2006 | Recall: 0.8860 | Precision: 0.9423 | F1: 0.9133 | AUC: 0.9748
  LR: 6.67e-05

Model saved! (Best recall: 0.8860)


Epoch 3 [Train]: 100%|██████████| 4323/4323 [01:29&lt;00:00, 48.19it/s]
Epoch 3 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 53.98it/s]



Epoch 3:
  Train → Loss: 0.2254 | Recall: 0.8958 | Precision: 0.9137 | F1: 0.9047 | AUC: 0.9688
  Val   → Loss: 0.2038 | Recall: 0.8708 | Precision: 0.9514 | F1: 0.9093 | AUC: 0.9760
  LR: 1.00e-04



Epoch 4 [Train]: 100%|██████████| 4323/4323 [01:29&lt;00:00, 48.35it/s]
Epoch 4 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 54.56it/s]



Epoch 4:
  Train → Loss: 0.2143 | Recall: 0.9021 | Precision: 0.9205 | F1: 0.9112 | AUC: 0.9717
  Val   → Loss: 0.1970 | Recall: 0.9023 | Precision: 0.9422 | F1: 0.9218 | AUC: 0.9764
  LR: 1.00e-04

Model saved! (Best recall: 0.9023)


Epoch 5 [Train]: 100%|██████████| 4323/4323 [01:29&lt;00:00, 48.34it/s]
Epoch 5 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 54.72it/s]



Epoch 5:
  Train → Loss: 0.2002 | Recall: 0.9083 | Precision: 0.9265 | F1: 0.9173 | AUC: 0.9754
  Val   → Loss: 0.1924 | Recall: 0.9001 | Precision: 0.9388 | F1: 0.9191 | AUC: 0.9773
  LR: 1.00e-04



Epoch 6 [Train]: 100%|██████████| 4323/4323 [01:29&lt;00:00, 48.36it/s]
Epoch 6 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 54.20it/s]



Epoch 6:
  Train → Loss: 0.1946 | Recall: 0.9109 | Precision: 0.9293 | F1: 0.9200 | AUC: 0.9768
  Val   → Loss: 0.1991 | Recall: 0.8903 | Precision: 0.9404 | F1: 0.9147 | AUC: 0.9767
  LR: 1.00e-04



Epoch 7 [Train]: 100%|██████████| 4323/4323 [01:28&lt;00:00, 48.76it/s]
Epoch 7 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 54.32it/s]



Epoch 7:
  Train → Loss: 0.1861 | Recall: 0.9177 | Precision: 0.9327 | F1: 0.9251 | AUC: 0.9788
  Val   → Loss: 0.1906 | Recall: 0.9197 | Precision: 0.9267 | F1: 0.9232 | AUC: 0.9788
  LR: 1.00e-04

Model saved! (Best recall: 0.9197)


Epoch 8 [Train]: 100%|██████████| 4323/4323 [01:29&lt;00:00, 48.46it/s]
Epoch 8 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 54.33it/s]



Epoch 8:
  Train → Loss: 0.1809 | Recall: 0.9201 | Precision: 0.9351 | F1: 0.9275 | AUC: 0.9801
  Val   → Loss: 0.1858 | Recall: 0.8990 | Precision: 0.9594 | F1: 0.9283 | AUC: 0.9800
  LR: 1.00e-04

Fine-tuning activated at epoch 9


Epoch 9 [Train]: 100%|██████████| 4323/4323 [02:00&lt;00:00, 35.98it/s]
Epoch 9 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 54.01it/s]



Epoch 9:
  Train → Loss: 0.1573 | Recall: 0.9287 | Precision: 0.9471 | F1: 0.9378 | AUC: 0.9848
  Val   → Loss: 0.1940 | Recall: 0.9045 | Precision: 0.9542 | F1: 0.9287 | AUC: 0.9808
  LR: 1.00e-05



Epoch 10 [Train]: 100%|██████████| 4323/4323 [02:01&lt;00:00, 35.64it/s]
Epoch 10 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 53.62it/s]



Epoch 10:
  Train → Loss: 0.1362 | Recall: 0.9385 | Precision: 0.9542 | F1: 0.9463 | AUC: 0.9884
  Val   → Loss: 0.1824 | Recall: 0.9175 | Precision: 0.9505 | F1: 0.9337 | AUC: 0.9822
  LR: 1.00e-05



Epoch 11 [Train]: 100%|██████████| 4323/4323 [01:59&lt;00:00, 36.29it/s]
Epoch 11 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 52.67it/s]



Epoch 11:
  Train → Loss: 0.1138 | Recall: 0.9490 | Precision: 0.9655 | F1: 0.9572 | AUC: 0.9916
  Val   → Loss: 0.1923 | Recall: 0.9175 | Precision: 0.9570 | F1: 0.9368 | AUC: 0.9824
  LR: 1.00e-05



Epoch 12 [Train]: 100%|██████████| 4323/4323 [02:01&lt;00:00, 35.71it/s]
Epoch 12 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 54.30it/s]



Epoch 12:
  Train → Loss: 0.1012 | Recall: 0.9545 | Precision: 0.9693 | F1: 0.9618 | AUC: 0.9933
  Val   → Loss: 0.1984 | Recall: 0.9218 | Precision: 0.9593 | F1: 0.9402 | AUC: 0.9819
  LR: 1.00e-05

Model saved! (Best recall: 0.9218)


Epoch 13 [Train]: 100%|██████████| 4323/4323 [02:01&lt;00:00, 35.45it/s]
Epoch 13 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 50.70it/s]



Epoch 13:
  Train → Loss: 0.0893 | Recall: 0.9601 | Precision: 0.9729 | F1: 0.9665 | AUC: 0.9947
  Val   → Loss: 0.2058 | Recall: 0.9186 | Precision: 0.9484 | F1: 0.9333 | AUC: 0.9813
  LR: 1.00e-05



Epoch 14 [Train]: 100%|██████████| 4323/4323 [02:01&lt;00:00, 35.71it/s]
Epoch 14 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 53.49it/s]



Epoch 14:
  Train → Loss: 0.0764 | Recall: 0.9670 | Precision: 0.9765 | F1: 0.9718 | AUC: 0.9961
  Val   → Loss: 0.2084 | Recall: 0.9305 | Precision: 0.9428 | F1: 0.9366 | AUC: 0.9824
  LR: 5.00e-06

Model saved! (Best recall: 0.9305)


Epoch 15 [Train]: 100%|██████████| 4323/4323 [02:02&lt;00:00, 35.35it/s]
Epoch 15 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 53.96it/s]



Epoch 15:
  Train → Loss: 0.0697 | Recall: 0.9709 | Precision: 0.9787 | F1: 0.9748 | AUC: 0.9968
  Val   → Loss: 0.2259 | Recall: 0.9110 | Precision: 0.9622 | F1: 0.9359 | AUC: 0.9820
  LR: 5.00e-06



Epoch 16 [Train]: 100%|██████████| 4323/4323 [02:01&lt;00:00, 35.63it/s]
Epoch 16 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 51.90it/s]



Epoch 16:
  Train → Loss: 0.0647 | Recall: 0.9716 | Precision: 0.9806 | F1: 0.9761 | AUC: 0.9971
  Val   → Loss: 0.2217 | Recall: 0.9142 | Precision: 0.9579 | F1: 0.9356 | AUC: 0.9824
  LR: 5.00e-06



Epoch 17 [Train]: 100%|██████████| 4323/4323 [02:02&lt;00:00, 35.42it/s]
Epoch 17 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 55.39it/s]



Epoch 17:
  Train → Loss: 0.0589 | Recall: 0.9743 | Precision: 0.9819 | F1: 0.9780 | AUC: 0.9976
  Val   → Loss: 0.2529 | Recall: 0.9099 | Precision: 0.9722 | F1: 0.9400 | AUC: 0.9820
  LR: 5.00e-06



Epoch 18 [Train]: 100%|██████████| 4323/4323 [02:00&lt;00:00, 35.98it/s]
Epoch 18 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 55.07it/s]



Epoch 18:
  Train → Loss: 0.0569 | Recall: 0.9762 | Precision: 0.9812 | F1: 0.9787 | AUC: 0.9978
  Val   → Loss: 0.2564 | Recall: 0.9077 | Precision: 0.9642 | F1: 0.9351 | AUC: 0.9801
  LR: 2.50e-06



Epoch 19 [Train]: 100%|██████████| 4323/4323 [02:00&lt;00:00, 35.96it/s]
Epoch 19 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 50.87it/s]



Epoch 19:
  Train → Loss: 0.0516 | Recall: 0.9774 | Precision: 0.9837 | F1: 0.9806 | AUC: 0.9982
  Val   → Loss: 0.2314 | Recall: 0.9186 | Precision: 0.9581 | F1: 0.9379 | AUC: 0.9817
  LR: 2.50e-06



Epoch 20 [Train]: 100%|██████████| 4323/4323 [02:00&lt;00:00, 35.76it/s]
Epoch 20 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 54.26it/s]



Epoch 20:
  Train → Loss: 0.0485 | Recall: 0.9795 | Precision: 0.9859 | F1: 0.9827 | AUC: 0.9983
  Val   → Loss: 0.2575 | Recall: 0.9153 | Precision: 0.9580 | F1: 0.9361 | AUC: 0.9810
  LR: 2.50e-06

Early stopping at epoch 20

FOLD 2 FINAL RESULTS:


Recall: 0.9305 | Precision: 0.9428 | F1: 0.9366 | AUC: 0.9824 | Loss: 0.2084
Fold 2: Checkpoint saved.


==================================================
FOLD 3 / 5
==================================================


Augmenting train set...: 100%|██████████| 7684/7684 [01:50&lt;00:00, 69.54it/s]



Train samples: 69156 | Val samples: 1921



Epoch 1 [Train]: 100%|██████████| 4323/4323 [01:29&lt;00:00, 48.05it/s]
Epoch 1 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 52.38it/s]



Epoch 1:
  Train → Loss: 0.2952 | Recall: 0.8547 | Precision: 0.8805 | F1: 0.8674 | AUC: 0.9459
  Val   → Loss: 0.2085 | Recall: 0.8697 | Precision: 0.9524 | F1: 0.9092 | AUC: 0.9741
  LR: 3.33e-05

Model saved! (Best recall: 0.8697)


Epoch 2 [Train]: 100%|██████████| 4323/4323 [01:29&lt;00:00, 48.05it/s]
Epoch 2 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 52.61it/s]



Epoch 2:
  Train → Loss: 0.2438 | Recall: 0.8855 | Precision: 0.9072 | F1: 0.8963 | AUC: 0.9631
  Val   → Loss: 0.1921 | Recall: 0.8860 | Precision: 0.9477 | F1: 0.9158 | AUC: 0.9768
  LR: 6.67e-05

Model saved! (Best recall: 0.8860)


Epoch 3 [Train]: 100%|██████████| 4323/4323 [01:29&lt;00:00, 48.21it/s]
Epoch 3 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 55.24it/s]



Epoch 3:
  Train → Loss: 0.2268 | Recall: 0.8963 | Precision: 0.9133 | F1: 0.9047 | AUC: 0.9685
  Val   → Loss: 0.1950 | Recall: 0.8903 | Precision: 0.9568 | F1: 0.9224 | AUC: 0.9771
  LR: 1.00e-04

Model saved! (Best recall: 0.8903)


Epoch 4 [Train]: 100%|██████████| 4323/4323 [01:30&lt;00:00, 48.01it/s]
Epoch 4 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 54.85it/s]



Epoch 4:
  Train → Loss: 0.2160 | Recall: 0.9024 | Precision: 0.9178 | F1: 0.9100 | AUC: 0.9715
  Val   → Loss: 0.1920 | Recall: 0.9034 | Precision: 0.9455 | F1: 0.9239 | AUC: 0.9773
  LR: 1.00e-04

Model saved! (Best recall: 0.9034)


Epoch 5 [Train]: 100%|██████████| 4323/4323 [01:29&lt;00:00, 48.06it/s]
Epoch 5 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 53.02it/s]



Epoch 5:
  Train → Loss: 0.2035 | Recall: 0.9087 | Precision: 0.9234 | F1: 0.9160 | AUC: 0.9747
  Val   → Loss: 0.1959 | Recall: 0.9175 | Precision: 0.9286 | F1: 0.9230 | AUC: 0.9770
  LR: 1.00e-04

Model saved! (Best recall: 0.9175)


Epoch 6 [Train]: 100%|██████████| 4323/4323 [01:30&lt;00:00, 47.70it/s]
Epoch 6 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 55.93it/s]



Epoch 6:
  Train → Loss: 0.1936 | Recall: 0.9140 | Precision: 0.9289 | F1: 0.9214 | AUC: 0.9771
  Val   → Loss: 0.1937 | Recall: 0.8990 | Precision: 0.9495 | F1: 0.9236 | AUC: 0.9795
  LR: 1.00e-04



Epoch 7 [Train]: 100%|██████████| 4323/4323 [01:29&lt;00:00, 48.19it/s]
Epoch 7 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 54.10it/s]



Epoch 7:
  Train → Loss: 0.1894 | Recall: 0.9160 | Precision: 0.9305 | F1: 0.9232 | AUC: 0.9781
  Val   → Loss: 0.1859 | Recall: 0.9023 | Precision: 0.9475 | F1: 0.9244 | AUC: 0.9794
  LR: 1.00e-04



Epoch 8 [Train]: 100%|██████████| 4323/4323 [01:29&lt;00:00, 48.07it/s]
Epoch 8 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 52.97it/s]



Epoch 8:
  Train → Loss: 0.1830 | Recall: 0.9195 | Precision: 0.9346 | F1: 0.9270 | AUC: 0.9796
  Val   → Loss: 0.1907 | Recall: 0.9034 | Precision: 0.9498 | F1: 0.9260 | AUC: 0.9792
  LR: 1.00e-04

Fine-tuning activated at epoch 9


Epoch 9 [Train]: 100%|██████████| 4323/4323 [02:00&lt;00:00, 35.84it/s]
Epoch 9 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 53.97it/s]



Epoch 9:
  Train → Loss: 0.1547 | Recall: 0.9318 | Precision: 0.9464 | F1: 0.9391 | AUC: 0.9852
  Val   → Loss: 0.2027 | Recall: 0.8958 | Precision: 0.9593 | F1: 0.9264 | AUC: 0.9816
  LR: 1.00e-05



Epoch 10 [Train]: 100%|██████████| 4323/4323 [02:02&lt;00:00, 35.35it/s]
Epoch 10 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 54.16it/s]



Epoch 10:
  Train → Loss: 0.1339 | Recall: 0.9397 | Precision: 0.9554 | F1: 0.9475 | AUC: 0.9888
  Val   → Loss: 0.1938 | Recall: 0.9099 | Precision: 0.9566 | F1: 0.9327 | AUC: 0.9830
  LR: 1.00e-05



Epoch 11 [Train]: 100%|██████████| 4323/4323 [02:00&lt;00:00, 35.74it/s]
Epoch 11 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 50.91it/s]



Epoch 11:
  Train → Loss: 0.1169 | Recall: 0.9463 | Precision: 0.9619 | F1: 0.9541 | AUC: 0.9913
  Val   → Loss: 0.2013 | Recall: 0.8990 | Precision: 0.9583 | F1: 0.9277 | AUC: 0.9842
  LR: 1.00e-05

Early stopping at epoch 11

FOLD 3 FINAL RESULTS:


Recall: 0.9175 | Precision: 0.9286 | F1: 0.9230 | AUC: 0.9770 | Loss: 0.1959
Fold 3: Checkpoint saved.


==================================================
FOLD 4 / 5
==================================================


Augmenting train set...: 100%|██████████| 7684/7684 [01:50&lt;00:00, 69.51it/s]



Train samples: 69156 | Val samples: 1921



Epoch 1 [Train]: 100%|██████████| 4323/4323 [01:30&lt;00:00, 47.83it/s]
Epoch 1 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 53.52it/s]



Epoch 1:
  Train → Loss: 0.2898 | Recall: 0.8599 | Precision: 0.8867 | F1: 0.8731 | AUC: 0.9477
  Val   → Loss: 0.2333 | Recall: 0.8599 | Precision: 0.9395 | F1: 0.8980 | AUC: 0.9671
  LR: 3.33e-05

Model saved! (Best recall: 0.8599)


Epoch 2 [Train]: 100%|██████████| 4323/4323 [01:29&lt;00:00, 48.17it/s]
Epoch 2 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 54.93it/s]



Epoch 2:
  Train → Loss: 0.2429 | Recall: 0.8891 | Precision: 0.9070 | F1: 0.8979 | AUC: 0.9633
  Val   → Loss: 0.2233 | Recall: 0.8697 | Precision: 0.9379 | F1: 0.9025 | AUC: 0.9704
  LR: 6.67e-05

Model saved! (Best recall: 0.8697)


Epoch 3 [Train]: 100%|██████████| 4323/4323 [01:30&lt;00:00, 47.69it/s]
Epoch 3 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 51.55it/s]



Epoch 3:
  Train → Loss: 0.2248 | Recall: 0.8975 | Precision: 0.9162 | F1: 0.9068 | AUC: 0.9688
  Val   → Loss: 0.2411 | Recall: 0.8545 | Precision: 0.9505 | F1: 0.8999 | AUC: 0.9708
  LR: 1.00e-04



Epoch 4 [Train]: 100%|██████████| 4323/4323 [01:29&lt;00:00, 48.13it/s]
Epoch 4 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 51.59it/s]



Epoch 4:
  Train → Loss: 0.2094 | Recall: 0.9062 | Precision: 0.9225 | F1: 0.9143 | AUC: 0.9731
  Val   → Loss: 0.2197 | Recall: 0.8817 | Precision: 0.9355 | F1: 0.9078 | AUC: 0.9722
  LR: 1.00e-04

Model saved! (Best recall: 0.8817)


Epoch 5 [Train]: 100%|██████████| 4323/4323 [01:30&lt;00:00, 47.70it/s]
Epoch 5 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 52.04it/s]



Epoch 5:
  Train → Loss: 0.2001 | Recall: 0.9095 | Precision: 0.9264 | F1: 0.9179 | AUC: 0.9754
  Val   → Loss: 0.2123 | Recall: 0.8882 | Precision: 0.9349 | F1: 0.9109 | AUC: 0.9721
  LR: 1.00e-04

Model saved! (Best recall: 0.8882)


Epoch 6 [Train]: 100%|██████████| 4323/4323 [01:30&lt;00:00, 47.71it/s]
Epoch 6 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 53.85it/s]



Epoch 6:
  Train → Loss: 0.1897 | Recall: 0.9153 | Precision: 0.9300 | F1: 0.9226 | AUC: 0.9779
  Val   → Loss: 0.2344 | Recall: 0.8545 | Precision: 0.9539 | F1: 0.9015 | AUC: 0.9732
  LR: 1.00e-04



Epoch 7 [Train]: 100%|██████████| 4323/4323 [01:30&lt;00:00, 47.61it/s]
Epoch 7 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 51.83it/s]



Epoch 7:
  Train → Loss: 0.1843 | Recall: 0.9164 | Precision: 0.9339 | F1: 0.9251 | AUC: 0.9792
  Val   → Loss: 0.2267 | Recall: 0.8882 | Precision: 0.9370 | F1: 0.9119 | AUC: 0.9730
  LR: 1.00e-04



Epoch 8 [Train]: 100%|██████████| 4323/4323 [01:30&lt;00:00, 47.99it/s]
Epoch 8 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 49.83it/s]



Epoch 8:
  Train → Loss: 0.1764 | Recall: 0.9221 | Precision: 0.9368 | F1: 0.9294 | AUC: 0.9809
  Val   → Loss: 0.2257 | Recall: 0.9001 | Precision: 0.9150 | F1: 0.9075 | AUC: 0.9722
  LR: 1.00e-04

Model saved! (Best recall: 0.9001)
Fine-tuning activated at epoch 9


Epoch 9 [Train]: 100%|██████████| 4323/4323 [02:02&lt;00:00, 35.33it/s]
Epoch 9 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 53.00it/s]



Epoch 9:
  Train → Loss: 0.1567 | Recall: 0.9317 | Precision: 0.9482 | F1: 0.9398 | AUC: 0.9850
  Val   → Loss: 0.2011 | Recall: 0.9045 | Precision: 0.9266 | F1: 0.9154 | AUC: 0.9788
  LR: 1.00e-05

Model saved! (Best recall: 0.9045)


Epoch 10 [Train]: 100%|██████████| 4323/4323 [02:01&lt;00:00, 35.52it/s]
Epoch 10 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 50.13it/s]



Epoch 10:
  Train → Loss: 0.1308 | Recall: 0.9426 | Precision: 0.9576 | F1: 0.9500 | AUC: 0.9891
  Val   → Loss: 0.2114 | Recall: 0.8958 | Precision: 0.9560 | F1: 0.9249 | AUC: 0.9805
  LR: 1.00e-05



Epoch 11 [Train]: 100%|██████████| 4323/4323 [02:00&lt;00:00, 35.87it/s]
Epoch 11 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 54.85it/s]



Epoch 11:
  Train → Loss: 0.1116 | Recall: 0.9488 | Precision: 0.9655 | F1: 0.9571 | AUC: 0.9918
  Val   → Loss: 0.2094 | Recall: 0.9034 | Precision: 0.9519 | F1: 0.9270 | AUC: 0.9808
  LR: 1.00e-05



Epoch 12 [Train]: 100%|██████████| 4323/4323 [02:00&lt;00:00, 35.83it/s]
Epoch 12 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 52.59it/s]



Epoch 12:
  Train → Loss: 0.0999 | Recall: 0.9553 | Precision: 0.9704 | F1: 0.9628 | AUC: 0.9934
  Val   → Loss: 0.2168 | Recall: 0.9110 | Precision: 0.9374 | F1: 0.9240 | AUC: 0.9801
  LR: 1.00e-05

Model saved! (Best recall: 0.9110)


Epoch 13 [Train]: 100%|██████████| 4323/4323 [02:01&lt;00:00, 35.54it/s]
Epoch 13 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 51.53it/s]



Epoch 13:
  Train → Loss: 0.0873 | Recall: 0.9622 | Precision: 0.9732 | F1: 0.9677 | AUC: 0.9951
  Val   → Loss: 0.2108 | Recall: 0.9164 | Precision: 0.9462 | F1: 0.9311 | AUC: 0.9810
  LR: 5.00e-06

Model saved! (Best recall: 0.9164)


Epoch 14 [Train]: 100%|██████████| 4323/4323 [02:00&lt;00:00, 35.80it/s]
Epoch 14 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 51.50it/s]



Epoch 14:
  Train → Loss: 0.0758 | Recall: 0.9659 | Precision: 0.9773 | F1: 0.9716 | AUC: 0.9961
  Val   → Loss: 0.2218 | Recall: 0.9164 | Precision: 0.9451 | F1: 0.9305 | AUC: 0.9816
  LR: 5.00e-06



Epoch 15 [Train]: 100%|██████████| 4323/4323 [01:59&lt;00:00, 36.07it/s]
Epoch 15 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 52.74it/s]



Epoch 15:
  Train → Loss: 0.0731 | Recall: 0.9694 | Precision: 0.9780 | F1: 0.9737 | AUC: 0.9965
  Val   → Loss: 0.2395 | Recall: 0.9077 | Precision: 0.9468 | F1: 0.9268 | AUC: 0.9800
  LR: 5.00e-06



Epoch 16 [Train]: 100%|██████████| 4323/4323 [02:01&lt;00:00, 35.64it/s]
Epoch 16 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 54.32it/s]



Epoch 16:
  Train → Loss: 0.0636 | Recall: 0.9721 | Precision: 0.9806 | F1: 0.9763 | AUC: 0.9972
  Val   → Loss: 0.2484 | Recall: 0.9153 | Precision: 0.9325 | F1: 0.9238 | AUC: 0.9785
  LR: 5.00e-06



Epoch 17 [Train]: 100%|██████████| 4323/4323 [02:00&lt;00:00, 36.02it/s]
Epoch 17 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 51.93it/s]



Epoch 17:
  Train → Loss: 0.0623 | Recall: 0.9737 | Precision: 0.9813 | F1: 0.9775 | AUC: 0.9973
  Val   → Loss: 0.2364 | Recall: 0.9164 | Precision: 0.9451 | F1: 0.9305 | AUC: 0.9811
  LR: 2.50e-06



Epoch 18 [Train]: 100%|██████████| 4323/4323 [02:02&lt;00:00, 35.29it/s]
Epoch 18 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 54.50it/s]



Epoch 18:
  Train → Loss: 0.0574 | Recall: 0.9746 | Precision: 0.9823 | F1: 0.9785 | AUC: 0.9977
  Val   → Loss: 0.2405 | Recall: 0.9088 | Precision: 0.9544 | F1: 0.9310 | AUC: 0.9822
  LR: 2.50e-06



Epoch 19 [Train]: 100%|██████████| 4323/4323 [02:01&lt;00:00, 35.70it/s]
Epoch 19 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 54.69it/s]



Epoch 19:
  Train → Loss: 0.0572 | Recall: 0.9764 | Precision: 0.9833 | F1: 0.9799 | AUC: 0.9977
  Val   → Loss: 0.2503 | Recall: 0.9262 | Precision: 0.9182 | F1: 0.9222 | AUC: 0.9804
  LR: 2.50e-06

Model saved! (Best recall: 0.9262)


Epoch 20 [Train]: 100%|██████████| 4323/4323 [02:01&lt;00:00, 35.64it/s]
Epoch 20 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 50.00it/s]



Epoch 20:
  Train → Loss: 0.0545 | Recall: 0.9768 | Precision: 0.9832 | F1: 0.9800 | AUC: 0.9979
  Val   → Loss: 0.2412 | Recall: 0.9175 | Precision: 0.9484 | F1: 0.9327 | AUC: 0.9820
  LR: 2.50e-06



Epoch 21 [Train]: 100%|██████████| 4323/4323 [01:59&lt;00:00, 36.11it/s]
Epoch 21 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 48.62it/s]



Epoch 21:
  Train → Loss: 0.0519 | Recall: 0.9779 | Precision: 0.9853 | F1: 0.9816 | AUC: 0.9980
  Val   → Loss: 0.2464 | Recall: 0.9164 | Precision: 0.9357 | F1: 0.9259 | AUC: 0.9800
  LR: 1.25e-06



Epoch 22 [Train]: 100%|██████████| 4323/4323 [02:02&lt;00:00, 35.35it/s]
Epoch 22 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 53.27it/s]



Epoch 22:
  Train → Loss: 0.0494 | Recall: 0.9793 | Precision: 0.9851 | F1: 0.9822 | AUC: 0.9982
  Val   → Loss: 0.2609 | Recall: 0.9088 | Precision: 0.9566 | F1: 0.9321 | AUC: 0.9817
  LR: 1.25e-06



Epoch 23 [Train]: 100%|██████████| 4323/4323 [02:00&lt;00:00, 35.74it/s]
Epoch 23 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 52.02it/s]



Epoch 23:
  Train → Loss: 0.0492 | Recall: 0.9784 | Precision: 0.9853 | F1: 0.9819 | AUC: 0.9983
  Val   → Loss: 0.2508 | Recall: 0.9218 | Precision: 0.9392 | F1: 0.9304 | AUC: 0.9808
  LR: 1.25e-06



Epoch 24 [Train]: 100%|██████████| 4323/4323 [02:01&lt;00:00, 35.67it/s]
Epoch 24 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 50.71it/s]



Epoch 24:
  Train → Loss: 0.0531 | Recall: 0.9802 | Precision: 0.9850 | F1: 0.9826 | AUC: 0.9981
  Val   → Loss: 0.2495 | Recall: 0.9218 | Precision: 0.9423 | F1: 0.9319 | AUC: 0.9816
  LR: 1.25e-06



Epoch 25 [Train]: 100%|██████████| 4323/4323 [02:00&lt;00:00, 35.74it/s]
Epoch 25 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 53.22it/s]



Epoch 25:
  Train → Loss: 0.0474 | Recall: 0.9801 | Precision: 0.9866 | F1: 0.9833 | AUC: 0.9983
  Val   → Loss: 0.2694 | Recall: 0.9034 | Precision: 0.9476 | F1: 0.9250 | AUC: 0.9806
  LR: 6.25e-07

Early stopping at epoch 25

FOLD 4 FINAL RESULTS:


Recall: 0.9262 | Precision: 0.9182 | F1: 0.9222 | AUC: 0.9804 | Loss: 0.2503
Fold 4: Checkpoint saved.


==================================================
FOLD 5 / 5
==================================================


Augmenting train set...: 100%|██████████| 7684/7684 [01:51&lt;00:00, 69.18it/s]



Train samples: 69156 | Val samples: 1921



Epoch 1 [Train]: 100%|██████████| 4323/4323 [01:29&lt;00:00, 48.25it/s]
Epoch 1 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 52.55it/s]



Epoch 1:
  Train → Loss: 0.2950 | Recall: 0.8579 | Precision: 0.8816 | F1: 0.8696 | AUC: 0.9458
  Val   → Loss: 0.2251 | Recall: 0.8882 | Precision: 0.9233 | F1: 0.9054 | AUC: 0.9679
  LR: 3.33e-05

Model saved! (Best recall: 0.8882)


Epoch 2 [Train]: 100%|██████████| 4323/4323 [01:30&lt;00:00, 47.59it/s]
Epoch 2 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 55.40it/s]



Epoch 2:
  Train → Loss: 0.2461 | Recall: 0.8837 | Precision: 0.9037 | F1: 0.8936 | AUC: 0.9625
  Val   → Loss: 0.2091 | Recall: 0.8806 | Precision: 0.9441 | F1: 0.9112 | AUC: 0.9733
  LR: 6.67e-05



Epoch 3 [Train]: 100%|██████████| 4323/4323 [01:30&lt;00:00, 47.89it/s]
Epoch 3 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 53.97it/s]



Epoch 3:
  Train → Loss: 0.2337 | Recall: 0.8936 | Precision: 0.9126 | F1: 0.9030 | AUC: 0.9664
  Val   → Loss: 0.2009 | Recall: 0.9023 | Precision: 0.9358 | F1: 0.9187 | AUC: 0.9749
  LR: 1.00e-04

Model saved! (Best recall: 0.9023)


Epoch 4 [Train]: 100%|██████████| 4323/4323 [01:31&lt;00:00, 47.28it/s]
Epoch 4 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 52.15it/s]



Epoch 4:
  Train → Loss: 0.2133 | Recall: 0.9013 | Precision: 0.9208 | F1: 0.9109 | AUC: 0.9719
  Val   → Loss: 0.2128 | Recall: 0.9034 | Precision: 0.9306 | F1: 0.9168 | AUC: 0.9718
  LR: 1.00e-04

Model saved! (Best recall: 0.9034)


Epoch 5 [Train]: 100%|██████████| 4323/4323 [01:29&lt;00:00, 48.26it/s]
Epoch 5 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 51.33it/s]



Epoch 5:
  Train → Loss: 0.2057 | Recall: 0.9064 | Precision: 0.9247 | F1: 0.9155 | AUC: 0.9740
  Val   → Loss: 0.2018 | Recall: 0.8936 | Precision: 0.9537 | F1: 0.9226 | AUC: 0.9762
  LR: 1.00e-04



Epoch 6 [Train]: 100%|██████████| 4323/4323 [01:29&lt;00:00, 48.28it/s]
Epoch 6 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 53.19it/s]



Epoch 6:
  Train → Loss: 0.1945 | Recall: 0.9117 | Precision: 0.9277 | F1: 0.9197 | AUC: 0.9768
  Val   → Loss: 0.2092 | Recall: 0.9045 | Precision: 0.9455 | F1: 0.9245 | AUC: 0.9732
  LR: 1.00e-04

Model saved! (Best recall: 0.9045)


Epoch 7 [Train]: 100%|██████████| 4323/4323 [01:30&lt;00:00, 47.89it/s]
Epoch 7 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 50.76it/s]



Epoch 7:
  Train → Loss: 0.1905 | Recall: 0.9141 | Precision: 0.9315 | F1: 0.9227 | AUC: 0.9779
  Val   → Loss: 0.2137 | Recall: 0.9066 | Precision: 0.9382 | F1: 0.9221 | AUC: 0.9738
  LR: 5.00e-05

Model saved! (Best recall: 0.9066)


Epoch 8 [Train]: 100%|██████████| 4323/4323 [01:30&lt;00:00, 47.63it/s]
Epoch 8 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 50.09it/s]



Epoch 8:
  Train → Loss: 0.1776 | Recall: 0.9192 | Precision: 0.9369 | F1: 0.9280 | AUC: 0.9806
  Val   → Loss: 0.2113 | Recall: 0.8958 | Precision: 0.9407 | F1: 0.9177 | AUC: 0.9743
  LR: 5.00e-05

Fine-tuning activated at epoch 9


Epoch 9 [Train]: 100%|██████████| 4323/4323 [02:01&lt;00:00, 35.63it/s]
Epoch 9 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 51.65it/s]



Epoch 9:
  Train → Loss: 0.1563 | Recall: 0.9293 | Precision: 0.9484 | F1: 0.9388 | AUC: 0.9848
  Val   → Loss: 0.2013 | Recall: 0.9251 | Precision: 0.9425 | F1: 0.9337 | AUC: 0.9784
  LR: 1.00e-05

Model saved! (Best recall: 0.9251)


Epoch 10 [Train]: 100%|██████████| 4323/4323 [02:01&lt;00:00, 35.69it/s]
Epoch 10 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 48.89it/s]



Epoch 10:
  Train → Loss: 0.1315 | Recall: 0.9384 | Precision: 0.9585 | F1: 0.9484 | AUC: 0.9890
  Val   → Loss: 0.2174 | Recall: 0.9121 | Precision: 0.9556 | F1: 0.9333 | AUC: 0.9799
  LR: 1.00e-05



Epoch 11 [Train]: 100%|██████████| 4323/4323 [02:00&lt;00:00, 35.93it/s]
Epoch 11 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 55.57it/s]



Epoch 11:
  Train → Loss: 0.1132 | Recall: 0.9493 | Precision: 0.9655 | F1: 0.9573 | AUC: 0.9916
  Val   → Loss: 0.2278 | Recall: 0.9131 | Precision: 0.9514 | F1: 0.9319 | AUC: 0.9782
  LR: 1.00e-05



Epoch 12 [Train]: 100%|██████████| 4323/4323 [02:00&lt;00:00, 36.01it/s]
Epoch 12 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 50.04it/s]



Epoch 12:
  Train → Loss: 0.0998 | Recall: 0.9556 | Precision: 0.9677 | F1: 0.9616 | AUC: 0.9935
  Val   → Loss: 0.2301 | Recall: 0.9055 | Precision: 0.9653 | F1: 0.9345 | AUC: 0.9801
  LR: 1.00e-05



Epoch 13 [Train]: 100%|██████████| 4323/4323 [02:00&lt;00:00, 35.98it/s]
Epoch 13 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 52.96it/s]



Epoch 13:
  Train → Loss: 0.0884 | Recall: 0.9611 | Precision: 0.9716 | F1: 0.9663 | AUC: 0.9949
  Val   → Loss: 0.2355 | Recall: 0.9066 | Precision: 0.9532 | F1: 0.9293 | AUC: 0.9789
  LR: 5.00e-06



Epoch 14 [Train]: 100%|██████████| 4323/4323 [02:00&lt;00:00, 35.81it/s]
Epoch 14 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 51.58it/s]



Epoch 14:
  Train → Loss: 0.0748 | Recall: 0.9653 | Precision: 0.9782 | F1: 0.9717 | AUC: 0.9962
  Val   → Loss: 0.2410 | Recall: 0.9131 | Precision: 0.9439 | F1: 0.9283 | AUC: 0.9793
  LR: 5.00e-06



Epoch 15 [Train]: 100%|██████████| 4323/4323 [02:00&lt;00:00, 35.75it/s]
Epoch 15 [Val]: 100%|██████████| 121/121 [00:02&lt;00:00, 51.75it/s]



Epoch 15:
  Train → Loss: 0.0701 | Recall: 0.9675 | Precision: 0.9794 | F1: 0.9734 | AUC: 0.9967
  Val   → Loss: 0.2530 | Recall: 0.9153 | Precision: 0.9590 | F1: 0.9367 | AUC: 0.9782
  LR: 5.00e-06

Early stopping at epoch 15

FOLD 5 FINAL RESULTS:


Recall: 0.9251 | Precision: 0.9425 | F1: 0.9337 | AUC: 0.9784 | Loss: 0.2013
Fold 5: Checkpoint saved.</code></pre>
<h3 id="evaluation">Evaluation</h3>
<p>Phew! That was some intense training. Now that it’s over, we get to see what our (and our model’s) hard work produced: success metrics!</p>
<p>In the case of our melanoma classification project, we have chosen to measure success by recall score, which is the ratio of true positives to true positives <em>plus</em> false negatives in our model’s predictions. We want to maximize this ratio, meaning we want to minimize the number of false negatives predicted by our model as our key measure of success.</p>
<p>In practical terms, this means we want to minimize the number of malignant cases that our model misclassifies as benign, because missing a case of malignant skin cancer and allowing the patient’s health to get worse is the worst thing our model could do.</p>
<p>On the other hand, if our model accidentally classifies a case of benign melanoma as malignant, the worst thing that could happen is a person receives some extra medical attention and soon realize that their melanoma is not actually malignant. So we are willing to minimize false negatives at the risk of increasing false positives.</p>
<p>It’s unlikely that we’ll be able to fully eliminate false negatives, but by iterating and tuning our model hyperparameters to maximize recall, we’ll get as close to zero false negatives as possible.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true"></a><span class="co"># Define path to recall curves</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true"></a>recall_curve_path <span class="op">=</span> <span class="ss">f&quot;</span><span class="sc">{</span>checkpoints_folder<span class="sc">}</span><span class="ss">/recall_curves.pkl&quot;</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true"></a></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true"></a><span class="co"># Open recall curve file in write-only mode</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true"></a><span class="cf">with</span> <span class="bu">open</span>(recall_curve_path, <span class="st">&#39;wb&#39;</span>) <span class="im">as</span> rcp:</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true"></a></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true"></a>    <span class="co"># Save recall curve path</span></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true"></a>    pickle.dump(fold_recall_curves, rcp)</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true"></a></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true"></a><span class="co"># Calculate mean validation recall score</span></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true"></a>mean_recall <span class="op">=</span> np.mean(fold_val_recalls)</span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true"></a></span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true"></a><span class="co"># Calculate standard deviation of validation recall score</span></span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true"></a>std_recall <span class="op">=</span> np.std(fold_val_recalls)</span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true"></a></span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true"></a><span class="co"># Calculate confidence interval of recall score</span></span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true"></a>ci_recall <span class="op">=</span> <span class="fl">1.96</span> <span class="op">*</span> std_recall <span class="op">/</span> np.sqrt(config[<span class="st">&#39;k&#39;</span>])</span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true"></a></span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true"></a><span class="co"># Print validation recall summary statistics</span></span>
<span id="cb53-20"><a href="#cb53-20" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;FINAL CROSS-VALIDATION RESULTS</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb53-21"><a href="#cb53-21" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Mean Recall: </span><span class="sc">{</span>mean_recall<span class="sc">:.4f}</span><span class="ss"> ± </span><span class="sc">{</span>std_recall<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb53-22"><a href="#cb53-22" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">95% CI: [</span><span class="sc">{</span>mean_recall <span class="op">-</span> ci_recall<span class="sc">:.4f}</span><span class="ss">, </span><span class="sc">{</span>mean_recall <span class="op">+</span> ci_recall<span class="sc">:.4f}</span><span class="ss">]&quot;</span>)</span>
<span id="cb53-23"><a href="#cb53-23" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Individual Folds: </span><span class="sc">{</span>[<span class="ss">f&#39;</span><span class="sc">{r:.4f}</span><span class="ss">&#39;</span> <span class="cf">for</span> r <span class="kw">in</span> fold_val_recalls]<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb53-24"><a href="#cb53-24" aria-hidden="true"></a></span>
<span id="cb53-25"><a href="#cb53-25" aria-hidden="true"></a><span class="co"># Save validation recall summary statistics to Weights &amp; Biases</span></span>
<span id="cb53-26"><a href="#cb53-26" aria-hidden="true"></a>wandb.log({</span>
<span id="cb53-27"><a href="#cb53-27" aria-hidden="true"></a>    <span class="st">&quot;kfold_mean_val_recall&quot;</span>: mean_recall,</span>
<span id="cb53-28"><a href="#cb53-28" aria-hidden="true"></a>    <span class="st">&quot;kfold_std_val_recall&quot;</span>: std_recall,</span>
<span id="cb53-29"><a href="#cb53-29" aria-hidden="true"></a>    <span class="st">&quot;kfold_ci_val_recall&quot;</span>: ci_recall</span>
<span id="cb53-30"><a href="#cb53-30" aria-hidden="true"></a>})</span>
<span id="cb53-31"><a href="#cb53-31" aria-hidden="true"></a></span>
<span id="cb53-32"><a href="#cb53-32" aria-hidden="true"></a><span class="co"># Notify user that training is complete</span></span>
<span id="cb53-33"><a href="#cb53-33" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Training complete!&quot;</span>)</span></code></pre></div>
<pre><code>FINAL CROSS-VALIDATION RESULTS


Mean Recall: 0.9277 ± 0.0071

95% CI: [0.9214, 0.9339]

Individual Folds: [&#39;0.9392&#39;, &#39;0.9305&#39;, &#39;0.9175&#39;, &#39;0.9262&#39;, &#39;0.9251&#39;]

Training complete!</code></pre>
<p>Looks like our hard work was worth it! Our best-performing models across each fold achieved, on average, <strong>92.77% recall</strong>, which is an impressive score. More importantly, our model showed a <strong>narrow spread of recall across folds (+/- 0.71%)</strong>, which means the architecture we’ve built is consistently performant and stable across folds.</p>
<p>While cross-validation results provide a decent estimate of generalization performance, the validation set ultimately comes from the same distribution as the training set. For an even better estimate of our model’s generalization performance, see our test evaluation later in this notebook.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true"></a><span class="co"># Open recall curves file in read-only mode</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true"></a><span class="cf">with</span> <span class="bu">open</span>(recall_curve_path, <span class="st">&#39;rb&#39;</span>) <span class="im">as</span> rcp:</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true"></a></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true"></a>  <span class="co"># Set visualization theme</span></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true"></a>  sns.set_theme(style <span class="op">=</span> <span class="st">&#39;whitegrid&#39;</span>, context <span class="op">=</span> <span class="st">&#39;talk&#39;</span>, palette <span class="op">=</span> <span class="st">&#39;muted&#39;</span>)</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true"></a></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true"></a>  <span class="co"># Extract K-fold CV recall curves</span></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true"></a>  curves <span class="op">=</span> pickle.load(rcp)</span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true"></a></span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true"></a>  <span class="co"># Create figure on which to plot recall curves</span></span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true"></a>  plt.figure(figsize <span class="op">=</span> (<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true"></a></span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true"></a>  <span class="co"># Iterate over all fold curves</span></span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true"></a>  <span class="cf">for</span> fold, curve <span class="kw">in</span> <span class="bu">enumerate</span>(curves):</span>
<span id="cb55-15"><a href="#cb55-15" aria-hidden="true"></a></span>
<span id="cb55-16"><a href="#cb55-16" aria-hidden="true"></a>    <span class="co"># Create lineplot of fold model recall score curve</span></span>
<span id="cb55-17"><a href="#cb55-17" aria-hidden="true"></a>    sns.lineplot(x <span class="op">=</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(curve) <span class="op">+</span> <span class="dv">1</span>), y <span class="op">=</span> curve,</span>
<span id="cb55-18"><a href="#cb55-18" aria-hidden="true"></a>        label <span class="op">=</span> <span class="ss">f&quot;Fold </span><span class="sc">{</span>fold <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">&quot;</span>,</span>
<span id="cb55-19"><a href="#cb55-19" aria-hidden="true"></a>        alpha <span class="op">=</span> <span class="fl">0.4</span></span>
<span id="cb55-20"><a href="#cb55-20" aria-hidden="true"></a>    )</span>
<span id="cb55-21"><a href="#cb55-21" aria-hidden="true"></a></span>
<span id="cb55-22"><a href="#cb55-22" aria-hidden="true"></a>  <span class="co"># Find maximum length of curves, to account for early stopping</span></span>
<span id="cb55-23"><a href="#cb55-23" aria-hidden="true"></a>  max_length <span class="op">=</span> <span class="bu">max</span>(<span class="bu">len</span>(curve) <span class="cf">for</span> curve <span class="kw">in</span> curves)</span>
<span id="cb55-24"><a href="#cb55-24" aria-hidden="true"></a></span>
<span id="cb55-25"><a href="#cb55-25" aria-hidden="true"></a>  <span class="co"># Compute mean fold model recall score curve, accounting for</span></span>
<span id="cb55-26"><a href="#cb55-26" aria-hidden="true"></a>  <span class="co"># null values in cases of early stopping</span></span>
<span id="cb55-27"><a href="#cb55-27" aria-hidden="true"></a>  mean_curve <span class="op">=</span> np.nanmean(</span>
<span id="cb55-28"><a href="#cb55-28" aria-hidden="true"></a>      np.array([np.pad(curve, (<span class="dv">0</span>, max_length<span class="op">-</span><span class="bu">len</span>(curve)), constant_values<span class="op">=</span>np.nan) <span class="cf">for</span> curve <span class="kw">in</span> curves]),</span>
<span id="cb55-29"><a href="#cb55-29" aria-hidden="true"></a>      axis<span class="op">=</span><span class="dv">0</span></span>
<span id="cb55-30"><a href="#cb55-30" aria-hidden="true"></a>  )</span>
<span id="cb55-31"><a href="#cb55-31" aria-hidden="true"></a></span>
<span id="cb55-32"><a href="#cb55-32" aria-hidden="true"></a>  <span class="co"># Create lineplot of mean fold recall curve</span></span>
<span id="cb55-33"><a href="#cb55-33" aria-hidden="true"></a>  sns.lineplot(x <span class="op">=</span> <span class="bu">range</span>(<span class="dv">1</span>, max_length <span class="op">+</span> <span class="dv">1</span>), y <span class="op">=</span> mean_curve,</span>
<span id="cb55-34"><a href="#cb55-34" aria-hidden="true"></a>      label <span class="op">=</span> <span class="st">&quot;Mean Recall&quot;</span>,</span>
<span id="cb55-35"><a href="#cb55-35" aria-hidden="true"></a>      linewidth <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb55-36"><a href="#cb55-36" aria-hidden="true"></a>  )</span>
<span id="cb55-37"><a href="#cb55-37" aria-hidden="true"></a></span>
<span id="cb55-38"><a href="#cb55-38" aria-hidden="true"></a>  <span class="co"># Set plot title and axis labels</span></span>
<span id="cb55-39"><a href="#cb55-39" aria-hidden="true"></a>  plt.title(<span class="st">&#39;Fold Model Validation Recall across Epochs&#39;</span>)</span>
<span id="cb55-40"><a href="#cb55-40" aria-hidden="true"></a>  plt.xlabel(<span class="st">&#39;Epoch&#39;</span>)</span>
<span id="cb55-41"><a href="#cb55-41" aria-hidden="true"></a>  plt.ylabel(<span class="st">&#39;Validation Recall&#39;</span>)</span>
<span id="cb55-42"><a href="#cb55-42" aria-hidden="true"></a></span>
<span id="cb55-43"><a href="#cb55-43" aria-hidden="true"></a>  <span class="co"># Bound y-axis at minimum and maximum</span></span>
<span id="cb55-44"><a href="#cb55-44" aria-hidden="true"></a>  plt.ylim(<span class="fl">0.88</span>, <span class="fl">0.95</span>)</span>
<span id="cb55-45"><a href="#cb55-45" aria-hidden="true"></a></span>
<span id="cb55-46"><a href="#cb55-46" aria-hidden="true"></a>  <span class="co"># Set plot layout</span></span>
<span id="cb55-47"><a href="#cb55-47" aria-hidden="true"></a>  plt.tight_layout()</span>
<span id="cb55-48"><a href="#cb55-48" aria-hidden="true"></a></span>
<span id="cb55-49"><a href="#cb55-49" aria-hidden="true"></a>  <span class="co"># Reveal plot</span></span>
<span id="cb55-50"><a href="#cb55-50" aria-hidden="true"></a>  plt.show()</span></code></pre></div>
<figure>
<img src="https://theonlineconverter.com/uploads/Checkpoint_3_1765927044_files/Checkpoint_3_1765927044_62_0.png" alt="" /><figcaption>png</figcaption>
</figure>
<p>As you can see, there’s quite a lot of noise in our recall curves. Recall is generally a noisy metric, more sample-dependent than a metric like ROC-AUC, which tracks the probability that a randomly chosen positive is classified as more likely to be positive than a randomly chosen negative. This probabilistic nature smooths out ROC-AUC, while recall tends to vary heavily because it depends on the probability threshold to predict malignancy of our model, which can vary depending on how our data is split between training and validation sets.</p>
<p>Despite this noise, we can clearly see an upward trend in recall, both in each individual fold and in the average of all folds. This upward trend is rapid at first, then slows down as it approaches 0.93 towards the end, which tracks with the typical learning pattern of neural networks. This is precisely why we implemented early stopping, to avoid wasting resources on further training once our model achieves its limit of performance.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true"></a><span class="co"># Notify user that model testing has begun</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;EVALUATING ON TEST SET&quot;</span>)</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true"></a></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true"></a><span class="co"># Configure test dataset path</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true"></a>test_zip <span class="op">=</span> <span class="st">&quot;/content/drive/MyDrive/DATA602_Final_Project/test_resized.zip&quot;</span></span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true"></a></span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true"></a><span class="co"># Define path to extract unzipped test set</span></span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true"></a>unzipped_test <span class="op">=</span> <span class="st">&quot;/content/Unzipped_Test&quot;</span></span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true"></a></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true"></a><span class="co"># Remove unzipped test destination file if</span></span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true"></a><span class="co"># already exists</span></span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true"></a><span class="cf">if</span> os.path.exists(unzipped_test):</span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true"></a>    shutil.rmtree(unzipped_test)</span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true"></a></span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true"></a><span class="co"># Open test dataset zipfile as read-only</span></span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true"></a><span class="cf">with</span> zipfile.ZipFile(test_zip, <span class="st">&#39;r&#39;</span>) <span class="im">as</span> zipped_test:</span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true"></a></span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true"></a>    <span class="co"># Extract unzipped test set to predefined path</span></span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true"></a>    zipped_test.extractall(unzipped_test)</span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true"></a></span>
<span id="cb56-21"><a href="#cb56-21" aria-hidden="true"></a><span class="co"># Load test data with same data preparation pipeline as training</span></span>
<span id="cb56-22"><a href="#cb56-22" aria-hidden="true"></a>test_set <span class="op">=</span> datasets.ImageFolder(root<span class="op">=</span>unzipped_test, transform<span class="op">=</span>data_prep)</span>
<span id="cb56-23"><a href="#cb56-23" aria-hidden="true"></a></span>
<span id="cb56-24"><a href="#cb56-24" aria-hidden="true"></a><span class="co"># Create test DataLoader to serve batches of test data to our model</span></span>
<span id="cb56-25"><a href="#cb56-25" aria-hidden="true"></a>test_loader <span class="op">=</span> DataLoader(test_set, batch_size<span class="op">=</span>config[<span class="st">&#39;batch_size&#39;</span>],</span>
<span id="cb56-26"><a href="#cb56-26" aria-hidden="true"></a>                         shuffle<span class="op">=</span><span class="va">False</span>, num_workers<span class="op">=</span><span class="dv">2</span>, pin_memory<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb56-27"><a href="#cb56-27" aria-hidden="true"></a></span>
<span id="cb56-28"><a href="#cb56-28" aria-hidden="true"></a><span class="co"># Notify user of test set size</span></span>
<span id="cb56-29"><a href="#cb56-29" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;Test samples: </span><span class="sc">{</span><span class="bu">len</span>(test_set)<span class="sc">}</span><span class="ch">\n</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>EVALUATING ON TEST SET
Test samples: 1000</code></pre>
<div class="sourceCode" id="cb58"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true"></a><span class="co"># Create empty list to hold test recall scores</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true"></a>test_recalls <span class="op">=</span> []</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true"></a></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true"></a><span class="co"># Create empty list to hold test precision scores</span></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true"></a>test_precisions <span class="op">=</span> []</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true"></a></span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true"></a><span class="co"># Create empty list to hold test F1 scores</span></span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true"></a>test_f1s <span class="op">=</span> []</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true"></a></span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true"></a><span class="co"># Create empty list to hold test ROC-AUC scores</span></span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true"></a>test_aucs <span class="op">=</span> []</span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true"></a></span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true"></a><span class="co"># Create variable to hold best recall</span></span>
<span id="cb58-14"><a href="#cb58-14" aria-hidden="true"></a>best_recall <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb58-15"><a href="#cb58-15" aria-hidden="true"></a></span>
<span id="cb58-16"><a href="#cb58-16" aria-hidden="true"></a><span class="co"># Create variable to hold best-performing fold number</span></span>
<span id="cb58-17"><a href="#cb58-17" aria-hidden="true"></a>best_fold <span class="op">=</span> <span class="va">None</span></span>
<span id="cb58-18"><a href="#cb58-18" aria-hidden="true"></a></span>
<span id="cb58-19"><a href="#cb58-19" aria-hidden="true"></a><span class="co"># Create list to hold false negatives</span></span>
<span id="cb58-20"><a href="#cb58-20" aria-hidden="true"></a><span class="co"># of best-performing model</span></span>
<span id="cb58-21"><a href="#cb58-21" aria-hidden="true"></a>best_fn_list <span class="op">=</span> []</span>
<span id="cb58-22"><a href="#cb58-22" aria-hidden="true"></a></span>
<span id="cb58-23"><a href="#cb58-23" aria-hidden="true"></a><span class="co"># Iterate over all K-folds</span></span>
<span id="cb58-24"><a href="#cb58-24" aria-hidden="true"></a><span class="cf">for</span> fold <span class="kw">in</span> <span class="bu">range</span>(config[<span class="st">&#39;k&#39;</span>]):</span>
<span id="cb58-25"><a href="#cb58-25" aria-hidden="true"></a></span>
<span id="cb58-26"><a href="#cb58-26" aria-hidden="true"></a>    <span class="co"># Notify user of which fold we&#39;re evaluating</span></span>
<span id="cb58-27"><a href="#cb58-27" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Evaluating Fold </span><span class="sc">{</span>fold <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss"> model:&quot;</span>)</span>
<span id="cb58-28"><a href="#cb58-28" aria-hidden="true"></a></span>
<span id="cb58-29"><a href="#cb58-29" aria-hidden="true"></a>    <span class="co"># Create list to hold false negatives</span></span>
<span id="cb58-30"><a href="#cb58-30" aria-hidden="true"></a>    fn_list <span class="op">=</span> []</span>
<span id="cb58-31"><a href="#cb58-31" aria-hidden="true"></a></span>
<span id="cb58-32"><a href="#cb58-32" aria-hidden="true"></a>    <span class="co"># Load best model from current fold</span></span>
<span id="cb58-33"><a href="#cb58-33" aria-hidden="true"></a>    model_path <span class="op">=</span> <span class="ss">f&quot;</span><span class="sc">{</span>checkpoints_folder<span class="sc">}</span><span class="ss">/best_model_fold</span><span class="sc">{</span>fold<span class="sc">}</span><span class="ss">.pt&quot;</span></span>
<span id="cb58-34"><a href="#cb58-34" aria-hidden="true"></a>    model.load_state_dict(torch.load(model_path))</span>
<span id="cb58-35"><a href="#cb58-35" aria-hidden="true"></a></span>
<span id="cb58-36"><a href="#cb58-36" aria-hidden="true"></a>    <span class="co"># Set model to evaluation mode</span></span>
<span id="cb58-37"><a href="#cb58-37" aria-hidden="true"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb58-38"><a href="#cb58-38" aria-hidden="true"></a></span>
<span id="cb58-39"><a href="#cb58-39" aria-hidden="true"></a>    <span class="co"># Create list to hold predictions, true class labels,</span></span>
<span id="cb58-40"><a href="#cb58-40" aria-hidden="true"></a>    <span class="co"># and predicted probabilities</span></span>
<span id="cb58-41"><a href="#cb58-41" aria-hidden="true"></a>    test_preds, test_labels, test_probs <span class="op">=</span> [], [], []</span>
<span id="cb58-42"><a href="#cb58-42" aria-hidden="true"></a></span>
<span id="cb58-43"><a href="#cb58-43" aria-hidden="true"></a>    <span class="co"># Turn off gradient tracking (loss computation)</span></span>
<span id="cb58-44"><a href="#cb58-44" aria-hidden="true"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb58-45"><a href="#cb58-45" aria-hidden="true"></a></span>
<span id="cb58-46"><a href="#cb58-46" aria-hidden="true"></a>        <span class="co"># Iterate over all images in each batch</span></span>
<span id="cb58-47"><a href="#cb58-47" aria-hidden="true"></a>        <span class="cf">for</span> batch_i, (images, labels_batch) <span class="kw">in</span> <span class="bu">enumerate</span>(tqdm(test_loader, desc<span class="op">=</span><span class="ss">f&quot;Fold </span><span class="sc">{</span>fold<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> Test&quot;</span>)):</span>
<span id="cb58-48"><a href="#cb58-48" aria-hidden="true"></a></span>
<span id="cb58-49"><a href="#cb58-49" aria-hidden="true"></a>            <span class="co"># Load images and class labels to device</span></span>
<span id="cb58-50"><a href="#cb58-50" aria-hidden="true"></a>            images <span class="op">=</span> images.to(chip)</span>
<span id="cb58-51"><a href="#cb58-51" aria-hidden="true"></a>            labels_batch <span class="op">=</span> labels_batch.<span class="bu">float</span>().unsqueeze(<span class="dv">1</span>).to(chip)</span>
<span id="cb58-52"><a href="#cb58-52" aria-hidden="true"></a></span>
<span id="cb58-53"><a href="#cb58-53" aria-hidden="true"></a>            <span class="co"># Predict class of images using model</span></span>
<span id="cb58-54"><a href="#cb58-54" aria-hidden="true"></a>            outputs <span class="op">=</span> model(images)</span>
<span id="cb58-55"><a href="#cb58-55" aria-hidden="true"></a></span>
<span id="cb58-56"><a href="#cb58-56" aria-hidden="true"></a>            <span class="co"># Extract predicted class labels</span></span>
<span id="cb58-57"><a href="#cb58-57" aria-hidden="true"></a>            pred_labels <span class="op">=</span> (outputs <span class="op">&gt;</span> <span class="fl">0.5</span>).cpu().numpy().flatten()</span>
<span id="cb58-58"><a href="#cb58-58" aria-hidden="true"></a></span>
<span id="cb58-59"><a href="#cb58-59" aria-hidden="true"></a>            <span class="co"># Extract true class labels</span></span>
<span id="cb58-60"><a href="#cb58-60" aria-hidden="true"></a>            true_labels <span class="op">=</span> labels_batch.cpu().numpy().flatten()</span>
<span id="cb58-61"><a href="#cb58-61" aria-hidden="true"></a></span>
<span id="cb58-62"><a href="#cb58-62" aria-hidden="true"></a>            <span class="co"># Add batch predictions to list</span></span>
<span id="cb58-63"><a href="#cb58-63" aria-hidden="true"></a>            test_preds.extend(pred_labels)</span>
<span id="cb58-64"><a href="#cb58-64" aria-hidden="true"></a></span>
<span id="cb58-65"><a href="#cb58-65" aria-hidden="true"></a>            <span class="co"># Add batch true labels to list</span></span>
<span id="cb58-66"><a href="#cb58-66" aria-hidden="true"></a>            test_labels.extend(true_labels)</span>
<span id="cb58-67"><a href="#cb58-67" aria-hidden="true"></a></span>
<span id="cb58-68"><a href="#cb58-68" aria-hidden="true"></a>            <span class="co"># Add batch predicted probabilities to list</span></span>
<span id="cb58-69"><a href="#cb58-69" aria-hidden="true"></a>            test_probs.extend(outputs.cpu().numpy().flatten())</span>
<span id="cb58-70"><a href="#cb58-70" aria-hidden="true"></a></span>
<span id="cb58-71"><a href="#cb58-71" aria-hidden="true"></a>            <span class="co"># Define batch starting index</span></span>
<span id="cb58-72"><a href="#cb58-72" aria-hidden="true"></a>            batch_start <span class="op">=</span> batch_i <span class="op">*</span> config[<span class="st">&#39;batch_size&#39;</span>]</span>
<span id="cb58-73"><a href="#cb58-73" aria-hidden="true"></a></span>
<span id="cb58-74"><a href="#cb58-74" aria-hidden="true"></a>            <span class="co"># Iterate over all extracted labels</span></span>
<span id="cb58-75"><a href="#cb58-75" aria-hidden="true"></a>            <span class="cf">for</span> i, (t, p) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(true_labels, pred_labels)):</span>
<span id="cb58-76"><a href="#cb58-76" aria-hidden="true"></a></span>
<span id="cb58-77"><a href="#cb58-77" aria-hidden="true"></a>                <span class="co"># If model prediction is false negative</span></span>
<span id="cb58-78"><a href="#cb58-78" aria-hidden="true"></a>                <span class="cf">if</span> t <span class="op">==</span> <span class="dv">1</span> <span class="kw">and</span> p <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb58-79"><a href="#cb58-79" aria-hidden="true"></a></span>
<span id="cb58-80"><a href="#cb58-80" aria-hidden="true"></a>                  <span class="co"># Append index to false negative list</span></span>
<span id="cb58-81"><a href="#cb58-81" aria-hidden="true"></a>                  fn_list.append(batch_start <span class="op">+</span> i)</span>
<span id="cb58-82"><a href="#cb58-82" aria-hidden="true"></a></span>
<span id="cb58-83"><a href="#cb58-83" aria-hidden="true"></a>    <span class="co"># Compute test set recall score</span></span>
<span id="cb58-84"><a href="#cb58-84" aria-hidden="true"></a>    test_recall <span class="op">=</span> recall_score(test_labels, test_preds, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb58-85"><a href="#cb58-85" aria-hidden="true"></a></span>
<span id="cb58-86"><a href="#cb58-86" aria-hidden="true"></a>    <span class="co"># Compute test set precision score</span></span>
<span id="cb58-87"><a href="#cb58-87" aria-hidden="true"></a>    test_precision <span class="op">=</span> precision_score(test_labels, test_preds, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb58-88"><a href="#cb58-88" aria-hidden="true"></a></span>
<span id="cb58-89"><a href="#cb58-89" aria-hidden="true"></a>    <span class="co"># Compute test set F1 score</span></span>
<span id="cb58-90"><a href="#cb58-90" aria-hidden="true"></a>    test_f1 <span class="op">=</span> f1_score(test_labels, test_preds, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb58-91"><a href="#cb58-91" aria-hidden="true"></a></span>
<span id="cb58-92"><a href="#cb58-92" aria-hidden="true"></a>    <span class="co"># Compute test set ROC-AUC score</span></span>
<span id="cb58-93"><a href="#cb58-93" aria-hidden="true"></a>    test_auc <span class="op">=</span> roc_auc_score(test_labels, test_probs) <span class="cf">if</span> <span class="bu">len</span>(<span class="bu">set</span>(test_labels)) <span class="op">&gt;</span> <span class="dv">1</span> <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb58-94"><a href="#cb58-94" aria-hidden="true"></a></span>
<span id="cb58-95"><a href="#cb58-95" aria-hidden="true"></a>    <span class="co"># Append test recall score to list</span></span>
<span id="cb58-96"><a href="#cb58-96" aria-hidden="true"></a>    test_recalls.append(test_recall)</span>
<span id="cb58-97"><a href="#cb58-97" aria-hidden="true"></a></span>
<span id="cb58-98"><a href="#cb58-98" aria-hidden="true"></a>    <span class="co"># Append test precision score to list</span></span>
<span id="cb58-99"><a href="#cb58-99" aria-hidden="true"></a>    test_precisions.append(test_precision)</span>
<span id="cb58-100"><a href="#cb58-100" aria-hidden="true"></a></span>
<span id="cb58-101"><a href="#cb58-101" aria-hidden="true"></a>    <span class="co"># Append test F1 score to list</span></span>
<span id="cb58-102"><a href="#cb58-102" aria-hidden="true"></a>    test_f1s.append(test_f1)</span>
<span id="cb58-103"><a href="#cb58-103" aria-hidden="true"></a></span>
<span id="cb58-104"><a href="#cb58-104" aria-hidden="true"></a>    <span class="co"># Append test ROC-AUC score to list</span></span>
<span id="cb58-105"><a href="#cb58-105" aria-hidden="true"></a>    test_aucs.append(test_auc)</span>
<span id="cb58-106"><a href="#cb58-106" aria-hidden="true"></a></span>
<span id="cb58-107"><a href="#cb58-107" aria-hidden="true"></a>    <span class="co"># Notify user of fold model test performance</span></span>
<span id="cb58-108"><a href="#cb58-108" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Fold </span><span class="sc">{</span>fold <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss"> Test Results:</span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb58-109"><a href="#cb58-109" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Recall: </span><span class="sc">{</span>test_recall<span class="sc">:.4f}</span><span class="ss"> | Precision: </span><span class="sc">{</span>test_precision<span class="sc">:.4f}</span><span class="ss"> | F1: </span><span class="sc">{</span>test_f1<span class="sc">:.4f}</span><span class="ss"> | AUC: </span><span class="sc">{</span>test_auc<span class="sc">:.4f}</span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb58-110"><a href="#cb58-110" aria-hidden="true"></a></span>
<span id="cb58-111"><a href="#cb58-111" aria-hidden="true"></a>    <span class="co"># If first fold or test recall score is new best test recall score achieved</span></span>
<span id="cb58-112"><a href="#cb58-112" aria-hidden="true"></a>    <span class="cf">if</span> fold <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> test_recall <span class="op">&gt;</span> best_recall:</span>
<span id="cb58-113"><a href="#cb58-113" aria-hidden="true"></a></span>
<span id="cb58-114"><a href="#cb58-114" aria-hidden="true"></a>      <span class="co"># Store best test recall score achieved</span></span>
<span id="cb58-115"><a href="#cb58-115" aria-hidden="true"></a>      best_recall <span class="op">=</span> test_recall</span>
<span id="cb58-116"><a href="#cb58-116" aria-hidden="true"></a></span>
<span id="cb58-117"><a href="#cb58-117" aria-hidden="true"></a>      <span class="co"># Store fold of best test recall score achieved</span></span>
<span id="cb58-118"><a href="#cb58-118" aria-hidden="true"></a>      best_fold <span class="op">=</span> fold</span>
<span id="cb58-119"><a href="#cb58-119" aria-hidden="true"></a></span>
<span id="cb58-120"><a href="#cb58-120" aria-hidden="true"></a>      <span class="co"># Store false negative list of best-performing fold model</span></span>
<span id="cb58-121"><a href="#cb58-121" aria-hidden="true"></a>      best_fn_list <span class="op">=</span> fn_list.copy()</span></code></pre></div>
<pre><code>Evaluating Fold 1 model:


Fold 1 Test: 100%|██████████| 63/63 [00:10&lt;00:00,  6.29it/s]


Fold 1 Test Results:


Recall: 0.9080 | Precision: 0.9439 | F1: 0.9256 | AUC: 0.9768

Evaluating Fold 2 model:


Fold 2 Test: 100%|██████████| 63/63 [00:01&lt;00:00, 50.07it/s]


Fold 2 Test Results:


Recall: 0.8920 | Precision: 0.9510 | F1: 0.9205 | AUC: 0.9763

Evaluating Fold 3 model:


Fold 3 Test: 100%|██████████| 63/63 [00:01&lt;00:00, 48.38it/s]


Fold 3 Test Results:


Recall: 0.8920 | Precision: 0.9215 | F1: 0.9065 | AUC: 0.9688

Evaluating Fold 4 model:


Fold 4 Test: 100%|██████████| 63/63 [00:01&lt;00:00, 50.44it/s]


Fold 4 Test Results:


Recall: 0.8940 | Precision: 0.9470 | F1: 0.9198 | AUC: 0.9769

Evaluating Fold 5 model:


Fold 5 Test: 100%|██████████| 63/63 [00:01&lt;00:00, 50.27it/s]

Fold 5 Test Results:


Recall: 0.8860 | Precision: 0.9307 | F1: 0.9078 | AUC: 0.9722</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true"></a><span class="co"># Create list to hold filepaths of false negatives of best model</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true"></a>wrong_files <span class="op">=</span> []</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true"></a></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true"></a><span class="co"># Iterate over list of false negatives of best model</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true"></a><span class="cf">for</span> index <span class="kw">in</span> best_fn_list:</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true"></a></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true"></a>    <span class="co"># Extract filepath and class index</span></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true"></a>    path, class_index <span class="op">=</span> test_set.samples[index]</span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true"></a></span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true"></a>    <span class="co"># Extract class folder name using class index</span></span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true"></a>    class_folder_name <span class="op">=</span> test_set.classes[class_index]</span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true"></a></span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true"></a>    <span class="co"># Extract filename from path</span></span>
<span id="cb60-14"><a href="#cb60-14" aria-hidden="true"></a>    filename <span class="op">=</span> os.path.basename(path)</span>
<span id="cb60-15"><a href="#cb60-15" aria-hidden="true"></a></span>
<span id="cb60-16"><a href="#cb60-16" aria-hidden="true"></a>    <span class="co"># Append class folder name and filename to list</span></span>
<span id="cb60-17"><a href="#cb60-17" aria-hidden="true"></a>    wrong_files.append([class_folder_name, filename])</span>
<span id="cb60-18"><a href="#cb60-18" aria-hidden="true"></a></span>
<span id="cb60-19"><a href="#cb60-19" aria-hidden="true"></a><span class="co"># Convert false negative filepath list to DataFrame</span></span>
<span id="cb60-20"><a href="#cb60-20" aria-hidden="true"></a>wrong_df <span class="op">=</span> pd.DataFrame(wrong_files, columns<span class="op">=</span>[<span class="st">&quot;Class&quot;</span>, <span class="st">&quot;Filename&quot;</span>])</span></code></pre></div>
<div class="sourceCode" id="cb61"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true"></a><span class="co"># Aggregate mean test recall score</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true"></a>mean_test_recall <span class="op">=</span> np.mean(test_recalls)</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true"></a></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true"></a><span class="co"># Compute standard deviation of test recall score</span></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true"></a>std_test_recall <span class="op">=</span> np.std(test_recalls)</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true"></a></span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true"></a><span class="co"># Compute confidence interval of test recall score</span></span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true"></a>ci_test_recall <span class="op">=</span> <span class="fl">1.96</span> <span class="op">*</span> std_test_recall <span class="op">/</span> np.sqrt(config[<span class="st">&#39;k&#39;</span>])</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true"></a></span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true"></a><span class="co"># Aggregate mean test precision score</span></span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true"></a>mean_test_precision <span class="op">=</span> np.mean(test_precisions)</span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true"></a></span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true"></a><span class="co"># Aggregate mean test F1 score</span></span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true"></a>mean_test_f1 <span class="op">=</span> np.mean(test_f1s)</span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true"></a></span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true"></a><span class="co"># Aggregate mean test ROC-AUC score</span></span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true"></a>mean_test_auc <span class="op">=</span> np.mean(test_aucs)</span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true"></a></span>
<span id="cb61-19"><a href="#cb61-19" aria-hidden="true"></a><span class="co"># Notify user of aggregated test results</span></span>
<span id="cb61-20"><a href="#cb61-20" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;FINAL TEST SET RESULTS</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb61-21"><a href="#cb61-21" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Mean Test Recall:    </span><span class="sc">{</span>mean_test_recall<span class="sc">:.4f}</span><span class="ss"> ± </span><span class="sc">{</span>std_test_recall<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb61-22"><a href="#cb61-22" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">95% CI (Recall):     [</span><span class="sc">{</span>mean_test_recall <span class="op">-</span> ci_test_recall<span class="sc">:.4f}</span><span class="ss">, </span><span class="sc">{</span>mean_test_recall <span class="op">+</span> ci_test_recall<span class="sc">:.4f}</span><span class="ss">]&quot;</span>)</span>
<span id="cb61-23"><a href="#cb61-23" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n\n</span><span class="ss">Mean Test Precision: </span><span class="sc">{</span>mean_test_precision<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb61-24"><a href="#cb61-24" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Mean Test F1:        </span><span class="sc">{</span>mean_test_f1<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb61-25"><a href="#cb61-25" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Mean Test AUC:       </span><span class="sc">{</span>mean_test_auc<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb61-26"><a href="#cb61-26" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n\n</span><span class="ss">Individual Folds:    </span><span class="sc">{</span>[<span class="ss">f&#39;</span><span class="sc">{r:.4f}</span><span class="ss">&#39;</span> <span class="cf">for</span> r <span class="kw">in</span> test_recalls]<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>FINAL TEST SET RESULTS


Mean Test Recall:    0.8944 ± 0.0073

95% CI (Recall):     [0.8880, 0.9008]


Mean Test Precision: 0.9388

Mean Test F1:        0.9160

Mean Test AUC:       0.9742


Individual Folds:    [&#39;0.9080&#39;, &#39;0.8920&#39;, &#39;0.8920&#39;, &#39;0.8940&#39;, &#39;0.8860&#39;]</code></pre>
<p>Here we have our best estimate of model generalization performance, test set metrics. The test set is both truly unseen by our model, so it better represents how our model is likely to perform on data from real world usage.</p>
<p>Our model architecture achieved a <strong>mean recall of 89.44%</strong> across the best model from each fold, a slight drop from our validation performance, most likely due to differences in data distribution between the test set and the training set. However, this is still an impressive result that makes our model a useful tool to support clinical decision-making in dermatology.</p>
<p>Additional test metrics include:</p>
<ul>
<li><strong>93.88% Mean Precision</strong> (strong performance): This represents the percentage of test data predicted positive that is truly positive.</li>
<li><strong>0.9742 Mean ROC-AUC</strong> (strong performance): This represents the probability that a random positive in our test set will be classified as more likely positive than a randomly chosen negative.</li>
<li><strong>91.60% Mean F1 Score</strong> (strong performance): This represents the harmonic mean of precision and recall.</li>
</ul>
<p>Strong results across all these metrics, not merely on our key metric of recall, indicate our model architecture is balanced and well-equipped for the diverse use cases in melanoma classification that real-world users may have. Overall, these results validate our architecture and show our model merits consideration for real-world application.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true"></a><span class="co"># Log aggregated test results to Weights &amp; Biases</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true"></a>wandb.log({</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true"></a>    <span class="st">&quot;test_mean_recall&quot;</span>: mean_test_recall,</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true"></a>    <span class="st">&quot;test_std_recall&quot;</span>: std_test_recall,</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true"></a>    <span class="st">&quot;test_ci_recall&quot;</span>: ci_test_recall,</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true"></a>    <span class="st">&quot;test_mean_precision&quot;</span>: mean_test_precision,</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true"></a>    <span class="st">&quot;test_mean_f1&quot;</span>: mean_test_f1,</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true"></a>    <span class="st">&quot;test_mean_auc&quot;</span>: mean_test_auc</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true"></a>})</span></code></pre></div>
<div class="sourceCode" id="cb64"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true"></a><span class="co"># Set theme of plot of test recall</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true"></a>sns.set_theme(style<span class="op">=</span><span class="st">&#39;whitegrid&#39;</span>, context<span class="op">=</span><span class="st">&#39;talk&#39;</span>, palette<span class="op">=</span><span class="st">&#39;deep&#39;</span>)</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true"></a></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true"></a><span class="co"># Create figure on which to plot test recall</span></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true"></a>plt.figure(figsize <span class="op">=</span> (<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true"></a></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true"></a><span class="co"># Create barplot on which to compare fold test recalls</span></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true"></a>sns.barplot(x <span class="op">=</span> [<span class="ss">f&#39;</span><span class="sc">{_</span> <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">&#39;</span> <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(config[<span class="st">&#39;k&#39;</span>])],</span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true"></a>            y <span class="op">=</span> test_recalls, hue <span class="op">=</span> [<span class="ss">f&#39;</span><span class="sc">{_</span> <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">&#39;</span> <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(config[<span class="st">&#39;k&#39;</span>])])</span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true"></a></span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true"></a><span class="co"># Set plot title and axes</span></span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true"></a>plt.title(<span class="st">&#39;Test Recall by K-Fold&#39;</span>)</span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true"></a>plt.xlabel(<span class="st">&#39;Fold&#39;</span>)</span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true"></a>plt.ylabel(<span class="st">&#39;Recall&#39;</span>)</span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true"></a></span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true"></a><span class="co"># Bound y-axis at maximum and minimum</span></span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true"></a>plt.ylim(<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true"></a></span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true"></a><span class="co"># Iterate over all test recall scores</span></span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true"></a><span class="cf">for</span> i, recall <span class="kw">in</span> <span class="bu">enumerate</span>(test_recalls):</span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true"></a></span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true"></a>  <span class="co"># Include exact recall score on plot</span></span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true"></a>  plt.text(i, recall <span class="op">+</span> <span class="fl">0.03</span>, <span class="ss">f&quot;</span><span class="sc">{</span>recall<span class="sc">:.3f}</span><span class="ss">&quot;</span>, ha <span class="op">=</span> <span class="st">&#39;center&#39;</span>)</span>
<span id="cb64-24"><a href="#cb64-24" aria-hidden="true"></a></span>
<span id="cb64-25"><a href="#cb64-25" aria-hidden="true"></a><span class="co"># Set plot layout</span></span>
<span id="cb64-26"><a href="#cb64-26" aria-hidden="true"></a>plt.tight_layout()</span>
<span id="cb64-27"><a href="#cb64-27" aria-hidden="true"></a></span>
<span id="cb64-28"><a href="#cb64-28" aria-hidden="true"></a><span class="co"># Reveal plot</span></span>
<span id="cb64-29"><a href="#cb64-29" aria-hidden="true"></a>plt.show()</span>
<span id="cb64-30"><a href="#cb64-30" aria-hidden="true"></a></span>
<span id="cb64-31"><a href="#cb64-31" aria-hidden="true"></a><span class="co"># Close our Weights &amp; Biases session</span></span>
<span id="cb64-32"><a href="#cb64-32" aria-hidden="true"></a>wandb.finish()</span>
<span id="cb64-33"><a href="#cb64-33" aria-hidden="true"></a></span>
<span id="cb64-34"><a href="#cb64-34" aria-hidden="true"></a><span class="co"># Notify user that evaluation is complete</span></span>
<span id="cb64-35"><a href="#cb64-35" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;Evaluation complete! Check Weights &amp; Biases for detailed results.&quot;</span>)</span></code></pre></div>
<figure>
<img src="https://theonlineconverter.com/uploads/Checkpoint_3_1765927044_files/Checkpoint_3_1765927044_70_0.png" alt="" /><figcaption>png</figcaption>
</figure>
<pre><code>Evaluation complete! Check Weights &amp; Biases for detailed results.</code></pre>
<p>Here, we can see the tight spread of our fold models’ test recall scores. The scores range from 0.886 at the lowest to 0.908 at the highest, which is a total range of only 2.2%. This tight spread is a testament to our model architecture’s consistency and ability to achieve strong generalization on diverse training data (i.e., diverse augmented training-validation splits).</p>
<p>The small differences in model architecture performance that are present can be attributed to the stochastic nature of the AdamW optimization algorithm, as well as nuances in data augmentation and/or training-validation split.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true"></a><span class="co"># Notify user of what following images represent</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;Sample false negative images of best-performing model:&quot;</span>)</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true"></a></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true"></a><span class="co"># Display sample false negative images of best-performing model</span></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true"></a>display_sample_images(</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true"></a>    zip_path<span class="op">=</span>test_zip,</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true"></a>    df<span class="op">=</span>wrong_df,</span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true"></a>    sample_size<span class="op">=</span><span class="bu">min</span>(<span class="dv">5</span>, <span class="bu">len</span>(wrong_df))</span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true"></a>)</span></code></pre></div>
<pre><code>Sample false negative images of best-performing model:</code></pre>
<figure>
<img src="https://theonlineconverter.com/uploads/Checkpoint_3_1765927044_files/Checkpoint_3_1765927044_72_1.png" alt="" /><figcaption>png</figcaption>
</figure>
<p>Here we can see some sample images that our model wrongly classified as negative. Since this is the outcome we most want to avoid, it’s important to assess common characteristics of false negatives to see if we can better understand the limitations of our model.</p>
<p>From this initial eye test, we can see that our model misclassified two images with light brown lesions on a rosy skin background, and two very dark lesions. We can also observe that, interestingly, none of the false negative images displayed by our function are jittered or rotated images.</p>
<p>Whether this lack of augmentation or these image characteristics represent limitation(s) of our model will require further analysis to decide. Unfortunately, this analysis is outside the time scope of this project, but this is the natural next step of this project, which we intend to pursue in the future.</p>
<h2 id="conclusion-further-learning">Conclusion &amp; Further Learning</h2>
<p>We hope this tutorial has been useful to you! Thank you for following along with our work. We hope you see now that Machine Learning is not some indecipherable, inaccessible black box, but a technique that can be leveraged even by mere students to achieve great success in impactful work like clinical diagnosis.</p>
<p>There are certainly some spots we glazed over for brevity and readability’s sake. If you enjoyed this tutorial and are wondering “what now?”, we encourage you to dive deeper using the resources below!</p>
<p>To</p>
<p>To understand a main driver of the machine learning process, optimization algorithms, see this overview:</p>
<ul>
<li>Srivastava, A., Rawat, B. S., Singh, G., Bhatnagar, V., Saini, P. K., &amp; Shiv Ashish Dhondiyal. (2023). <em>A Review of Optimization Algorithms for Training Neural Networks.</em> https://doi.org/10.1109/icseiet58677.2023.10303287</li>
</ul>
<p>To understand why we ultimately chose the AdamW optimizer after several iterations using the Adam optimizer:</p>
<ul>
<li>Loshchilov, I., &amp; Hutter, F. (2017). Decoupled Weight Decay Regularization. <em>Arxiv.org.</em> https://arxiv.org/abs/1711.05101</li>
</ul>
